diff -urN exp-flowcheck-base/fc_errors.c exp-flowcheck/fc_errors.c
--- exp-flowcheck-base/fc_errors.c	2016-08-30 16:12:49.912947327 -0500
+++ exp-flowcheck/fc_errors.c	2016-08-30 15:23:13.956399727 -0500
@@ -6,7 +6,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
@@ -48,6 +51,9 @@
 #include "pub_tool_addrinfo.h"
 
 #include "fc_include.h"
+#include "popcount.h"
+#include "trace_runtime.h"
+
 
 
 /*------------------------------------------------------------*/
@@ -721,6 +727,8 @@
    doing the --workaround-gcc296-bugs hack. */
 #define VG_GCC296_BUG_STACK_SLOP 1024
 
+
+
 /* Is this address within some small distance below %ESP?  Used only
    for the --workaround-gcc296-bugs kludge. */
 static Bool is_just_below_ESP( Addr esp, Addr aa )
@@ -760,29 +768,106 @@
    VG_(maybe_record_error)( tid, Err_Addr, a, /*s*/NULL, &extra );
 }
 
-void FC_(record_value_error) ( ThreadId tid, Int szB, UInt otag )
-{
-   FC_Error extra;
-   tl_assert( FC_(clo_fc_level) >= 2 );
-   if (otag > 0)
-      tl_assert( FC_(clo_fc_level) == 3 );
-   extra.Err.Value.szB       = szB;
-   extra.Err.Value.otag      = otag;
-   extra.Err.Value.origin_ec = NULL;  /* Filled in later */
-   VG_(maybe_record_error)( tid, Err_Value, /*addr*/0, /*s*/NULL, &extra );
+void FC_(record_value_error) ( ThreadId tid, Int size,
+				    UInt val, UInt vbits, ULong tag,
+				    ULong location, UWord origin )
+{
+   if (FC_(enclosure_level) && !FC_(enclosure_mode)) {
+      FC_(enclosure_mode) = 1;
+      if (FC_(clo_trace_secret_graph))
+	 trace_enter_enclose();
+   }
+   if (!FC_(enclosure_mode)) {
+      int num_leaked;
+      Addr eip = VG_(get_IP)(tid);
+      VG_(printf)("At %08lx (%d): ", eip, FC_(enclosure_level));
+      if (size == 0) {
+	 num_leaked = 1;
+	 VG_(printf)("Leaked a bit from tainted branch condition\n");
+      } else if (size == 1) {
+	 num_leaked = popcount8(vbits & 0xff);
+	 VG_(printf)("Tainted bits leaked to byte operation: "
+		     "%02x %02x (%d bits)\n", val & 0xff, vbits & 0xff,
+		     num_leaked);
+      } else if (size == 4) {
+	 num_leaked = popcount32(vbits);
+	 VG_(printf)("Tainted bits leaked to word operation: "
+		     "%08x %08x (%d bits)\n", val, vbits, num_leaked);
+      } else {
+	 tl_assert(0);
+      }
+      if (FC_(clo_trace_secret_graph))
+	 trace_leak(tag, num_leaked, location);
+      FC_(add_to_leaked)(num_leaked);
+      
+      if (FC_(clo_detailed_leak_report)) {
+	 FC_Error extra;
+	 extra.Err.Value.szB = size;
+	 VG_(maybe_record_error)( tid, Err_Value, /*addr*/0,
+				  /*s*/NULL, &extra );
+      }
+   } else {
+      int num_leaked;
+      if (size == 0) {
+	 num_leaked = 1;
+      } else if (size == 1) {
+	 num_leaked = popcount8(vbits & 0xff);
+      } else if (size == 4) {
+	 num_leaked = popcount32(vbits);
+      } else {
+	 tl_assert(0);
+      }
+      if (FC_(clo_trace_secret_graph))
+	 trace_leak_enclosed(tag, num_leaked, location);
+   }
 }
 
-void FC_(record_cond_error) ( ThreadId tid, UInt otag )
-{
-   FC_Error extra;
-   tl_assert( FC_(clo_fc_level) >= 2 );
-   if (otag > 0)
-      tl_assert( FC_(clo_fc_level) == 3 );
-   extra.Err.Cond.otag      = otag;
-   extra.Err.Cond.origin_ec = NULL;  /* Filled in later */
-   VG_(maybe_record_error)( tid, Err_Cond, /*addr*/0, /*s*/NULL, &extra );
+void FC_(record_revert_error)(ThreadId tid, Addr addr, Int ident,
+				UChar cur_value, UChar old_value,
+				Addr written_at) {
+   if (FC_(clo_trace_reverts)) {
+      VG_(printf)("Rollback %d: reverting %p from 0x%x to 0x%x "
+		  "(written at 0x%08lx)\n",
+		  ident, (void *) addr, cur_value, old_value, written_at);
+      if (FC_(clo_detailed_leak_report)) {
+	 FC_Error extra;
+	 extra.Err.Value.szB = 8;
+	 VG_(maybe_record_error)( tid, Err_Value, addr,
+				  /*s*/NULL, &extra );
+      }
+   }
+}
+
+
+
+void FC_(check_mem_is_defined_secret) ( CorePart part, ThreadId tid, const HChar* s,
+                                 Addr base, SizeT size )
+{    
+   Addr eip = VG_(get_IP)(tid);
+   ULong location = (ULong)eip << 16;
+   int num_leaked = FC_(count_tainted_bits)(base, size);
+
+   /* Note, have to do this on every write, not just the ones where
+      the output bits are tainted, to include the leak cascade. */
+   count_leaked_tags(base, size, location);
+
+   if (num_leaked) {
+      tl_assert(part == Vg_CoreSysCall);
+      VG_(printf)("Leaking %d bits via %s of %lu bytes\n", num_leaked, s, size);
+      FC_(add_to_leaked)(num_leaked);
+
+      if (FC_(clo_detailed_leak_report)) {
+	 FC_Error extra;
+	 extra.Err.Value.szB = size;
+	 extra.Err.User.ai.tag = Addr_Undescribed;
+	 /* err_extra.isUnaddr = False; */
+	 VG_(maybe_record_error)( tid, Err_MemParam, /*addr*/0,
+				  /*s*/NULL, &extra );
+      }
+   }
 }
 
+
 /* --- Called from non-generated code --- */
 
 /* This is for memory errors in signal-related memory. */
@@ -864,13 +949,12 @@
 void FC_(record_overlap_error) ( ThreadId tid, const HChar* function,
                                  Addr src, Addr dst, SizeT szB )
 {
-   FC_Error extra;
+//   FC_Error extra;
    tl_assert(VG_INVALID_THREADID != tid);
-   extra.Err.Overlap.src = src;
-   extra.Err.Overlap.dst = dst;
-   extra.Err.Overlap.szB = szB;
-   VG_(maybe_record_error)( 
-      tid, Err_Overlap, /*addr*/0, /*s*/function, &extra );
+//   extra.Err.Overlap.src = src;
+//  extra.Err.Overlap.dst = dst;
+//   extra.Err.Overlap.szB = szB;
+
 }
 
 Bool FC_(record_leak_error) ( ThreadId tid, UInt n_this_record,
@@ -925,6 +1009,7 @@
    VG_(maybe_record_error)( tid, Err_User, a, /*s*/NULL, &extra );
 }
 
+
 /*------------------------------------------------------------*/
 /*--- Other error operations                               ---*/
 /*------------------------------------------------------------*/
@@ -1579,6 +1664,14 @@
    }
 }
 
+void FC_(record_leaked_word)(Addr addr){
+	 FC_Error extra;
+	 extra.Err.Value.szB = 4;
+	 VG_(maybe_record_error)(VG_(get_running_tid)(), Err_Value, addr,
+				 /*s*/NULL, &extra );
+
+}
+
 /*--------------------------------------------------------------------*/
 /*--- end                                              fc_errors.c ---*/
 /*--------------------------------------------------------------------*/
diff -urN exp-flowcheck-base/fc_include.h exp-flowcheck/fc_include.h
--- exp-flowcheck-base/fc_include.h	2016-08-30 16:12:49.884946867 -0500
+++ exp-flowcheck/fc_include.h	2016-08-30 15:23:13.960196717 -0500
@@ -1,12 +1,15 @@
 
 /*--------------------------------------------------------------------*/
-/*--- A header file for all parts of the FlowCheck tool.            ---*/
+/*--- A header file for all parts of the FlowCheck tool.           ---*/
 /*---                                                 fc_include.h ---*/
 /*--------------------------------------------------------------------*/
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
@@ -36,7 +39,7 @@
 
 
 /* This is a private header file for use only within the
-   flowcheck/ directory. */
+   exp-flowcheck/ directory. */
 
 /*------------------------------------------------------------*/
 /*--- Tracking the heap                                    ---*/
@@ -49,6 +52,7 @@
 // effective redzone, as (possibly) modified by --redzone-size:
 extern SizeT FC_(Malloc_Redzone_SzB);
 
+
 /* For malloc()/new/new[] vs. free()/delete/delete[] mismatch checking. */
 typedef
    enum {
@@ -134,11 +138,16 @@
 
 /* Shadow memory functions */
 Bool FC_(check_mem_is_noaccess)( Addr a, SizeT len, Addr* bad_addr );
+void FC_(check_mem_is_defined_secret) ( CorePart part, ThreadId tid, const HChar* s,
+                                 Addr base, SizeT size );
 void FC_(make_mem_noaccess)        ( Addr a, SizeT len );
 void FC_(make_mem_undefined_w_otag)( Addr a, SizeT len, UInt otag );
+void FC_(make_mem_undefined) 	   ( Addr a, SizeT len );
 void FC_(make_mem_defined)         ( Addr a, SizeT len );
+void FC_(make_mem_accessible)(Addr a, SizeT len);
 void FC_(copy_address_range_state) ( Addr src, Addr dst, SizeT len );
 
+
 void FC_(print_malloc_stats) ( void );
 /* nr of free operations done */
 SizeT FC_(get_cmalloc_n_frees) ( void );
@@ -256,6 +265,7 @@
    MCPE_MAKE_ALIGNED_WORD32_UNDEFINED_SLOW,
    MCPE_MAKE_ALIGNED_WORD64_UNDEFINED,
    MCPE_MAKE_ALIGNED_WORD64_UNDEFINED_SLOW,
+   MCPE_MAKE_ALIGNED_WORD64_READABLE,
    MCPE_MAKE_ALIGNED_WORD32_NOACCESS,
    MCPE_MAKE_ALIGNED_WORD32_NOACCESS_SLOW,
    MCPE_MAKE_ALIGNED_WORD64_NOACCESS,
@@ -322,6 +332,7 @@
    MCPE_MAKE_STACK_UNINIT_128_NO_O_ALIGNED_16,
    MCPE_MAKE_STACK_UNINIT_128_NO_O_ALIGNED_8,
    MCPE_MAKE_STACK_UNINIT_128_NO_O_SLOWCASE,
+   MCPE_CHECK_READABLE_LOOP,
    /* Do not add enumerators past this line. */
    MCPE_LAST
 };
@@ -532,7 +543,9 @@
 void FC_(record_address_error) ( ThreadId tid, Addr a, Int szB,
                                  Bool isWrite );
 void FC_(record_cond_error)    ( ThreadId tid, UInt otag );
-void FC_(record_value_error)   ( ThreadId tid, Int szB, UInt otag );
+void FC_(record_value_error)  ( ThreadId tid, Int size,
+				    UInt val, UInt vbits, ULong tag,
+				    ULong location, UWord origin );
 void FC_(record_jump_error)    ( ThreadId tid, Addr a );
 
 void FC_(record_free_error)            ( ThreadId tid, Addr a ); 
@@ -558,6 +571,13 @@
 Bool FC_(record_fishy_value_error)  ( ThreadId tid, const HChar* function,
                                       const HChar *argument_name, SizeT value );
 
+void FC_(record_revert_error)      ( ThreadId tid, Addr addr, Int ident,
+				       UChar cur_value, UChar old_value,
+				       Addr written_at );
+
+void FC_(record_leaked_word)(Addr addr);
+
+
 /* Leak kinds tokens to call VG_(parse_enum_set). */
 extern const HChar* FC_(parse_leak_kinds_tokens);
 
@@ -567,6 +587,9 @@
 /* Is this address in a user-specified "ignored range" ? */
 Bool FC_(in_ignored_range) ( Addr a );
 
+int FC_(count_tainted_bits)(Addr a, SizeT len);
+
+void FC_(add_to_leaked)(int num_to_add);
 
 /*------------------------------------------------------------*/
 /*--- Client blocks                                        ---*/
@@ -710,6 +733,87 @@
    operations? Default: NO */
 extern Bool FC_(clo_expensive_definedness_checks);
 
+
+/* Treat data read from non-world-readable files as tainted?
+ * default: NO */
+extern Bool FC_(clo_private_files_are_secret);
+
+/* Treat data that's unpredictable, like /dev/urandom, the PID, and
+ * the time of day, as `secret' (for use in testing RNGs, etc.)?
+ * default: NO */
+extern Bool FC_(clo_unpredictable_is_secret);
+
+/* Treat data read from the standard input as tainted?
+ * default: NO */
+extern Bool FC_(clo_stdin_is_secret);
+
+/* Report leaks with a backtrace and other Memcheck-like details?
+ * default: NO */
+extern Bool  FC_(clo_detailed_leak_report);
+
+/* Abort execution after this many bits have been leaked
+ * default: effectively infinite */
+extern Long  FC_(clo_max_bits_leaked);
+
+/* Output a graph showing all the operations on secrets along with
+ * edge capacities?
+ * default: NO */
+extern Bool  FC_(clo_trace_secret_graph);
+
+/* Mark all bits generated inside an enclosure region as secret?
+ * default: NO */
+extern Bool  FC_(clo_taint_all_enclosed);
+
+/* Print a warning message every time an enclosure region reverts a *
+ * memory location on exit?
+ * default: NO */
+extern Bool  FC_(clo_trace_reverts);
+
+/* Taint the modified bits every time an enclosure region reverts a *
+ * memory location on exit?
+ * default: NO */
+extern Bool  FC_(clo_taint_reverts);
+
+extern Int  FC_(clo_revert_notbelow);
+
+/* To what extent should the flow graph be collapsed by combining
+   similar edges? Currently defined values:
+
+   0   - do not collapse at all (edges will be output incrementally)
+   10  - collapse edges with the same source location and call stack
+         (fully context sensitive)
+   50  - collapse edges with the same source location and caller
+         (one level context sensitive)
+   100 - collapse edges with the same source location, no matter
+         what call stack (context insensitive)
+   default: 10 */
+extern Int FC_(clo_folding_level);
+
+/* Run the maximum flow computation after each output? */
+extern Bool FC_(clo_incremental);
+
+/* File to store the produced graph in */
+extern const char *FC_(clo_graph_file);
+
+/* Maximum flow program to pass it to */
+extern const char *FC_(clo_max_flow_program);
+
+/*------------------------------------------------------------*/
+/*--- Variables                                            ---*/
+/*------------------------------------------------------------*/
+
+extern Long FC_(total_bits_leaked);
+extern int  FC_(enclosure_mode);
+extern int  FC_(enclosure_level);
+
+/*------------------------------------------------------------*/
+/*--- Rollback                                             ---*/
+/*------------------------------------------------------------*/
+
+extern int  FC_(rollback_mode);
+
+extern Bool FC_(is_escaping)(Addr addr);
+
 /*------------------------------------------------------------*/
 /*--- Instrumentation                                      ---*/
 /*------------------------------------------------------------*/
@@ -720,19 +824,26 @@
    origin tag and should really be UInt, but to be simple and safe
    considering it's called from generated code, just claim it to be a
    UWord. */
-VG_REGPARM(2) void FC_(helperc_value_checkN_fail_w_o) ( HWord, UWord );
-VG_REGPARM(1) void FC_(helperc_value_check8_fail_w_o) ( UWord );
-VG_REGPARM(1) void FC_(helperc_value_check4_fail_w_o) ( UWord );
-VG_REGPARM(1) void FC_(helperc_value_check1_fail_w_o) ( UWord );
-VG_REGPARM(1) void FC_(helperc_value_check0_fail_w_o) ( UWord );
+
+void FC_(helperc_value_checkN_fail_w_o) ( HWord, UInt,
+							UInt, ULong, ULong, UWord );
+void FC_(helperc_value_check8_fail_w_o) ( ULong, ULong, UWord );
+void FC_(helperc_value_check4_fail_w_o) ( UInt, UInt, ULong,
+							   ULong, UWord );
+void FC_(helperc_value_check1_fail_w_o) ( UChar, UChar,
+							   ULong, ULong, UWord);
+void FC_(helperc_value_check0_fail_w_o) ( ULong, ULong, UWord );
 
 /* And call these ones instead to report an uninitialised value error
    but with no origin available. */
-VG_REGPARM(1) void FC_(helperc_value_checkN_fail_no_o) ( HWord );
-VG_REGPARM(0) void FC_(helperc_value_check8_fail_no_o) ( void );
-VG_REGPARM(0) void FC_(helperc_value_check4_fail_no_o) ( void );
-VG_REGPARM(0) void FC_(helperc_value_check1_fail_no_o) ( void );
-VG_REGPARM(0) void FC_(helperc_value_check0_fail_no_o) ( void );
+void FC_(helperc_value_checkN_fail_no_o) ( HWord, UInt,
+							UInt, ULong, ULong );
+void FC_(helperc_value_check8_fail_no_o) ( ULong, ULong);
+void FC_(helperc_value_check4_fail_no_o) ( UInt, UInt, ULong,
+							   ULong );
+void FC_(helperc_value_check1_fail_no_o) ( UChar, UChar,
+							   ULong, ULong);
+void FC_(helperc_value_check0_fail_no_o) ( ULong, ULong );
 
 /* V-bits load/store helpers */
 VG_REGPARM(1) void FC_(helperc_STOREV64be) ( Addr, ULong );
@@ -755,6 +866,12 @@
 VG_REGPARM(1) UWord FC_(helperc_LOADV16le)  ( Addr );
 VG_REGPARM(1) UWord FC_(helperc_LOADV8)     ( Addr );
 
+VG_REGPARM(1) void FC_(save_for_rollback8) (Addr location);
+VG_REGPARM(1) void FC_(save_for_rollback16)(Addr location);
+VG_REGPARM(1) void FC_(save_for_rollback32)(Addr location);
+VG_REGPARM(1) void FC_(save_for_rollback64)(Addr location);
+VG_REGPARM(1) void FC_(save_for_rollback128)(Addr location);
+
 VG_REGPARM(3)
 void FC_(helperc_MAKE_STACK_UNINIT_w_o) ( Addr base, UWord len, Addr nia );
 
@@ -788,9 +905,6 @@
 
 IRSB* FC_(final_tidy) ( IRSB* );
 
-/* Check some assertions to do with the instrumentation machinery. */
-void FC_(do_instrumentation_startup_checks)( void );
-
 #endif /* ndef __FC_INCLUDE_H */
 
 /*--------------------------------------------------------------------*/
diff -urN exp-flowcheck-base/fc_leakcheck.c exp-flowcheck/fc_leakcheck.c
--- exp-flowcheck-base/fc_leakcheck.c	2016-08-30 16:12:49.892946998 -0500
+++ exp-flowcheck/fc_leakcheck.c	2016-08-30 15:23:13.964724299 -0500
@@ -5,7 +5,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
diff -urN exp-flowcheck-base/fc_machine.c exp-flowcheck/fc_machine.c
--- exp-flowcheck-base/fc_machine.c	2016-08-30 16:12:49.916947392 -0500
+++ exp-flowcheck/fc_machine.c	2016-08-30 15:23:13.969071066 -0500
@@ -7,7 +7,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2008-2015 OpenWorks Ltd
       info@open-works.co.uk
@@ -183,22 +186,13 @@
    if (o == GOF(IP_AT_SYSCALL) && sz == 8) return -1; /* slot unused */
    if (o == GOF(FPROUND)   && sz == 1) return -1;
    if (o == GOF(DFPROUND)  && sz == 1) return -1;
-   if (o == GOF(C_FPCC)    && sz == 1) return -1;
+   if (o == GOF(FPCC)      && sz == 1) return -1;
    if (o == GOF(EMNOTE)    && sz == 4) return -1;
    if (o == GOF(CMSTART)   && sz == 8) return -1;
    if (o == GOF(CMLEN)     && sz == 8) return -1;
    if (o == GOF(VSCR)      && sz == 4) return -1;
    if (o == GOF(VRSAVE)    && sz == 4) return -1;
    if (o == GOF(REDIR_SP)  && sz == 8) return -1;
-   if (o == GOF(NRADDR)    && sz == 8) return -1;
-   if (o == GOF(NRADDR_GPR2) && sz == 8) return -1;
-   if (o == GOF(REDIR_STACK) && sz == 8) return -1;
-   if (o == GOF(TFHAR)     && sz == 8) return -1;
-   if (o == GOF(TEXASR)    && sz == 8) return -1;
-   if (o == GOF(TEXASRU)   && sz == 8) return -1;
-   if (o == GOF(TFIAR)     && sz == 8) return -1;
-   if (o == GOF(PPR)       && sz == 8) return -1;
-   if (o == GOF(PSPB)      && sz == 8) return -1;
 
    // With ISA 2.06, the "Vector-Scalar Floating-point" category
    // provides facilities to support vector and scalar binary floating-
diff -urN exp-flowcheck-base/fc_main.c exp-flowcheck/fc_main.c
--- exp-flowcheck-base/fc_main.c	2016-08-30 16:12:49.908947262 -0500
+++ exp-flowcheck/fc_main.c	2016-08-30 15:28:37.566119484 -0500
@@ -1,14 +1,17 @@
 /* -*- mode: C; c-basic-offset: 3; -*- */
 
 /*--------------------------------------------------------------------*/
-/*--- FlowCheck: Maintain bitmaps of memory, tracking the           ---*/
-/*--- accessibility (A) and validity (V) status of each byte.      ---*/
+/*--- FlowCheck: Maintain bitmaps of memory, tracking the          ---*/
+/*--- accessibility (A) and secrecy (V) status of each byte.       ---*/
 /*---                                                    fc_main.c ---*/
 /*--------------------------------------------------------------------*/
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
@@ -32,11 +35,16 @@
 */
 
 #include "pub_tool_basics.h"
+
+#include "pub_tool_vki.h"
+#include "pub_tool_vkiscnums.h"
+
 #include "pub_tool_aspacemgr.h"
 #include "pub_tool_gdbserver.h"
 #include "pub_tool_poolalloc.h"
 #include "pub_tool_hashtable.h"     // For fc_include.h
 #include "pub_tool_libcbase.h"
+#include "pub_tool_libcfile.h"
 #include "pub_tool_libcassert.h"
 #include "pub_tool_libcprint.h"
 #include "pub_tool_machine.h"
@@ -50,7 +58,9 @@
 
 #include "fc_include.h"
 #include "flowcheck.h"   /* for client requests */
-
+#include "trace_runtime.h"
+#include "popcount.h"
+#include "fold.h"
 
 /* Set to 1 to enable handwritten assembly helpers on targets for
    which it is supported. */
@@ -70,13 +80,14 @@
 /*------------------------------------------------------------*/
  
 // Comment these out to disable the fast cases (don't just set them to zero).
-
+/*
 #define PERF_FAST_LOADV    1
 #define PERF_FAST_STOREV   1
 
 #define PERF_FAST_SARP     1
 
 #define PERF_FAST_STACK    1
+*/
 #define PERF_FAST_STACK2   1
 
 /* Change this to 1 to enable assertions on origin tracking cache fast
@@ -778,13 +789,113 @@
 }
 
 
+#define MAX_TAINT_EXCEPTIONS 10
+struct taint_region {
+   Addr start;
+   SizeT size;
+   int num_exceptions;
+   Addr exceptions[MAX_TAINT_EXCEPTIONS];
+};
+
+#define MAX_TAINT_REGIONS 30
+static struct taint_region taint_regions[MAX_TAINT_REGIONS];
+static int num_taint_regions;
+
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+
+static void add_taint_region(Addr start, SizeT size) {
+   int i;
+   Addr end = start + size;
+   for (i = 0; i < num_taint_regions; i++) {
+      Addr old_start = taint_regions[i].start;
+      Addr old_end = old_start + taint_regions[i].size;
+      if (!(end < old_start) && !(old_end < start)) {
+	 /* overlapping or adjacent regions: merge */
+	 Addr new_end;
+	 int j;
+	 taint_regions[i].start = MIN(start, old_start);
+	 new_end = MAX(end, old_end);
+	 taint_regions[i].size = new_end - taint_regions[i].start;
+	 /* if (taint_regions[i].start != old_start || new_end != old_end)
+	    VG_(printf)("Expanding taint region %d to 0x%08x to 0x%08x"
+			" (size %d)\n",
+			i, taint_regions[i].start, end, taint_regions[i].size); */
+	 for (j = 0; j < taint_regions[i].num_exceptions; j++) {
+	    Addr a = taint_regions[i].exceptions[j];
+	    if (a >= start && a < end) {
+	       Addr old_size = taint_regions[i].num_exceptions;
+	       /* VG_(printf)("Removing exception for 0x%08x\n", a); */
+	       taint_regions[i].exceptions[j] = 
+		  taint_regions[i].exceptions[old_size - 1];
+	       --taint_regions[i].num_exceptions;
+	       j--;
+	    }
+	 }
+	 return;
+      }
+   }
+   i = num_taint_regions++;
+   tl_assert(num_taint_regions <= MAX_TAINT_REGIONS);
+   taint_regions[i].start = start;
+   taint_regions[i].size = size;
+   taint_regions[i].num_exceptions = 0;
+   /*VG_(printf)("Adding tainting region %d from 0x%08x to 0x%08x (size %d)\n",
+     i, start, end, size);*/
+}
+
 // Forward declarations
 static UWord get_sec_vbits8(Addr a);
 static void  set_sec_vbits8(Addr a, UWord vbits8);
 
-// Returns False if there was an addressability error.
+
+static Bool get_vbits8_noregion ( Addr a, UChar* vbits8 ) {
+   Bool  ok      = True;
+   UChar vabits2 = get_vabits2(a);
+
+   // Convert the in-memory format to in-register format.
+   if      ( VA_BITS2_DEFINED   == vabits2 ) { *vbits8 = V_BITS8_DEFINED;   }
+   else if ( VA_BITS2_UNDEFINED == vabits2 ) { *vbits8 = V_BITS8_UNDEFINED; }
+   else if ( VA_BITS2_NOACCESS  == vabits2 ) {
+      *vbits8 = V_BITS8_DEFINED;    // Make V bits defined!
+      ok = False;
+   } else {
+      tl_assert( VA_BITS2_PARTDEFINED == vabits2 );
+      *vbits8 = get_sec_vbits8(a);
+   }
+   /*VG_(printf)("taint[%08x] is %02x (flat)\n", a, *vbits8);*/
+   return ok;
+}
+
 static INLINE
-Bool set_vbits8 ( Addr a, UChar vbits8 )
+Bool get_vbits8 ( Addr addr, UChar* vbits8 )
+{
+   int i;
+   for (i = 0; i < num_taint_regions; i++) {
+      /*VG_(printf)("Checking 0x%08x versus 0x%08x and 0x%08x\n",
+		  addr, taint_regions[i].start,
+		  taint_regions[i].start + taint_regions[i].size);*/
+      if (addr >= taint_regions[i].start &&
+	  addr < taint_regions[i].start + taint_regions[i].size) {
+	 int j;
+	 /*VG_(printf)("Hit tainting region %d\n", i);*/
+	 for (j = 0; j < taint_regions[i].num_exceptions; j++) {
+	    if (taint_regions[i].exceptions[j] == addr) {
+	       /*VG_(printf)("    but 0x%08x is an exception\n", addr);*/
+	       return get_vbits8_noregion(addr, vbits8);
+	    }
+	 }
+	 *vbits8 = 0xff;
+	 return True;
+      }
+   }
+   return get_vbits8_noregion(addr, vbits8);
+}
+
+
+// Returns False if there was an addressability error.  In that case, we put
+// all defined bits into vbits8.
+static Bool set_vbits8_noregion ( Addr a, UChar vbits8 )
 {
    Bool  ok      = True;
    UChar vabits2 = get_vabits2(a);
@@ -807,28 +918,69 @@
    return ok;
 }
 
-// Returns False if there was an addressability error.  In that case, we put
-// all defined bits into vbits8.
+static void give_up_on_taint_region(int i) {
+   Addr a;
+   Addr end = taint_regions[i].start + taint_regions[i].size;
+   UChar exception_vbits[MAX_TAINT_EXCEPTIONS];
+   int j;
+
+   /*VG_(printf)("Destroying taint region %d\n", i);*/
+
+   for (j = 0; j < taint_regions[i].num_exceptions; j++) {
+      UChar vbits8;
+      get_vbits8_noregion(taint_regions[i].exceptions[j], &vbits8);
+      exception_vbits[j] = vbits8;
+   }
+
+   /* VG_(printf)("On destruction, tainting from 0x%08x to 0x%08x\n",
+      a = taint_regions[i].start, end); */
+   for (a = taint_regions[i].start; a < end; a++)
+      set_vbits8_noregion(a, 0xff);
+
+   for (j = 0; j < taint_regions[i].num_exceptions; j++) {
+      a = taint_regions[i].exceptions[j];
+      tl_assert(a >= taint_regions[i].start && a < end);
+      /* VG_(printf)("  except 0x%08x\n", a); */
+      set_vbits8_noregion(a, exception_vbits[j]);
+   }
+
+   taint_regions[i] = taint_regions[--num_taint_regions];
+}
+
+
+// Returns False if there was an addressability error.
 static INLINE
-Bool get_vbits8 ( Addr a, UChar* vbits8 )
+Bool set_vbits8 ( Addr a, UChar vbits8 )
 {
-   Bool  ok      = True;
-   UChar vabits2 = get_vabits2(a);
-
-   // Convert the in-memory format to in-register format.
-   if      ( VA_BITS2_DEFINED   == vabits2 ) { *vbits8 = V_BITS8_DEFINED;   }
-   else if ( VA_BITS2_UNDEFINED == vabits2 ) { *vbits8 = V_BITS8_UNDEFINED; }
-   else if ( VA_BITS2_NOACCESS  == vabits2 ) {
-      *vbits8 = V_BITS8_DEFINED;    // Make V bits defined!
-      ok = False;
-   } else {
-      tl_assert( VA_BITS2_PARTDEFINED == vabits2 );
-      *vbits8 = get_sec_vbits8(a);
+   int i;
+   for (i = 0; i < num_taint_regions; i++) {
+      /*VG_(printf)("Checking 0x%08x versus 0x%08x and 0x%08x\n",
+		  a, taint_regions[i].start,
+		  taint_regions[i].start + taint_regions[i].size);*/
+      if (a >= taint_regions[i].start &&
+	  a < taint_regions[i].start + taint_regions[i].size) {
+	 int j;
+	 for (j = 0; j < taint_regions[i].num_exceptions; j++)
+	    if (taint_regions[i].exceptions[j] == a) {
+	       /*VG_(printf)("0x%08x is already a taint exception\n", a);*/
+	       return set_vbits8_noregion(a, vbits8);
+	    }
+	 j = taint_regions[i].num_exceptions++;
+	 if (j == MAX_TAINT_EXCEPTIONS) {
+	    taint_regions[i].num_exceptions--;
+	    give_up_on_taint_region(i);
+	    return set_vbits8_noregion(a, vbits8);
+	 }
+	 /*VG_(printf)("As a new taint exception, 0x%08x is %02x\n", a, vbits8);*/
+	 taint_regions[i].exceptions[j] = a;
+	 return set_vbits8_noregion(a, vbits8);
+      }
    }
-   return ok;
+   return set_vbits8_noregion(a, vbits8);
 }
 
 
+
 /* --------------- Secondary V bit table ------------ */
 
 // This table holds the full V bit pattern for partially-defined bytes
@@ -1205,6 +1357,7 @@
 
 /* --------------- Load/store slow cases. --------------- */
 
+
 static
 __attribute__((noinline))
 void fc_LOADV_128_or_256_slow ( /*OUT*/ULong* res,
@@ -1327,7 +1480,7 @@
 ULong fc_LOADVn_slow ( Addr a, SizeT nBits, Bool bigendian )
 {
    PROF_EVENT(MCPE_LOADVN_SLOW);
-
+#if 0
    /* ------------ BEGIN semi-fast cases ------------ */
    /* These deal quickly-ish with the common auxiliary primary map
       cases on 64-bit platforms.  Are merely a speedup hack; can be
@@ -1336,6 +1489,7 @@
       folded out by compilers on 32-bit platforms.  These are derived
       from LOADV64 and LOADV32.
    */
+
    if (LIKELY(sizeof(void*) == 8 
                       && nBits == 64 && VG_IS_8_ALIGNED(a))) {
       SecMap* sm       = get_secmap_for_reading(a);
@@ -1359,6 +1513,7 @@
       /* else fall into slow case */
    }
    /* ------------ END semi-fast cases ------------ */
+#endif
 
    ULong  vbits64     = V_BITS64_UNDEFINED; /* result */
    ULong  pessim64    = V_BITS64_DEFINED;   /* only used when p-l-ok=yes */
@@ -1467,6 +1622,12 @@
    return vbits64;
 }
 
+static __inline__
+void make_aligned_word32_readable ( Addr aA )
+{
+   FC_(make_mem_defined)(aA, 4);
+}
+
 
 static
 __attribute__((noinline))
@@ -1479,7 +1640,7 @@
    Bool  ok;
 
    PROF_EVENT(MCPE_STOREVN_SLOW);
-
+#if 0
    /* ------------ BEGIN semi-fast cases ------------ */
    /* These deal quickly-ish with the common auxiliary primary map
       cases on 64-bit platforms.  Are merely a speedup hack; can be
@@ -1534,6 +1695,7 @@
       /* else fall into the slow case */
    }
    /* ------------ END semi-fast cases ------------ */
+#endif
 
    tl_assert(nBits == 64 || nBits == 32 || nBits == 16 || nBits == 8);
 
@@ -1553,6 +1715,19 @@
       FC_(record_address_error)( VG_(get_running_tid)(), a, szB, True );
 }
 
+void FC_(make_mem_accessible)(Addr a, SizeT len)
+{
+   SizeT i;
+   UChar vabits2;
+   DEBUG("make_mem_accessible(%p, %llu)\n", a, (ULong)len);
+   for (i = 0; i < len; i++) {
+      vabits2 = get_vabits2( a+i );
+      if (LIKELY(VA_BITS2_NOACCESS == vabits2)) {
+         set_vabits2(a+i, VA_BITS2_DEFINED);
+      }
+   }
+}
+
 
 /*------------------------------------------------------------*/
 /*--- Setting permissions over address ranges.             ---*/
@@ -1801,7 +1976,7 @@
       ocache_sarp_Clear_Origins ( a, len );
 }
 
-static void make_mem_undefined ( Addr a, SizeT len )
+void FC_(make_mem_undefined) ( Addr a, SizeT len )
 {
    PROF_EVENT(MCPE_MAKE_MEM_UNDEFINED);
    DEBUG("make_mem_undefined(%p, %lu)\n", a, len);
@@ -1833,12 +2008,14 @@
    FC_(make_mem_undefined_w_otag) ( a, len, ecu | okind );
 }
 
+__attribute__((unused))
 static
 void fc_new_mem_w_tid_make_ECU  ( Addr a, SizeT len, ThreadId tid )
 {
    make_mem_undefined_w_tid_and_okind ( a, len, tid, FC_OKIND_UNKNOWN );
 }
 
+__attribute__((unused))
 static
 void fc_new_mem_w_tid_no_ECU  ( Addr a, SizeT len, ThreadId tid )
 {
@@ -1854,7 +2031,7 @@
       ocache_sarp_Clear_Origins ( a, len );
 }
 
-__attribute__((unused))
+
 static void make_mem_defined_w_tid ( Addr a, SizeT len, ThreadId tid )
 {
    FC_(make_mem_defined)(a, len);
@@ -2619,7 +2796,7 @@
   PROF_EVENT(MCPE_MAKE_ALIGNED_WORD32_UNDEFINED);
 
 #ifndef PERF_FAST_STACK2
-   make_mem_undefined(a, 4);
+   FC_(make_mem_undefined)(a, 4);
 #else
    {
       UWord   sm_off;
@@ -2627,7 +2804,7 @@
 
       if (UNLIKELY(a > MAX_PRIMARY_ADDRESS)) {
          PROF_EVENT(MCPE_MAKE_ALIGNED_WORD32_UNDEFINED_SLOW);
-         make_mem_undefined(a, 4);
+         FC_(make_mem_undefined)(a, 4);
          return;
       }
 
@@ -2703,7 +2880,7 @@
    PROF_EVENT(MCPE_MAKE_ALIGNED_WORD64_UNDEFINED);
 
 #ifndef PERF_FAST_STACK2
-   make_mem_undefined(a, 8);
+   FC_(make_mem_undefined)(a, 8);
 #else
    {
       UWord   sm_off16;
@@ -2711,7 +2888,7 @@
 
       if (UNLIKELY(a > MAX_PRIMARY_ADDRESS)) {
          PROF_EVENT(MCPE_MAKE_ALIGNED_WORD64_UNDEFINED_SLOW);
-         make_mem_undefined(a, 8);
+         FC_(make_mem_undefined)(a, 8);
          return;
       }
 
@@ -2741,6 +2918,16 @@
    //// END inlined, specialised version of FC_(helperc_b_store8)
 }
 
+/* Nb: by "aligned" here we mean 8-byte aligned */
+static INLINE
+void make_aligned_word64_defined ( Addr a )
+{
+   PROF_EVENT(MCPE_MAKE_ALIGNED_WORD64_READABLE);
+
+   FC_(make_mem_defined)(a, 8);
+}
+
+
 static INLINE
 void make_aligned_word64_noaccess ( Addr a )
 {
@@ -2779,6 +2966,241 @@
 #endif
 }
 
+/*------------------------------------------------------------*/
+/*--- Counting secret bits                                 ---*/
+/*------------------------------------------------------------*/
+
+Long FC_(total_bits_leaked) = 0;
+int  FC_(enclosure_level) = 0;
+int  FC_(enclosure_mode) = 0;
+
+void FC_(add_to_leaked)(int num_to_add) {
+   FC_(total_bits_leaked) += num_to_add;
+   if (FC_(total_bits_leaked) > FC_(clo_max_bits_leaked)) {
+     tl_assert(0);
+   }
+}
+
+int FC_(count_tainted_bits)(Addr a, SizeT len) {
+   SizeT i;
+   SizeT count = 0;
+   UChar vbyte;
+   for (i = 0; i < len; i++) {
+      PROF_EVENT(MCPE_CHECK_READABLE_LOOP);
+      get_vbits8(a, &vbyte);
+      count += popcount8(vbyte);
+      a++;
+   }
+   return count;
+}
+
+/*------------------------------------------------------------*/
+/*--- Memory rollback support                              ---*/
+/*------------------------------------------------------------*/
+/* If 1, changes to memory are being recorded, so that they can be
+   rolled-back in the future. */
+int FC_(rollback_mode) = 0;
+
+int FC_(rollback_level) = 0;
+
+struct rollback_entry {
+   Addr location; /* this is the key for the OSet */
+   UChar old_value;
+   UChar old_vbits;
+   UChar escape;
+   UChar revert_quietly;
+   Addr written_at;
+};
+
+struct pending_escapee {
+   struct pending_escapee *next;
+   Addr location;
+   Addr end;
+};
+
+#define ENCLOSURE_DEPTH 100
+OSet *rollbacks[ENCLOSURE_DEPTH];
+struct pending_escapee *pending_escapees[ENCLOSURE_DEPTH];
+
+static void push_escapee_region(Addr start, SizeT size) {
+   struct pending_escapee *pend = VG_(malloc)("fc.per.1",sizeof(struct pending_escapee));
+   pend->location = start;
+   pend->end = start + size;
+   pend->next = pending_escapees[FC_(rollback_level)];
+   pending_escapees[FC_(rollback_level)] = pend;
+}
+
+static void activate_escapees(Bool tainted, ULong location) {
+   struct pending_escapee *pend = pending_escapees[FC_(rollback_level)];
+   while (pend) {
+      struct pending_escapee *next = pend->next;
+      SizeT size = pend->end - pend->location;
+      if (tainted) {
+	 add_taint_region(pend->location, size);
+	 if (FC_(clo_trace_secret_graph))
+	    trace_enclosed_output_region(pend->location, size, location);
+      }
+      VG_(free)(pend);
+      pend = next;
+   }
+   pending_escapees[FC_(rollback_level)] = 0;
+}
+
+static Addr quiet_writes_at;
+
+Bool FC_(is_escaping)(Addr addr) {
+   struct pending_escapee *pend = pending_escapees[FC_(rollback_level)];
+   for (; pend; pend = pend->next) {
+      if (addr >= pend->location && addr < pend->end)
+	 return True;
+   }
+   return False;
+}
+
+static void save_byte_for_rollback(Addr location, UChar old_value,
+				   UChar old_vbits) {
+   struct rollback_entry *node;
+   if (FC_(is_escaping)(location))
+      return;
+   if (VG_(OSetGen_Contains)(rollbacks[FC_(rollback_level)], &location))
+      return;
+   node = VG_(OSetGen_AllocNode)(rollbacks[FC_(rollback_level)],
+				 sizeof(struct rollback_entry));
+   node->location = location;
+   node->old_value = old_value;
+   node->old_vbits = old_vbits;
+   node->escape = 0;
+   node->written_at = VG_(get_IP)(VG_(get_running_tid)());
+   if (node->written_at == quiet_writes_at ||
+       node->written_at == quiet_writes_at + 3 ||
+       node->written_at == quiet_writes_at + 6 ||
+       node->written_at == quiet_writes_at + 12)
+      node->revert_quietly = 1;
+   else
+      node->revert_quietly = 0;      
+   VG_(OSetGen_Insert)(rollbacks[FC_(rollback_level)], node);
+}
+
+static void add_escaping_byte(Addr location) {
+   struct rollback_entry *node =
+      VG_(OSetGen_Lookup)(rollbacks[FC_(rollback_level)], &location);
+   if (!node) {
+      node = VG_(OSetGen_AllocNode)(rollbacks[FC_(rollback_level)],
+				    sizeof(struct rollback_entry));
+      node->location = location;
+      node->old_value = node->old_vbits = -1;
+      node->revert_quietly = 0;
+      VG_(OSetGen_Insert)(rollbacks[FC_(rollback_level)], node);
+   }
+   /*VG_(printf)("Adding escaping byte 0x%08x\n", location);*/
+   node->escape = 1;
+}
+
+static void add_escapee(Addr start, SizeT size) {
+   if (size < 10) {
+      int j;
+      for (j = 0; j < size; j++)
+	 add_escaping_byte(start + j);
+   } else {
+      push_escapee_region(start, size);
+   }
+}
+
+static void quiet_revert(Addr location) {
+   struct rollback_entry *node =
+      VG_(OSetGen_Lookup)(rollbacks[FC_(rollback_level)], &location);
+   if (node) {
+      node->revert_quietly = 1;
+   }
+}
+
+void VG_REGPARM(1) FC_(save_for_rollback8)(Addr location) {
+   UChar old_value = *(UChar *)location;
+   UChar old_vbits;
+   get_vbits8(location, &old_vbits);
+   save_byte_for_rollback(location, old_value, old_vbits);
+}
+
+void VG_REGPARM(1) FC_(save_for_rollback16)(Addr location) {
+   UChar old_value, old_vbits;
+   Int i;
+   for (i = 0; i < 2; i++) {
+      old_value = *(UChar *)(location + i);
+      get_vbits8(location + i, &old_vbits);
+      save_byte_for_rollback(location + i, old_value, old_vbits);
+   }
+}
+
+void VG_REGPARM(1) FC_(save_for_rollback32)(Addr location) {
+   UChar old_value, old_vbits;
+   Int i;
+   for (i = 0; i < 4; i++) {
+      old_value = *(UChar *)(location + i);
+      get_vbits8(location + i, &old_vbits);
+      save_byte_for_rollback(location + i, old_value, old_vbits);
+   }
+}
+
+void VG_REGPARM(1) FC_(save_for_rollback64)(Addr location) {
+   UChar old_value, old_vbits;
+   Int i;
+   for (i = 0; i < 8; i++) {
+      old_value = *(UChar *)(location + i);
+      get_vbits8(location + i, &old_vbits);
+      save_byte_for_rollback(location + i, old_value, old_vbits);
+   }
+}
+void VG_REGPARM(1) FC_(save_for_rollback128)(Addr location) {
+   UChar old_value, old_vbits;
+   Int i;
+   for (i = 0; i < 16; i++) {
+      old_value = *(UChar *)(location + i);
+      get_vbits8(location + i, &old_vbits);
+      save_byte_for_rollback(location + i, old_value, old_vbits);
+   }
+}
+
+static void do_rollback(Int tainted, Int ident, UWord *args, ULong location) {
+   struct rollback_entry *elem;
+   Int reverts = 0;
+   int escapee_count = 0;
+
+   VG_(OSetGen_ResetIter)(rollbacks[FC_(rollback_level)]);
+   while ((elem = VG_(OSetGen_Next)(rollbacks[FC_(rollback_level)]))) {
+      if (elem->escape) {
+	 if (tainted) {
+	    ULong loc;
+	    escapee_count++;
+	    set_vbits8(elem->location, 0xff);
+	    loc = location | ((escapee_count & 0xff) << 8);
+	    if (FC_(clo_trace_secret_graph))
+	       trace_enclosed_output(elem->location, loc);
+	 }
+      } else {
+	 UChar cur_value = *(UChar *)elem->location;
+	 if (cur_value != elem->old_value && !elem->revert_quietly) {
+	    if (FC_(clo_taint_reverts)) {
+	       set_vbits8(elem->location, cur_value ^ elem->old_value);
+	    }
+	    FC_(record_revert_error)(VG_(get_running_tid)(), elem->location,
+				ident, cur_value, elem->old_value,
+				elem->written_at);
+	    reverts++;
+	 }
+	 if (ident >= FC_(clo_revert_notbelow)) {
+	    *(UChar *)elem->location = elem->old_value;
+	    set_vbits8(elem->location, elem->old_vbits);
+	 }
+      }
+   }
+   
+   VG_(OSetGen_Destroy)(rollbacks[FC_(rollback_level)]);
+   rollbacks[FC_(rollback_level)] = 0;
+   if ((FC_(clo_trace_reverts) || FC_(clo_taint_reverts)) && reverts)
+      VG_(printf)("Rollback %d reverted %d bytes\n", ident, reverts);
+}
+
+
 
 /*------------------------------------------------------------*/
 /*--- Stack pointer adjustment                             ---*/
@@ -2811,7 +3233,7 @@
    if (VG_IS_4_ALIGNED( -VG_STACK_REDZONE_SZB + new_SP )) {
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 4 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 4 );
    }
 }
 
@@ -2853,7 +3275,7 @@
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP );
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP+4 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 8 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 8 );
    }
 }
 
@@ -2906,7 +3328,7 @@
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP );
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+4 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 12 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 12 );
    }
 }
 
@@ -2968,7 +3390,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+4  );
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP+12 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 16 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 16 );
    }
 }
 
@@ -3035,7 +3457,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+20 );
       make_aligned_word32_undefined ( -VG_STACK_REDZONE_SZB + new_SP+28 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 32 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 32 );
    }
 }
 
@@ -3109,7 +3531,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+96 );
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+104 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 112 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 112 );
    }
 }
 
@@ -3188,7 +3610,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+112 );
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+120 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 128 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 128 );
    }
 }
 
@@ -3273,7 +3695,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+128 );
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+136 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 144 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 144 );
    }
 }
 
@@ -3364,7 +3786,7 @@
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+144 );
       make_aligned_word64_undefined ( -VG_STACK_REDZONE_SZB + new_SP+152 );
    } else {
-      make_mem_undefined ( -VG_STACK_REDZONE_SZB + new_SP, 160 );
+      FC_(make_mem_undefined) ( -VG_STACK_REDZONE_SZB + new_SP, 160 );
    }
 }
 
@@ -3402,21 +3824,21 @@
 
 static void fc_new_mem_stack_w_ECU ( Addr a, SizeT len, UInt ecu )
 {
-   UInt otag = ecu | FC_OKIND_STACK;
+
    PROF_EVENT(MCPE_NEW_MEM_STACK);
-   FC_(make_mem_undefined_w_otag) ( -VG_STACK_REDZONE_SZB + a, len, otag );
+   FC_(make_mem_accessible)(-VG_STACK_REDZONE_SZB + a, len);
 }
 
 static void fc_new_mem_stack ( Addr a, SizeT len )
 {
    PROF_EVENT(MCPE_NEW_MEM_STACK);
-   make_mem_undefined ( -VG_STACK_REDZONE_SZB + a, len );
+   FC_(make_mem_accessible)(-VG_STACK_REDZONE_SZB + a, len);
 }
 
 static void fc_die_mem_stack ( Addr a, SizeT len )
 {
    PROF_EVENT(MCPE_DIE_MEM_STACK);
-   FC_(make_mem_noaccess) ( -VG_STACK_REDZONE_SZB + a, len );
+
 }
 
 
@@ -3762,7 +4184,7 @@
       make_aligned_word64_undefined(base + 112);
       make_aligned_word64_undefined(base + 120);
    } else {
-      make_mem_undefined(base, len);
+      FC_(make_mem_undefined)(base, len);
    }
 #  endif 
 
@@ -3869,7 +4291,7 @@
    }
 
    /* else fall into slow case */
-   make_mem_undefined(base, len);
+   FC_(make_mem_undefined)(base, len);
 }
 
 
@@ -3906,7 +4328,7 @@
       make_aligned_word64_undefined(base + 112);
       make_aligned_word64_undefined(base + 120);
    } else {
-      make_mem_undefined(base, 128);
+      FC_(make_mem_undefined)(base, 128);
    }
 #  endif 
 
@@ -3996,7 +4418,7 @@
 
    /* else fall into slow case */
    PROF_EVENT(MCPE_MAKE_STACK_UNINIT_128_NO_O_SLOWCASE);
-   make_mem_undefined(base, 128);
+   FC_(make_mem_undefined)(base, 128);
 }
 
 
@@ -4386,6 +4808,78 @@
 }
 
 
+static
+void fc_post_mem_write_secret(CorePart part, ThreadId tid, const HChar *name, Int fd,
+			      Addr a, SizeT len)
+{
+   Bool is_secret = False;
+   struct vg_stat stat_buf;
+   if (fd == 0 && FC_(clo_stdin_is_secret)) {
+      is_secret = True;
+   }
+   VG_(fstat)(fd, &stat_buf);
+   if (!(stat_buf.mode & 004) && FC_(clo_private_files_are_secret)) {
+      is_secret = True;
+   }
+   if ((stat_buf.rdev == 0x108 || stat_buf.rdev == 0x109) &&
+       FC_(clo_unpredictable_is_secret)) {
+      /* 0x108 is /dev/random, 0x109 is /dev/urandom. */
+      is_secret = True;
+   }
+   if (is_secret) {
+      if (len > 0)
+	 VG_(printf)("Read %lu secret bytes from %s\n", len, name);
+      FC_(make_mem_undefined)(a, len);
+      if (FC_(clo_trace_secret_graph)) {
+	 Addr eip = VG_(get_IP)(tid);
+	 make_freshly_tagged_input(a, len, (ULong)eip << 16);
+      }
+   } else {
+      FC_(make_mem_defined)(a, len);
+   }
+}
+
+static
+void fc_pre_syscall(ThreadId tid, UInt syscallno, UWord* args, UInt nArgs) {
+   /* do nothing */
+}
+
+static
+void fc_post_syscall(ThreadId tid, UInt syscallno, UWord* args, UInt nArgs ,SysRes res) {
+   if (FC_(clo_unpredictable_is_secret)) {
+      UWord mask = 0;
+      OffT offset;
+      int capacity;
+      if (syscallno == __NR_getpid) {
+	 VG_(printf)("Getpid returnning %d\n", (int)sr_Res(res));
+	 mask = 0xffff;
+      } else if (syscallno == __NR_set_tid_address) {
+	 VG_(printf)("Set_tid_address returnning %d\n", (int)sr_Res(res));
+	 mask = 0xffff;
+      } else if (syscallno == __NR_getppid) {
+	 VG_(printf)("Getppid returnning %d\n", (int)sr_Res(res));
+	 mask = 0xffff;
+      }
+      if (mask) {
+	 capacity = popcount64((ULong)mask);
+	 /* XXX fix offset for non-x86 platforms */
+#if defined(VGA_x86)
+#define OFFSET_x86_EAX 0
+	 offset = OFFSET_x86_EAX;
+#else
+#error Need offset of system call returns for this platform
+#endif
+	 VG_(set_shadow_regs_area)(tid, 1, offset, sizeof(UWord), (UChar*)&mask);
+	 if (FC_(clo_trace_secret_graph)) {
+	    Addr eip = VG_(get_IP)(tid);
+	    make_freshly_tagged_register(tid, offset, capacity,
+					 (ULong)eip << 16);
+	 }
+      }
+   }
+}
+
+
 /*------------------------------------------------------------*/
 /*--- Register event handlers                              ---*/
 /*------------------------------------------------------------*/
@@ -5652,56 +6146,73 @@
 /*------------------------------------------------------------*/
 
 /* Call these ones when an origin is available ... */
-VG_REGPARM(1)
-void FC_(helperc_value_check0_fail_w_o) ( UWord origin ) {
-   FC_(record_cond_error) ( VG_(get_running_tid)(), (UInt)origin );
+
+void FC_(helperc_value_check0_fail_w_o) (ULong tag, ULong location, UWord origin )
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 0, 0, 0, tag, location, (UInt)origin  );
 }
 
-VG_REGPARM(1)
-void FC_(helperc_value_check1_fail_w_o) ( UWord origin ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 1, (UInt)origin );
+void FC_(helperc_value_check1_fail_w_o) (UChar val, UChar vbits,
+						   ULong tag, ULong location, UWord origin )
+{
+   FC_(record_value_error) ( VG_(get_running_tid)(), 1, (UInt)val, (UInt)vbits,
+			   tag, location, (UInt)origin);
 }
 
-VG_REGPARM(1)
-void FC_(helperc_value_check4_fail_w_o) ( UWord origin ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 4, (UInt)origin );
+void FC_(helperc_value_check4_fail_w_o) (UInt val, UInt vbits,
+						   ULong tag, ULong location, UWord origin  )
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 4, val, vbits, tag,
+			  location, (UInt)origin);
 }
 
-VG_REGPARM(1)
-void FC_(helperc_value_check8_fail_w_o) ( UWord origin ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 8, (UInt)origin );
+void FC_(helperc_value_check8_fail_w_o) (ULong tag, ULong location, UWord origin )
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 8, 0, 0, tag, location, (UInt)origin);
 }
 
-VG_REGPARM(2) 
-void FC_(helperc_value_checkN_fail_w_o) ( HWord sz, UWord origin ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), (Int)sz, (UInt)origin );
+void FC_(helperc_value_checkN_fail_w_o) ( HWord sz,
+						 UInt val, UInt vbits,
+						 ULong tag, ULong location, UWord origin )
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), (Int)sz, val, vbits, tag,
+			  location, (UInt)origin);
 }
 
+
+
 /* ... and these when an origin isn't available. */
 
-VG_REGPARM(0)
-void FC_(helperc_value_check0_fail_no_o) ( void ) {
-   FC_(record_cond_error) ( VG_(get_running_tid)(), 0/*origin*/ );
+void FC_(helperc_value_check0_fail_no_o) (ULong tag, ULong location)
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 0, 0, 0, tag, location, 0/*origin*/  );
 }
 
-VG_REGPARM(0)
-void FC_(helperc_value_check1_fail_no_o) ( void ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 1, 0/*origin*/ );
+void FC_(helperc_value_check1_fail_no_o) (UChar val, UChar vbits,
+						   ULong tag, ULong location)
+{
+   FC_(record_value_error) ( VG_(get_running_tid)(), 1, (UInt)val, (UInt)vbits,
+			   tag, location, 0/*origin*/ );
 }
 
-VG_REGPARM(0)
-void FC_(helperc_value_check4_fail_no_o) ( void ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 4, 0/*origin*/ );
+void FC_(helperc_value_check4_fail_no_o) (UInt val, UInt vbits,
+						   ULong tag, ULong location )
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 4, val, vbits, tag,
+			  location, 0/*origin*/ );
 }
 
-VG_REGPARM(0)
-void FC_(helperc_value_check8_fail_no_o) ( void ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), 8, 0/*origin*/ );
+void FC_(helperc_value_check8_fail_no_o) (ULong tag, ULong location)
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), 8, 0, 0, tag, location, 0/*origin*/  );
 }
 
-VG_REGPARM(1) 
-void FC_(helperc_value_checkN_fail_no_o) ( HWord sz ) {
-   FC_(record_value_error) ( VG_(get_running_tid)(), (Int)sz, 0/*origin*/ );
+void FC_(helperc_value_checkN_fail_no_o) ( HWord sz,
+						 UInt val, UInt vbits,
+						 ULong tag, ULong location)
+{
+  FC_(record_value_error) ( VG_(get_running_tid)(), (Int)sz, val, vbits, tag,
+			  location, 0/*origin*/ );
 }
 
 
@@ -5977,6 +6488,22 @@
 Bool          FC_(clo_show_mismatched_frees)  = True;
 Bool          FC_(clo_expensive_definedness_checks) = False;
 
+
+Bool          FC_(clo_private_files_are_secret) = False;
+Bool          FC_(clo_stdin_is_secret)        = False;
+Bool          FC_(clo_unpredictable_is_secret) = False;
+Bool          FC_(clo_detailed_leak_report)   = False;
+Long          FC_(clo_max_bits_leaked)        = 9223372036854775807LL;
+Bool          FC_(clo_trace_secret_graph)     = False;
+Bool          FC_(clo_taint_all_enclosed)     = False;
+Bool          FC_(clo_trace_reverts)          = False;
+Bool          FC_(clo_taint_reverts)          = False;
+Int           FC_(clo_revert_notbelow)        = 0;
+Int           FC_(clo_folding_level)          = 10;
+Bool          FC_(clo_incremental)            = False;
+const char *  FC_(clo_graph_file)             = 0;
+const char *  FC_(clo_max_flow_program)       = 0;
+
 static const HChar * FC_(parse_leak_heuristics_tokens) =
    "-,stdstring,length64,newarray,multipleinheritance";
 /* The first heuristic value (LchNone) has no keyword, as this is
@@ -6125,6 +6652,24 @@
    else if VG_BOOL_CLO(arg, "--expensive-definedness-checks",
                        FC_(clo_expensive_definedness_checks)) {}
 
+   else if VG_BOOL_CLO(arg, "--private-files-are-secret",FC_(clo_private_files_are_secret)) {}
+   else if VG_BOOL_CLO(arg, "--stdin-is-secret",       FC_(clo_stdin_is_secret)) {}
+   else if VG_BOOL_CLO(arg, "--unpredictable-is-secret",FC_(clo_unpredictable_is_secret)) {}
+   else if VG_BOOL_CLO(arg, "--detailed-leak-report",  FC_(clo_detailed_leak_report)) {}
+   else if VG_BOOL_CLO(arg, "--trace-secret-graph",  FC_(clo_trace_secret_graph)) {}
+   else if VG_BOOL_CLO(arg, "--taint-all-enclosed",  FC_(clo_taint_all_enclosed)) {}
+   else if VG_BOOL_CLO(arg, "--trace-reverts",  FC_(clo_trace_reverts)) {}
+   else if VG_BOOL_CLO(arg, "--taint-reverts",  FC_(clo_taint_reverts)) {}
+
+   else if VG_BINT_CLO(arg, "--max-bits-leaked",  FC_(clo_max_bits_leaked), 0, 9223372036854775807LL) {}
+   else if VG_BINT_CLO(arg, "--revert-notbelow",  FC_(clo_revert_notbelow), 0, 1000000) {}
+   else if VG_BINT_CLO(arg, "--folding-level",  FC_(clo_folding_level), 0, 100) {}
+   else if VG_BOOL_CLO(arg, "--incremental", FC_(clo_incremental)) {}
+   else if  VG_STR_CLO(arg, "--graph-file", FC_(clo_graph_file)) {}
+   else if  VG_STR_CLO(arg, "--max-flow-program", FC_(clo_max_flow_program)) {}
+
+
+
    else
       return VG_(replacement_malloc_process_cmd_line_option)(arg);
 
@@ -6171,6 +6716,21 @@
 "        stack trace(s) to keep for malloc'd/free'd areas       [alloc-and-free]\n"
 "    --show-mismatched-frees=no|yes   show frees that don't match the allocator? [yes]\n"
    );
+
+   VG_(printf)(
+"    --private-files-are-secret=no|yes  taint bytes from non-004 files [no]\n"
+"    --stdin-is-secret=no|yes         taint bits read from standard input [no]\n"
+"    --unpredictable-is-secret=no|yes taint bits from getpid(), etc. [no]\n" 
+"    --detailed-leak-report=no|yes    report leaks like Memcheck errors [no]\n"
+"    --max-bits-leaked=<number>       abort after this many bits leaked [inf]\n"
+"    --trace-secret-graph=no|yes      build graph of secret operations [no]\n"
+"    --folding-level=<number>         collapse graph by location [10]\n"
+"    --graph-file=<file>              write secret graph to file [stderr]\n"
+"    --max-flow-program=<program>     program to run on graph [none]\n"
+"    --incremental=no|yes             compute graph flow every second [no]\n"
+"    --trace-reverts=no|yes           give error message on memory rollback [no]\n"
+"    --revert-notbelow=<number>       disable some rollbacks [0]\n"
+   );
 }
 
 static void fc_print_debug_usage(void)
@@ -6182,6 +6742,227 @@
 
 
 /*------------------------------------------------------------*/
+/*--- MD5 checksum implementation                          ---*/
+/*------------------------------------------------------------*/
+
+struct MD5Context {
+   UInt buf[4];
+   UInt bits[2];
+   unsigned char in[64];
+};
+
+static void MD5Init(struct MD5Context *ctx);
+static void MD5Update(struct MD5Context *ctx, unsigned char *buf,
+		      unsigned int len);
+static void MD5Final(unsigned char *digest, struct MD5Context *ctx);
+static void MD5Transform(UInt *buf, UInt *in);
+
+static void byteReverse(unsigned char *buf, unsigned longs)
+{
+    UInt t;
+    do {
+        t = (UInt) ((unsigned) buf[3] << 8 | buf[2]) << 16 |
+            ((unsigned) buf[1] << 8 | buf[0]);
+        *(UInt *) buf = t;
+        buf += 4;
+    } while (--longs);
+}
+
+static void MD5Init(struct MD5Context *ctx)
+{
+   ctx->buf[0] = 0x67452301;
+   ctx->buf[1] = 0xefcdab89;
+   ctx->buf[2] = 0x98badcfe;
+   ctx->buf[3] = 0x10325476;
+   
+   ctx->bits[0] = 0;
+   ctx->bits[1] = 0;
+}
+
+static void MD5Update(struct MD5Context *ctx, unsigned char *buf,
+		      unsigned int len)
+{
+   UInt t;
+   
+   /* Update bitcount */
+   
+   t = ctx->bits[0];
+   if ((ctx->bits[0] = t + ((UInt) len << 3)) < t)
+      ctx->bits[1]++;         /* Carry from low to high */
+   ctx->bits[1] += len >> 29;
+   
+   t = (t >> 3) & 0x3f;        /* Bytes already in shsInfo->data */
+   
+   /* Handle any leading odd-sized chunks */
+   
+   if (t) {
+      unsigned char *p = (unsigned char *) ctx->in + t;
+      
+      t = 64 - t;
+      if (len < t) {
+	 VG_(memcpy)(p, buf, len);
+	 return;
+      }
+      VG_(memcpy)(p, buf, t);
+      byteReverse(ctx->in, 16);
+      MD5Transform(ctx->buf, (UInt *) ctx->in);
+      buf += t;
+      len -= t;
+   }
+   /* Process data in 64-byte chunks */
+   
+   while (len >= 64) {
+      VG_(memcpy)(ctx->in, buf, 64);
+      byteReverse(ctx->in, 16);
+      MD5Transform(ctx->buf, (UInt *) ctx->in);
+      buf += 64;
+      len -= 64;
+   }
+   
+   /* Handle any remaining bytes of data. */
+   
+   VG_(memcpy)(ctx->in, buf, len);
+}
+
+static void MD5Final(unsigned char *digest, struct MD5Context *ctx)
+{
+   unsigned count;
+   unsigned char *p;
+   
+   /* Compute number of bytes mod 64 */
+   count = (ctx->bits[0] >> 3) & 0x3F;
+
+   /* Set the first char of padding to 0x80.  This is safe since there is
+      always at least one byte free */
+   p = ctx->in + count;
+   *p++ = 0x80;
+   
+   /* Bytes of padding needed to make 64 bytes */
+   count = 64 - 1 - count;
+
+   /* Pad out to 56 mod 64 */
+   if (count < 8) {
+      /* Two lots of padding:  Pad the first block to 64 bytes */
+      VG_(memset)(p, 0, count);
+      byteReverse(ctx->in, 16);
+      MD5Transform(ctx->buf, (UInt *) ctx->in);
+      
+      /* Now fill the next block with 56 bytes */
+      VG_(memset)(ctx->in, 0, 56);
+   } else {
+      /* Pad block to 56 bytes */
+      VG_(memset)(p, 0, count - 8);
+   }
+   byteReverse(ctx->in, 14);
+   
+   /* Append length in bits and transform */
+   ((UInt *) ctx->in)[14] = ctx->bits[0];
+   ((UInt *) ctx->in)[15] = ctx->bits[1];
+   
+   MD5Transform(ctx->buf, (UInt *) ctx->in);
+   byteReverse((unsigned char *) ctx->buf, 4);
+   VG_(memcpy)(digest, ctx->buf, 16);
+}
+
+#define F1(x, y, z) (z ^ (x & (y ^ z)))
+#define F2(x, y, z) F1(z, x, y)
+#define F3(x, y, z) (x ^ y ^ z)
+#define F4(x, y, z) (y ^ (x | ~z))
+#define MD5STEP(f, w, x, y, z, data, s) \
+        ( w += f(x, y, z) + data,  w = w<<s | w>>(32-s),  w += x )
+
+
+static void MD5Transform(UInt *buf, UInt *in)
+{
+   register UInt a, b, c, d;
+   
+   a = buf[0];
+   b = buf[1];
+   c = buf[2];
+   d = buf[3];
+
+   MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
+   MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
+   MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
+   MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
+   MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
+   MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
+   MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
+   MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
+   MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
+   MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
+   MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
+   MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
+   MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
+   MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
+   MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
+   MD5STEP(F1, b, c, d, a, in[15] + 0x49b40821, 22);
+
+   MD5STEP(F2, a, b, c, d, in[1] + 0xf61e2562, 5);
+   MD5STEP(F2, d, a, b, c, in[6] + 0xc040b340, 9);
+   MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
+   MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
+   MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
+   MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
+   MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
+   MD5STEP(F2, b, c, d, a, in[4] + 0xe7d3fbc8, 20);
+   MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
+   MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
+   MD5STEP(F2, c, d, a, b, in[3] + 0xf4d50d87, 14);
+   MD5STEP(F2, b, c, d, a, in[8] + 0x455a14ed, 20);
+   MD5STEP(F2, a, b, c, d, in[13] + 0xa9e3e905, 5);
+   MD5STEP(F2, d, a, b, c, in[2] + 0xfcefa3f8, 9);
+   MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
+   MD5STEP(F2, b, c, d, a, in[12] + 0x8d2a4c8a, 20);
+   
+   MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
+   MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
+   MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
+   MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
+   MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
+   MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
+   MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
+   MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
+   MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
+   MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
+   MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
+   MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
+   MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
+   MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
+   MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
+   MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);
+
+   MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
+   MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
+   MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
+   MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
+   MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
+   MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
+   MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
+   MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
+   MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
+   MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
+   MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
+   MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
+   MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
+   MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
+   MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
+   MD5STEP(F4, b, c, d, a, in[9] + 0xeb86d391, 21);
+
+   buf[0] += a;
+   buf[1] += b;
+   buf[2] += c;
+   buf[3] += d;
+}
+
+#undef F1
+#undef F2
+#undef F3
+#undef F4
+#undef MD5STEP
+
+
+/*------------------------------------------------------------*/
 /*--- Client blocks                                        ---*/
 /*------------------------------------------------------------*/
 
@@ -6796,10 +7577,54 @@
 /*--- Client requests                                      ---*/
 /*------------------------------------------------------------*/
 
+static int expected_ident[ENCLOSURE_DEPTH];
+static int expected_esp[ENCLOSURE_DEPTH];
+static int expected_ebp[ENCLOSURE_DEPTH];
+static int saved_mode[ENCLOSURE_DEPTH];
+
+static void leak_word(Addr addr, ULong location) {
+   UWord vword;
+   UChar vbytes[4];
+   int leak_count;
+   get_vbits8(addr,     &vbytes[0]);
+   get_vbits8(addr + 1, &vbytes[1]);
+   get_vbits8(addr + 2, &vbytes[2]);
+   get_vbits8(addr + 3, &vbytes[3]);
+   vword = vbytes[0] | vbytes[1] << 8 | vbytes[2] << 16
+      | vbytes[3] << 24;
+   leak_count = popcount32(vword);
+   if (leak_count) {
+      VG_(printf)("Explicit word leaked: %08x %08lx (%d bits) at %llx\n",
+		  *((unsigned int *)addr), vword, leak_count, location);
+      if (FC_(clo_detailed_leak_report)) {
+	 FC_(record_leaked_word)(addr);
+      }
+   }
+   FC_(add_to_leaked)(leak_count);
+}
+
+
 static Bool fc_handle_client_request ( ThreadId tid, UWord* arg, UWord* ret )
 {
    Int   i;
    Addr  bad_addr;
+   Addr  eip = VG_(get_IP)(tid);
+   ULong location = (ULong)eip << 16;
+
+   if (FC_(rollback_level) > 0) {
+      /* Don't warn when the memory writes that assigned the arguments
+	 to this client request get reverted. */
+      Addr a;
+      for (a = (Addr)arg; a < (Addr)arg + 6 * sizeof(UWord); a++)
+	 quiet_revert(a);
+   }
+   if (FC_(rollback_level) > 0 || arg[0] == VG_USERREQ__PREPARE_ROLLBACK) {
+      /* Don't warn when the memory write that saves the result of
+	 this client request gets reverted */
+      quiet_writes_at = VG_(get_IP)(VG_(get_running_tid)());
+      /*VG_(printf)("Silencing write at %x et seq\n", quiet_writes_at);*/
+   }
+
 
    if (!VG_IS_TOOL_USERREQ('F','C',arg[0])
        && VG_USERREQ__MALLOCLIKE_BLOCK != arg[0]
@@ -7114,6 +7939,170 @@
          return True;
       }
 
+
+      case VG_USERREQ__PUSH_ENCLOSE:
+	 {
+	    Addr esp = VG_(get_SP)(tid);
+	    Addr ebp = VG_(get_FP)(tid);
+	    expected_ident[FC_(enclosure_level)] = arg[1];	    
+	    expected_esp[FC_(enclosure_level)] = esp - arg[2];
+	    expected_ebp[FC_(enclosure_level)] = ebp - arg[3];
+	    saved_mode[FC_(enclosure_level)] = FC_(enclosure_mode);
+	    FC_(enclosure_level)++;
+	    tl_assert(FC_(enclosure_level) < ENCLOSURE_DEPTH);
+	    *ret = 0;
+	    break;
+	 }
+
+      case VG_USERREQ__POP_ENCLOSE:
+	 {
+	    Addr esp = VG_(get_SP)(tid);
+	    Addr ebp = VG_(get_FP)(tid);	    
+	    FC_(enclosure_level)--;
+	    tl_assert(FC_(enclosure_level) >= 0);
+	    tl_assert(expected_ident[FC_(enclosure_level)] == arg[1]);
+	    if (expected_esp[FC_(enclosure_level)] != esp) {
+	       VG_(printf)("ESP offset off by %lu\n",
+			   expected_esp[FC_(enclosure_level)] - esp);
+	       tl_assert(expected_esp[FC_(enclosure_level)] == esp);
+	    }
+	    if (expected_ebp[FC_(enclosure_level)] != ebp) {
+	       VG_(printf)("EBP offset off by %lu\n",
+			   expected_ebp[FC_(enclosure_level)] - ebp);
+	       tl_assert(expected_ebp[FC_(enclosure_level)] == ebp);
+	    }
+	    /* Sanitize %esp and %ebp (verified to be unchanged) */
+	    fc_post_reg_write(/*dummy*/0, tid, 16/*esp*/, 4);
+	    fc_post_reg_write(/*dummy*/0, tid, 20/*ebp*/, 4);
+	    FC_(enclosure_mode) = saved_mode[FC_(enclosure_level)];
+	    *ret = 0;
+	    break;
+	 }
+
+      case VG_USERREQ__LEAK_WORD:
+	 leak_word((Addr)arg[1], location);
+	 *ret = *((unsigned int *)arg[1]);
+	 break;
+
+      case VG_USERREQ__MAYBE_LEAK_WORD:
+	 *ret = *((unsigned int *)arg[1]);
+	 if (!FC_(clo_trace_secret_graph)) {
+	    Addr addr = (Addr)arg[1];
+	    leak_word(addr, location);
+	    set_vbits8(addr,     V_BITS8_DEFINED);
+	    set_vbits8(addr + 1, V_BITS8_DEFINED);
+	    set_vbits8(addr + 2, V_BITS8_DEFINED);
+	    set_vbits8(addr + 3, V_BITS8_DEFINED);	    
+	    *ret = 1;
+	 } else {
+	    *ret = 0;
+	 }
+	 break;
+	 
+      case VG_USERREQ__TAINT_WORD:
+	 {
+	    Addr addr = (Addr)arg[1];
+	    set_vbits8(addr,     V_BITS8_UNDEFINED);
+	    set_vbits8(addr + 1, V_BITS8_UNDEFINED);
+	    set_vbits8(addr + 2, V_BITS8_UNDEFINED);
+	    set_vbits8(addr + 3, V_BITS8_UNDEFINED);
+	    if (FC_(clo_trace_secret_graph))
+	       make_freshly_tagged_input(addr, 4, location);
+	    *ret = *((unsigned int *)arg[1]);
+	    break;
+	 }
+
+      case VG_USERREQ__UNTAINT_WORD:
+	 {
+	    Addr addr = (Addr)arg[1];
+	    set_vbits8(addr,     V_BITS8_DEFINED);
+	    set_vbits8(addr + 1, V_BITS8_DEFINED);
+	    set_vbits8(addr + 2, V_BITS8_DEFINED);
+	    set_vbits8(addr + 3, V_BITS8_DEFINED);
+	    *ret = *((unsigned int *)arg[1]);
+	    break;
+	 }
+	 
+      case VG_USERREQ__MD5SUM_BLOCK:
+	 {
+	    unsigned char *inptr = (unsigned char *)arg[1];
+	    SizeT inlen = (SizeT)arg[2];
+	    unsigned char *outptr = (unsigned char *)arg[3];
+	    struct MD5Context ctx;
+	    int num_bits = FC_(count_tainted_bits)((Addr)inptr, inlen);
+	    /* XXX check that args are legal pointers */
+	    MD5Init(&ctx);
+	    MD5Update(&ctx, inptr, inlen);
+	    MD5Final(outptr, &ctx);
+	    if (num_bits > 128)
+	       num_bits = 128;
+	    if (num_bits)
+	       VG_(printf)("Explicit leak of %d bits in MD5 checksum\n",
+			   num_bits);
+	    FC_(add_to_leaked)(num_bits);
+	    *ret = 0;
+	    break;
+	 }
+     
+      case VG_USERREQ__PREPARE_ROLLBACK:
+	 {
+	    Int ident = arg[1];
+	    Addr preserve = (Addr)arg[2];
+	    SizeT len = (SizeT)arg[3];
+	    (void)ident; /* Useful for debugging */
+	    leak_word((Addr)&arg[2], location);
+	    leak_word((Addr)&arg[3], location);
+	    FC_(rollback_level)++;
+	    tl_assert(!pending_escapees[FC_(rollback_level)]);
+	    /*VG_(printf)("Rollback level is now %d (push for %d)\n",
+	      FC_(rollback_level), ident);*/
+	    if (!rollbacks[FC_(rollback_level)])
+	       rollbacks[FC_(rollback_level)] 
+		  = VG_(OSetGen_Create)(offsetof(struct rollback_entry,
+						 location), 0,
+					VG_(malloc), "fc.urpr.1" ,VG_(free));
+	    tl_assert(FC_(rollback_level) < 1000000);
+	    tl_assert(len < 10000000);
+	    FC_(rollback_mode) = 1;
+	    add_escapee(preserve, len);
+	    break;
+	 }
+
+      case VG_USERREQ__PREPARE_ESCAPEE:
+	 {
+	    /* Like above, but only add another variable to escape. */
+	    Addr preserve = (Addr)arg[1];
+	    SizeT len = (SizeT)arg[2];
+	    leak_word((Addr)&arg[1], location);
+	    leak_word((Addr)&arg[2], location);
+	    tl_assert(rollbacks[FC_(rollback_level)]);
+	    add_escapee(preserve, len);
+	    break;
+	 }
+
+      case VG_USERREQ__DO_ROLLBACK:
+	 {
+	    int ident = arg[1];
+	    tl_assert(FC_(rollback_level) > 0);
+	    do_rollback(FC_(enclosure_mode), ident, arg, location);
+	    activate_escapees(FC_(enclosure_mode), location);
+	    FC_(rollback_level)--;
+	    /*VG_(printf)("Rollback level is now %d (pop for %d)\n",
+	      FC_(rollback_level), ident);*/
+	    if (FC_(rollback_level) == 0) {
+	       FC_(rollback_mode) = 0;
+	    }
+	    break;
+	 }
+
+      case VG_USERREQ__NOTE_ITERATION:
+	 {
+	    shadow_stack_iterate();
+	    break;
+	 }
+
+
+
       default:
          VG_(message)(
             Vg_UserMsg, 
@@ -7685,7 +8674,7 @@
       VG_(track_new_mem_stack_160_w_ECU) ( fc_new_mem_stack_160_w_ECU );
 #     endif
       VG_(track_new_mem_stack_w_ECU)     ( fc_new_mem_stack_w_ECU     );
-      VG_(track_new_mem_stack_signal)    ( fc_new_mem_w_tid_make_ECU );
+      VG_(track_new_mem_stack_signal)    ( make_mem_defined_w_tid );
    } else {
       /* Not doing origin tracking */
 #     ifdef PERF_FAST_STACK
@@ -7700,7 +8689,7 @@
       VG_(track_new_mem_stack_160) ( fc_new_mem_stack_160 );
 #     endif
       VG_(track_new_mem_stack)     ( fc_new_mem_stack     );
-      VG_(track_new_mem_stack_signal) ( fc_new_mem_w_tid_no_ECU );
+      VG_(track_new_mem_stack_signal) (make_mem_defined_w_tid);
    }
 
    // We assume that brk()/sbrk() does not initialise new memory.  Is this
@@ -7755,13 +8744,13 @@
    //
 #  if !defined(VGO_solaris)
    if (FC_(clo_fc_level) == 3)
-      VG_(track_new_mem_brk)         ( fc_new_mem_w_tid_make_ECU );
+      VG_(track_new_mem_brk)         ( make_mem_defined_w_tid );
    else
-      VG_(track_new_mem_brk)         ( fc_new_mem_w_tid_no_ECU );
+      VG_(track_new_mem_brk)         (make_mem_defined_w_tid);
 #  else
    // On Solaris, brk memory has to be marked as defined, otherwise we get
    // many false positives.
-   VG_(track_new_mem_brk)         ( make_mem_defined_w_tid );
+   VG_(track_new_mem_brk)         ( make_mem_defined_w_tid);
 #  endif
 
    /* This origin tracking cache is huge (~100M), so only initialise
@@ -7785,6 +8774,17 @@
    /* Do not check definedness of guest state if --undef-value-errors=no */
    if (FC_(clo_fc_level) >= 2)
       VG_(track_pre_reg_read) ( fc_pre_reg_read );
+
+
+   init_tag_memory();
+
+   /* Making VEX be more conservative about its optimizations makes it
+      easier for our call/return tracing to tell what's going on.
+      But enhancing it would make these unnecessary. */
+   VG_(clo_vex_control).iropt_unroll_thresh = 0;
+   VG_(clo_vex_control).guest_chase_thresh = 0;
+
+
 }
 
 static void print_SM_info(const HChar* type, Int n_SMs)
@@ -7963,6 +8963,22 @@
 
    done_prof_mem();
 
+   VG_(printf)("Leaked %lld bits total\n", FC_(total_bits_leaked));
+   if (FC_(clo_trace_secret_graph)) {
+      output_graph_stats();
+      if (!FC_(clo_incremental)) {
+	 if (FC_(clo_folding_level)) {
+	    if (FC_(clo_graph_file))
+	       output_folded_edges_filename(FC_(clo_graph_file));
+	    else
+	       output_folded_edges_stderr();
+	 } else if (FC_(clo_graph_file) && FC_(clo_max_flow_program)) {
+	    run_max_flow_program(FC_(clo_graph_file));
+	 }
+      }
+   }
+
+
    if (VG_(clo_stats))
       fc_print_stats();
 
@@ -7971,6 +8987,8 @@
         "------ Valgrind's client block stats follow ---------------\n" );
       show_client_block_stats();
    }
+
+
 }
 
 /* mark the given addr/len unaddressable for watchpoint implementation
@@ -7991,12 +9009,12 @@
 
 static void fc_pre_clo_init(void)
 {
-   VG_(details_name)            ("Memcheck");
-   VG_(details_version)         (NULL);
-   VG_(details_description)     ("a memory error detector");
+   VG_(details_name)            ("Flowcheck");
+   VG_(details_version)         ("1.20");
+   VG_(details_description)     ("a secret-leakage detector");
    VG_(details_copyright_author)(
       "Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.");
-   VG_(details_bug_reports_to)  (VG_BUGS_TO);
+   VG_(details_bug_reports_to)  ("mccamant@cs.umn.edu");
    VG_(details_avg_translation_sizeB) ( 640 );
 
    VG_(basic_tool_funcs)          (fc_post_clo_init,
@@ -8078,9 +9096,11 @@
    VG_(track_ban_mem_stack)       ( FC_(make_mem_noaccess) );
 
    VG_(track_pre_mem_read)        ( check_mem_is_defined );
+   VG_(track_pre_mem_read_secret) ( FC_(check_mem_is_defined_secret) );
    VG_(track_pre_mem_read_asciiz) ( check_mem_is_defined_asciiz );
    VG_(track_pre_mem_write)       ( check_mem_is_addressable );
    VG_(track_post_mem_write)      ( fc_post_mem_write );
+   VG_(track_post_mem_write_secret) ( fc_post_mem_write_secret );
 
    VG_(track_post_reg_write)                  ( fc_post_reg_write );
    VG_(track_post_reg_write_clientcall_return)( fc_post_reg_write_clientcall );
@@ -8091,6 +9111,7 @@
    }
 
    VG_(needs_watchpoint)          ( fc_mark_unaddressable_for_watchpoint );
+   VG_(needs_syscall_wrapper)(fc_pre_syscall, fc_post_syscall );
 
    init_shadow_memory();
    // FC_(chunk_poolalloc) must be allocated in post_clo_init
@@ -8142,9 +9163,6 @@
    tl_assert(MASK(4) == 0xFFFFFFF000000003ULL);
    tl_assert(MASK(8) == 0xFFFFFFF000000007ULL);
 #  endif
-
-   /* Check some assertions to do with the instrumentation machinery. */
-   FC_(do_instrumentation_startup_checks)();
 }
 
 STATIC_ASSERT(sizeof(UWord) == sizeof(SizeT));
diff -urN exp-flowcheck-base/fc_malloc_wrappers.c exp-flowcheck/fc_malloc_wrappers.c
--- exp-flowcheck-base/fc_malloc_wrappers.c	2016-08-30 16:12:49.924947524 -0500
+++ exp-flowcheck/fc_malloc_wrappers.c	2016-08-30 15:23:13.984145235 -0500
@@ -6,7 +6,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
@@ -369,9 +372,7 @@
    if (is_zeroed)
       FC_(make_mem_defined)( p, szB );
    else {
-      UInt ecu = VG_(get_ECU_from_ExeContext)(FC_(allocated_at)(mc));
-      tl_assert(VG_(is_plausible_ECU)(ecu));
-      FC_(make_mem_undefined_w_otag)( p, szB, ecu | FC_OKIND_HEAP );
+      FC_(make_mem_accessible)( p, szB );
    }
 
    return (void*)p;
@@ -574,8 +575,6 @@
          VG_(memcpy)((void*)a_new, p_old, new_szB);
       } else {
          /* new size is bigger */
-         UInt        ecu;
-
          /* Copy address range state and value from old to new */
          FC_(copy_address_range_state) ( (Addr)p_old, a_new, old_szB );
          VG_(memcpy)((void*)a_new, p_old, old_szB);
@@ -583,11 +582,8 @@
          // If the block has grown, we mark the grown area as undefined.
          // We have to do that after VG_(HT_add_node) to ensure the ecu
          // execontext is for a fully allocated block.
-         ecu = VG_(get_ECU_from_ExeContext)(FC_(allocated_at)(new_mc));
-         tl_assert(VG_(is_plausible_ECU)(ecu));
-         FC_(make_mem_undefined_w_otag)( a_new+old_szB,
-                                         new_szB-old_szB,
-                                         ecu | FC_OKIND_HEAP );
+         FC_(make_mem_accessible)( a_new+old_szB,
+                                         new_szB-old_szB);
 
          /* Possibly fill new area with specified junk */
          if (FC_(clo_malloc_fill) != -1) {
diff -urN exp-flowcheck-base/fc_replace_strmem.c exp-flowcheck/fc_replace_strmem.c
--- exp-flowcheck-base/fc_replace_strmem.c	2016-08-30 16:12:49.924947524 -0500
+++ exp-flowcheck/fc_replace_strmem.c	2016-08-30 15:23:13.987292550 -0500
@@ -7,7 +7,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward
       jseward@acm.org
diff -urN exp-flowcheck-base/fc_translate.c exp-flowcheck/fc_translate.c
--- exp-flowcheck-base/fc_translate.c	2016-08-30 16:12:49.944947853 -0500
+++ exp-flowcheck/fc_translate.c	2016-08-30 15:23:13.995525944 -0500
@@ -6,7 +6,10 @@
 
 /*
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward 
       jseward@acm.org
@@ -41,7 +44,8 @@
 #include "pub_tool_libcbase.h"
 
 #include "fc_include.h"
-
+#include "fc_translate.h"
+#include "trace_translate.h"
 
 /* FIXMEs JRS 2011-June-16.
 
@@ -135,12 +139,13 @@
 /*--- Forward decls                                        ---*/
 /*------------------------------------------------------------*/
 
-struct _FCEnv;
+
 
 static IRType  shadowTypeV ( IRType ty );
-static IRExpr* expr2vbits ( struct _FCEnv* fce, IRExpr* e );
+
 static IRTemp  findShadowTmpB ( struct _FCEnv* fce, IRTemp orig );
 
+
 static IRExpr *i128_const_zero(void);
 
 /*------------------------------------------------------------*/
@@ -160,9 +165,12 @@
    When .kind is VSh or BSh then the tmp is holds a V- or B- value,
    and so .shadowV and .shadowB must be IRTemp_INVALID, since it is
    illogical for a shadow tmp itself to be shadowed.
+
+   TSh, analogous to VSh and BSh, is added for the graph tracing node
+   temporaries used in trace_translate.c.
 */
 typedef
-   enum { Orig=1, VSh=2, BSh=3 }
+   enum { Orig=1, VSh=2, BSh=3, TSh=4 }
    TempKind;
 
 typedef
@@ -170,54 +178,27 @@
       TempKind kind;
       IRTemp   shadowV;
       IRTemp   shadowB;
+      IRTemp   shadowT;
    }
    TempMapEnt;
 
 
-/* Carries around state during flowcheck instrumentation. */
-typedef
-   struct _FCEnv {
-      /* MODIFIED: the superblock being constructed.  IRStmts are
-         added. */
-      IRSB* sb;
-      Bool  trace;
-
-      /* MODIFIED: a table [0 .. #temps_in_sb-1] which gives the
-         current kind and possibly shadow temps for each temp in the
-         IRSB being constructed.  Note that it does not contain the
-         type of each tmp.  If you want to know the type, look at the
-         relevant entry in sb->tyenv.  It follows that at all times
-         during the instrumentation process, the valid indices for
-         tmpMap and sb->tyenv are identical, being 0 .. N-1 where N is
-         total number of Orig, V- and B- temps allocated so far.
-
-         The reason for this strange split (types in one place, all
-         other info in another) is that we need the types to be
-         attached to sb so as to make it possible to do
-         "typeOfIRExpr(fce->bb->tyenv, ...)" at various places in the
-         instrumentation process. */
-      XArray* /* of TempMapEnt */ tmpMap;
-
-      /* MODIFIED: indicates whether "bogus" literals have so far been
-         found.  Starts off False, and may change to True. */
-      Bool bogusLiterals;
-
-      /* READONLY: indicates whether we should use expensive
-         interpretations of integer adds, since unfortunately LLVM
-         uses them to do ORs in some circumstances.  Defaulted to True
-         on MacOS and False everywhere else. */
-      Bool useLLVMworkarounds;
-
-      /* READONLY: the guest layout.  This indicates which parts of
-         the guest state should be regarded as 'always defined'. */
-      const VexGuestLayout* layout;
-
-      /* READONLY: the host word type.  Needed for constructing
-         arguments of type 'HWord' to be passed to helper functions.
-         Ity_I32 or Ity_I64 only. */
-      IRType hWordTy;
-   }
-   FCEnv;
+/* These are computed based on fce->layout, but we keep them as
+   globals rather than in the FCEnv because some other code (e.g.,
+   make_freshly_tagged_register) needs tags_guest_offset. At the
+   moment, because the secrecy bit (Memcheck-V-bit-like) shadows have
+   the same size as the registers they shadow, these two offsets will
+   happen be the same, but better not to depend on that. */
+
+/* The offset of the shadow area, within the guest state. */
+Int FC_(shadow_guest_offset) = 0;
+
+/* The offset of the shadow area for tags, within the shadow area. */
+Int FC_(tags_guest_offset) = 0;
+
+
+/* The definition of FCEnv has moved to fc_translate.h, so that
+   it can be used by trace_translate.c as well. */
 
 /* SHADOW TMP MANAGEMENT.  Shadow tmps are allocated lazily (on
    demand), as they are encountered.  This is for two reasons.
@@ -255,6 +236,7 @@
    ent.kind    = kind;
    ent.shadowV = IRTemp_INVALID;
    ent.shadowB = IRTemp_INVALID;
+   ent.shadowT = IRTemp_INVALID;
    newIx = VG_(addToXA)( fce->tmpMap, &ent );
    tl_assert(newIx == (Word)tmp);
    return tmp;
@@ -263,7 +245,7 @@
 
 /* Find the tmp currently shadowing the given original tmp.  If none
    so far exists, allocate one.  */
-static IRTemp findShadowTmpV ( FCEnv* fce, IRTemp orig )
+IRTemp findShadowTmpV ( FCEnv* fce, IRTemp orig )
 {
    TempMapEnt* ent;
    /* VG_(indexXA) range-checks 'orig', hence no need to check
@@ -283,6 +265,31 @@
    return ent->shadowV;
 }
 
+/* Almost identical to findShadowTmpB. */
+IRTemp findShadowTmpT ( FCEnv* fce, IRTemp orig )
+{
+   TempMapEnt* ent;
+   /* VG_(indexXA) range-checks 'orig', hence no need to check
+      here. */
+   ent = (TempMapEnt*)VG_(indexXA)( fce->tmpMap, (Word)orig );
+   tl_assert(ent->kind == Orig);
+   if (ent->shadowT == IRTemp_INVALID) {
+      IRTemp tmpT
+        = newTemp( fce, Ity_I64, TSh );
+      /* newTemp may cause fce->tmpMap to resize, hence previous results
+         from VG_(indexXA) are invalid. */
+      ent = (TempMapEnt*)VG_(indexXA)( fce->tmpMap, (Word)orig );
+      tl_assert(ent->kind == Orig);
+      tl_assert(ent->shadowT == IRTemp_INVALID);
+      ent->shadowT = tmpT;
+   }
+   return ent->shadowT;
+}
+
+IRTemp newTempT( FCEnv* fce ) {
+   return newTemp(fce, Ity_I64, TSh);
+}
+
 /* Allocate a new shadow for the given original tmp.  This means any
    previous shadow is abandoned.  This is needed because it is
    necessary to give a new value to a shadow once it has been tested
@@ -323,7 +330,7 @@
    by constants, and temps are shadowed by the corresponding shadow
    temporary. */
 
-typedef  IRExpr  IRAtom;
+
 
 /* (used for sanity checks only): is this an atom which looks
    like it's from original code? */
@@ -340,13 +347,13 @@
 
 /* (used for sanity checks only): is this an atom which looks
    like it's from shadow code? */
-static Bool isShadowAtom ( FCEnv* fce, IRAtom* a1 )
+Bool isShadowAtom ( FCEnv* fce, IRAtom* a1 )
 {
    if (a1->tag == Iex_Const)
       return True;
    if (a1->tag == Iex_RdTmp) {
       TempMapEnt* ent = VG_(indexXA)( fce->tmpMap, a1->Iex.RdTmp.tmp );
-      return ent->kind == VSh || ent->kind == BSh;
+      return ent->kind == VSh || ent->kind == BSh || ent->kind == TSh;
    }
    return False;
 }
@@ -411,6 +418,19 @@
    }
 }
 
+/* Produce a 'undefined' value of the given shadow type.  Should only be
+   supplied shadow types (Bit/I8/I16/I32/UI64). */
+static IRExpr* undefinedOfType ( IRType ty ) {
+   switch (ty) {
+      case Ity_I1:   return IRExpr_Const(IRConst_U1(True));
+      case Ity_I8:   return IRExpr_Const(IRConst_U8((UChar)-1));
+      case Ity_I16:  return IRExpr_Const(IRConst_U16((UShort)-1));
+      case Ity_I32:  return IRExpr_Const(IRConst_U32((UInt)-1));
+      case Ity_I64:  return IRExpr_Const(IRConst_U64((ULong)-1));
+      case Ity_V128: return IRExpr_Const(IRConst_V128(0xffff));
+      default:      VG_(tool_panic)("flowcheck:undefinedOfType");
+   }
+}
 
 /*------------------------------------------------------------*/
 /*--- Constructing IR fragments                            ---*/
@@ -453,7 +473,7 @@
    needs to be.  But passing it in is redundant, since we can deduce
    the type merely by inspecting 'e'.  So at least use that fact to
    assert that the two types agree. */
-static IRAtom* assignNew ( HChar cat, FCEnv* fce, IRType ty, IRExpr* e )
+IRAtom* assignNew ( HChar cat, FCEnv* fce, IRType ty, IRExpr* e )
 {
    TempKind k;
    IRTemp   t;
@@ -463,6 +483,7 @@
    switch (cat) {
       case 'V': k = VSh;  break;
       case 'B': k = BSh;  break;
+      case 'T': k = TSh;  break;
       case 'C': k = Orig; break; 
                 /* happens when we are making up new "orig"
                    expressions, for IRCAS handling */
@@ -629,7 +650,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_I8, binop(Iop_Or8, data, vbits));
 }
 
@@ -637,7 +658,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_I16, binop(Iop_Or16, data, vbits));
 }
 
@@ -645,7 +666,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_I32, binop(Iop_Or32, data, vbits));
 }
 
@@ -653,7 +674,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_I64, binop(Iop_Or64, data, vbits));
 }
 
@@ -661,7 +682,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_V128, binop(Iop_OrV128, data, vbits));
 }
 
@@ -669,7 +690,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew('V', fce, Ity_V256, binop(Iop_OrV256, data, vbits));
 }
 
@@ -680,7 +701,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_I8, 
              binop(Iop_Or8, 
@@ -692,7 +713,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_I16, 
              binop(Iop_Or16, 
@@ -704,7 +725,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_I32, 
              binop(Iop_Or32, 
@@ -716,7 +737,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_I64, 
              binop(Iop_Or64, 
@@ -728,7 +749,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_V128, 
              binop(Iop_OrV128, 
@@ -740,7 +761,7 @@
 {
    tl_assert(isOriginalAtom(fce, data));
    tl_assert(isShadowAtom(fce, vbits));
-   tl_assert(sameKindedAtoms(data, vbits));
+  /* tl_assert(sameKindedAtoms(data, vbits));*/
    return assignNew(
              'V', fce, Ity_V256, 
              binop(Iop_OrV256, 
@@ -993,8 +1014,8 @@
    tl_assert(isShadowAtom(fce,vyy));
    tl_assert(isOriginalAtom(fce,xx));
    tl_assert(isOriginalAtom(fce,yy));
-   tl_assert(sameKindedAtoms(vxx,xx));
-   tl_assert(sameKindedAtoms(vyy,yy));
+ /*  tl_assert(sameKindedAtoms(vxx,xx));
+   tl_assert(sameKindedAtoms(vyy,yy)); */
  
    switch (ty) {
       case Ity_I16:
@@ -1124,8 +1145,8 @@
    tl_assert(isShadowAtom(fce,yyhash));
    tl_assert(isOriginalAtom(fce,xx));
    tl_assert(isOriginalAtom(fce,yy));
-   tl_assert(sameKindedAtoms(xxhash,xx));
-   tl_assert(sameKindedAtoms(yyhash,yy));
+ /*  tl_assert(sameKindedAtoms(xxhash,xx));
+   tl_assert(sameKindedAtoms(yyhash,yy));  */
    tl_assert(cmp_op == Iop_CmpORD32S || cmp_op == Iop_CmpORD32U
              || cmp_op == Iop_CmpORD64S || cmp_op == Iop_CmpORD64U);
 
@@ -1222,6 +1243,7 @@
 static void complainIfUndefined ( FCEnv* fce, IRAtom* atom, IRExpr *guard )
 {
    IRAtom*  vatom;
+   IRAtom*  tatom;
    IRType   ty;
    Int      sz;
    IRDirty* di;
@@ -1246,10 +1268,15 @@
    tl_assert(isOriginalAtom(fce, atom));
    vatom = expr2vbits( fce, atom );
    tl_assert(isShadowAtom(fce, vatom));
-   tl_assert(sameKindedAtoms(atom, vatom));
+/*   tl_assert(sameKindedAtoms(atom, vatom)); */
 
    ty = typeOfIRExpr(fce->sb->tyenv, vatom);
 
+   if (FC_(clo_trace_secret_graph))
+      tatom = expr2tag_trace(fce, fce->trace_env, atom);
+   else
+      tatom = IRExpr_Const(IRConst_U64(0));
+
    /* sz is only used for constructing the error message */
    sz = ty==Ity_I1 ? 0 : sizeofIRType(ty);
 
@@ -1278,52 +1305,56 @@
          if (origin) {
             fn    = &FC_(helperc_value_check0_fail_w_o);
             nm    = "FC_(helperc_value_check0_fail_w_o)";
-            args  = mkIRExprVec_1(origin);
-            nargs = 1;
+            args  = mkIRExprVec_3(tatom, mkU64(fce->trace_env->location), origin);
+            nargs = 3;
          } else {
             fn    = &FC_(helperc_value_check0_fail_no_o);
             nm    = "FC_(helperc_value_check0_fail_no_o)";
-            args  = mkIRExprVec_0();
-            nargs = 0;
+            args  = mkIRExprVec_2(tatom, mkU64(fce->trace_env->location));
+            nargs = 2;
          }
          break;
       case 1:
          if (origin) {
             fn    = &FC_(helperc_value_check1_fail_w_o);
             nm    = "FC_(helperc_value_check1_fail_w_o)";
-            args  = mkIRExprVec_1(origin);
-            nargs = 1;
+            args  = mkIRExprVec_5(atom, vatom, tatom,
+			       mkU64(fce->trace_env->location), origin);
+            nargs = 5;
          } else {
             fn    = &FC_(helperc_value_check1_fail_no_o);
             nm    = "FC_(helperc_value_check1_fail_no_o)";
-            args  = mkIRExprVec_0();
-            nargs = 0;
+            args  = mkIRExprVec_4(atom, vatom, tatom,
+			       mkU64(fce->trace_env->location));
+            nargs = 4;
          }
          break;
       case 4:
          if (origin) {
             fn    = &FC_(helperc_value_check4_fail_w_o);
             nm    = "FC_(helperc_value_check4_fail_w_o)";
-            args  = mkIRExprVec_1(origin);
-            nargs = 1;
+            args  = mkIRExprVec_5(atom, vatom, tatom,
+			       mkU64(fce->trace_env->location), origin);
+            nargs = 5;
          } else {
             fn    = &FC_(helperc_value_check4_fail_no_o);
             nm    = "FC_(helperc_value_check4_fail_no_o)";
-            args  = mkIRExprVec_0();
-            nargs = 0;
+            args  = mkIRExprVec_4(atom, vatom, tatom,
+			       mkU64(fce->trace_env->location));
+            nargs = 4;
          }
          break;
       case 8:
          if (origin) {
             fn    = &FC_(helperc_value_check8_fail_w_o);
             nm    = "FC_(helperc_value_check8_fail_w_o)";
-            args  = mkIRExprVec_1(origin);
-            nargs = 1;
+            args  = mkIRExprVec_3(tatom, mkU64(fce->trace_env->location), origin);
+            nargs = 3;
          } else {
             fn    = &FC_(helperc_value_check8_fail_no_o);
             nm    = "FC_(helperc_value_check8_fail_no_o)";
-            args  = mkIRExprVec_0();
-            nargs = 0;
+            args  = mkIRExprVec_2(tatom, mkU64(fce->trace_env->location));
+            nargs = 2;
          }
          break;
       case 2:
@@ -1331,13 +1362,15 @@
          if (origin) {
             fn    = &FC_(helperc_value_checkN_fail_w_o);
             nm    = "FC_(helperc_value_checkN_fail_w_o)";
-            args  = mkIRExprVec_2( mkIRExpr_HWord( sz ), origin);
-            nargs = 2;
+            args  = mkIRExprVec_6( mkIRExpr_HWord( sz ), atom, vatom, tatom,
+				mkU64(fce->trace_env->location), origin);
+            nargs = 6;
          } else {
             fn    = &FC_(helperc_value_checkN_fail_no_o);
             nm    = "FC_(helperc_value_checkN_fail_no_o)";
-            args  = mkIRExprVec_1( mkIRExpr_HWord( sz ) );
-            nargs = 1;
+            args  = mkIRExprVec_5(  mkIRExpr_HWord( sz ), atom, vatom, tatom,
+				mkU64(fce->trace_env->location) );
+            nargs = 5;
          }
          break;
       default:
@@ -1347,11 +1380,12 @@
    tl_assert(fn);
    tl_assert(nm);
    tl_assert(args);
-   tl_assert(nargs >= 0 && nargs <= 2);
+//   tl_assert(nargs >= 0 && nargs <= 2);
+   (void) nargs;
    tl_assert( (FC_(clo_fc_level) == 3 && origin != NULL)
               || (FC_(clo_fc_level) == 2 && origin == NULL) );
 
-   di = unsafeIRDirty_0_N( nargs/*regparms*/, nm, 
+   di = unsafeIRDirty_0_N( 0/*regparms*/, nm, 
                            VG_(fnptr_to_fnentry)( fn ), args );
    di->guard = cond; // and cond is PCast-to-1(atom#)
 
@@ -1374,8 +1408,7 @@
       a new value. */
    tl_assert(isIRAtom(vatom));
    /* sameKindedAtoms ... */
-   if (vatom->tag == Iex_RdTmp) {
-      tl_assert(atom->tag == Iex_RdTmp);
+   if (atom->tag == Iex_RdTmp) {
       if (guard == NULL) {
          // guard is 'always True', hence update unconditionally
          newShadowTmpV(fce, atom->Iex.RdTmp.tmp);
@@ -1461,6 +1494,7 @@
 
    ty = typeOfIRExpr(fce->sb->tyenv, vatom);
    tl_assert(ty != Ity_I1);
+   tl_assert(ty != Ity_I128);
    if (isAlwaysDefd(fce, offset, sizeofIRType(ty))) {
       /* later: no ... */
       /* emit code to emit a complaint if any of the vbits are 1. */
@@ -1504,7 +1538,7 @@
    
    tl_assert(isOriginalAtom(fce,atom));
    vatom = expr2vbits( fce, atom );
-   tl_assert(sameKindedAtoms(atom, vatom));
+   /* tl_assert(sameKindedAtoms(atom, vatom));*/
    ty   = descr->elemTy;
    tyS  = shadowTypeV(ty);
    arrSize = descr->nElems * sizeofIRType(ty);
@@ -1782,25 +1816,8 @@
       operation.  Here are some special cases which use PCast only
       twice rather than three times. */
 
-   /* Standard FP idiom: rm x FParg1 x FParg2 x FParg3 -> FPresult */
-
-   if (t1 == Ity_I32 && t2 == Ity_I128 && t3 == Ity_I128 && t4 == Ity_I128
-       && finalVty == Ity_I128) {
-      if (0) VG_(printf)("mkLazy4: I32 x I128 x I128 x I128 -> I128\n");
-      /* Widen 1st arg to I128.  Since 1st arg is typically a rounding
-         mode indication which is fully defined, this should get
-         folded out later. */
-      at = mkPCastTo(fce, Ity_I128, va1);
-      /* Now fold in 2nd, 3rd, 4th args. */
-      at = mkUifU(fce, Ity_I128, at, va2);
-      at = mkUifU(fce, Ity_I128, at, va3);
-      at = mkUifU(fce, Ity_I128, at, va4);
-      /* and PCast once again. */
-      at = mkPCastTo(fce, Ity_I128, at);
-      return at;
-   }
-
    /* I32 x I64 x I64 x I64 -> I64 */
+   /* Standard FP idiom: rm x FParg1 x FParg2 x FParg3 -> FPresult */
    if (t1 == Ity_I32 && t2 == Ity_I64 && t3 == Ity_I64 && t4 == Ity_I64
        && finalVty == Ity_I64) {
       if (0) VG_(printf)("mkLazy4: I32 x I64 x I64 x I64 -> I64\n");
@@ -1919,8 +1936,8 @@
    tl_assert(isShadowAtom(fce,qbb));
    tl_assert(isOriginalAtom(fce,aa));
    tl_assert(isOriginalAtom(fce,bb));
-   tl_assert(sameKindedAtoms(qaa,aa));
-   tl_assert(sameKindedAtoms(qbb,bb));
+  /* tl_assert(sameKindedAtoms(qaa,aa));
+   tl_assert(sameKindedAtoms(qbb,bb)); */
 
    switch (ty) {
       case Ity_I32:
@@ -2079,8 +2096,8 @@
    tl_assert(isShadowAtom(fce,qbb));
    tl_assert(isOriginalAtom(fce,aa));
    tl_assert(isOriginalAtom(fce,bb));
-   tl_assert(sameKindedAtoms(qaa,aa));
-   tl_assert(sameKindedAtoms(qbb,bb));
+  /* tl_assert(sameKindedAtoms(qaa,aa));
+   tl_assert(sameKindedAtoms(qbb,bb)); */
    return 
       assignNew(
          'V', fce, ty,
@@ -2526,7 +2543,6 @@
       case Iop_QNarrowUn32Uto16Ux4:
       case Iop_QNarrowUn32Sto16Sx4:
       case Iop_QNarrowUn32Sto16Ux4:
-      case Iop_F32toF16x4:
          return Iop_NarrowUn32to16x4;
       case Iop_QNarrowUn16Uto8Ux8:
       case Iop_QNarrowUn16Sto8Sx8:
@@ -2598,7 +2614,6 @@
       case Iop_NarrowUn16to8x8:
       case Iop_NarrowUn32to16x4:
       case Iop_NarrowUn64to32x2:
-      case Iop_F32toF16x4:
          at1 = assignNew('V', fce, Ity_I64, unop(narrow_op, vatom1));
          return at1;
       default:
@@ -2637,7 +2652,6 @@
       case Iop_Widen16Sto32x4: pcast = mkPCast32x4; break;
       case Iop_Widen32Uto64x2: pcast = mkPCast64x2; break;
       case Iop_Widen32Sto64x2: pcast = mkPCast64x2; break;
-      case Iop_F16toF32x4:     pcast = mkPCast32x4; break;
       default: VG_(tool_panic)("vectorWidenI64");
    }
    tl_assert(isShadowAtom(fce,vatom1));
@@ -2809,10 +2823,10 @@
    tl_assert(isShadowAtom(fce,vatom2));
    tl_assert(isShadowAtom(fce,vatom3));
    tl_assert(isShadowAtom(fce,vatom4));
-   tl_assert(sameKindedAtoms(atom1,vatom1));
+  /* tl_assert(sameKindedAtoms(atom1,vatom1)); 
    tl_assert(sameKindedAtoms(atom2,vatom2));
    tl_assert(sameKindedAtoms(atom3,vatom3));
-   tl_assert(sameKindedAtoms(atom4,vatom4));
+   tl_assert(sameKindedAtoms(atom4,vatom4));*/
    switch (op) {
       case Iop_MAddF64:
       case Iop_MAddF64r32:
@@ -2826,13 +2840,6 @@
          /* I32(rm) x F32 x F32 x F32 -> F32 */
          return mkLazy4(fce, Ity_I32, vatom1, vatom2, vatom3, vatom4);
 
-      case Iop_MAddF128:
-      case Iop_MSubF128:
-      case Iop_NegMAddF128:
-      case Iop_NegMSubF128:
-         /* I32(rm) x F128 x F128 x F128 -> F128 */
-         return mkLazy4(fce, Ity_I128, vatom1, vatom2, vatom3, vatom4);
-
       /* V256-bit data-steering */
       case Iop_64x4toV256:
          return assignNew('V', fce, Ity_V256,
@@ -2860,17 +2867,17 @@
    tl_assert(isShadowAtom(fce,vatom1));
    tl_assert(isShadowAtom(fce,vatom2));
    tl_assert(isShadowAtom(fce,vatom3));
-   tl_assert(sameKindedAtoms(atom1,vatom1));
+   /*tl_assert(sameKindedAtoms(atom1,vatom1));
    tl_assert(sameKindedAtoms(atom2,vatom2));
-   tl_assert(sameKindedAtoms(atom3,vatom3));
+   tl_assert(sameKindedAtoms(atom3,vatom3)); */
    switch (op) {
       case Iop_AddF128:
-      case Iop_SubF128:
-      case Iop_MulF128:
-      case Iop_DivF128:
       case Iop_AddD128:
+      case Iop_SubF128:
       case Iop_SubD128:
+      case Iop_MulF128:
       case Iop_MulD128:
+      case Iop_DivF128:
       case Iop_DivD128:
       case Iop_QuantizeD128:
          /* I32(rm) x F128/D128 x F128/D128 -> F128/D128 */
@@ -2975,8 +2982,8 @@
    tl_assert(isOriginalAtom(fce,atom2));
    tl_assert(isShadowAtom(fce,vatom1));
    tl_assert(isShadowAtom(fce,vatom2));
-   tl_assert(sameKindedAtoms(atom1,vatom1));
-   tl_assert(sameKindedAtoms(atom2,vatom2));
+   /*tl_assert(sameKindedAtoms(atom1,vatom1));
+   tl_assert(sameKindedAtoms(atom2,vatom2)); */
    switch (op) {
 
       /* 32-bit SIMD */
@@ -3445,8 +3452,6 @@
       case Iop_CipherLV128:
       case Iop_NCipherV128:
       case Iop_NCipherLV128:
-      case Iop_MulI128by10E:
-      case Iop_MulI128by10ECarry:
         return binary64Ix2(fce, vatom1, vatom2);
 
       case Iop_QNarrowBin64Sto32Sx4:
@@ -3751,7 +3756,6 @@
 
       case Iop_ShrV128:
       case Iop_ShlV128:
-      case Iop_I128StoBCD128:
          /* Same scheme as with all other shifts.  Note: 10 Nov 05:
             this is wrong now, scalar shifts are done properly lazily.
             Vector shifts should be fixed too. */
@@ -3891,10 +3895,6 @@
       case Iop_D128toI32U: /* IRRoundingMode(I32) x D128 -> unsigned I32  */
          return mkLazy2(fce, Ity_I32, vatom1, vatom2);
 
-      case Iop_F128toI128S:   /* IRRoundingMode(I32) x F128 -> signed I128 */
-      case Iop_RndF128:       /* IRRoundingMode(I32) x F128 -> F128 */
-         return mkLazy2(fce, Ity_I128, vatom1, vatom2);
-
       case Iop_F128toI64S: /* IRRoundingMode(I32) x F128 -> signed I64  */
       case Iop_F128toI64U: /* IRRoundingMode(I32) x F128 -> unsigned I64  */
       case Iop_F128toF64:  /* IRRoundingMode(I32) x F128 -> F64         */
@@ -4007,13 +4007,13 @@
          return mkLazy2(fce, Ity_I64, vatom1, vatom2);
 
       case Iop_Add32:
-         if (fce->bogusLiterals || fce->useLLVMworkarounds)
+         if (1)
             return expensiveAddSub(fce,True,Ity_I32, 
                                    vatom1,vatom2, atom1,atom2);
          else
             goto cheap_AddSub32;
       case Iop_Sub32:
-         if (fce->bogusLiterals)
+         if (1)
             return expensiveAddSub(fce,False,Ity_I32, 
                                    vatom1,vatom2, atom1,atom2);
          else
@@ -4406,20 +4406,8 @@
 
       case Iop_NegF128:
       case Iop_AbsF128:
-      case Iop_RndF128:
-      case Iop_TruncF128toI64S: /* F128 -> I64S */
-      case Iop_TruncF128toI32S: /* F128 -> I32S (result stored in 64-bits) */
-      case Iop_TruncF128toI64U: /* F128 -> I64U */
-      case Iop_TruncF128toI32U: /* F128 -> I32U (result stored in 64-bits) */
          return mkPCastTo(fce, Ity_I128, vatom);
 
-      case Iop_BCD128toI128S:
-      case Iop_MulI128by10:
-      case Iop_MulI128by10Carry:
-      case Iop_F16toF64x2:
-      case Iop_F64toF16x2:
-         return vatom;
-
       case Iop_I32StoF128: /* signed I32 -> F128 */
       case Iop_I64StoF128: /* signed I64 -> F128 */
       case Iop_I32UtoF128: /* unsigned I32 -> F128 */
@@ -4555,7 +4543,6 @@
       case Iop_Clz8x16:
       case Iop_Cls8x16:
       case Iop_Abs8x16:
-      case Iop_Ctz8x16:
          return mkPCast8x16(fce, vatom);
 
       case Iop_CmpNEZ16x4:
@@ -4568,7 +4555,6 @@
       case Iop_Clz16x8:
       case Iop_Cls16x8:
       case Iop_Abs16x8:
-      case Iop_Ctz16x8:
          return mkPCast16x8(fce, vatom);
 
       case Iop_CmpNEZ32x2:
@@ -4586,7 +4572,6 @@
       case Iop_FtoI32Sx4_RZ:
       case Iop_Abs32x4:
       case Iop_RSqrtEst32Ux4:
-      case Iop_Ctz32x4:
          return mkPCast32x4(fce, vatom);
 
       case Iop_CmpwNEZ32:
@@ -4599,7 +4584,6 @@
       case Iop_CipherSV128:
       case Iop_Clz64x2:
       case Iop_Abs64x2:
-      case Iop_Ctz64x2:
          return mkPCast64x2(fce, vatom);
 
       case Iop_PwBitMtxXpose64x2:
@@ -4617,7 +4601,6 @@
       case Iop_QNarrowUn64Sto32Sx2:
       case Iop_QNarrowUn64Sto32Ux2:
       case Iop_QNarrowUn64Uto32Ux2:
-      case Iop_F32toF16x4:
          return vectorNarrowUnV128(fce, op, vatom);
 
       case Iop_Widen8Sto16x8:
@@ -4626,7 +4609,6 @@
       case Iop_Widen16Uto32x4:
       case Iop_Widen32Sto64x2:
       case Iop_Widen32Uto64x2:
-      case Iop_F16toF32x4:
          return vectorWidenI64(fce, op, vatom);
 
       case Iop_PwAddL32Ux2:
@@ -4946,7 +4928,7 @@
 
 /* --------- This is the main expression-handling function. --------- */
 
-static
+
 IRExpr* expr2vbits ( FCEnv* fce, IRExpr* e )
 {
    switch (e->tag) {
@@ -5063,6 +5045,43 @@
    VG_(tool_panic)("zwidenToHostWord");
 }
 
+/* Generate code to save the old data value in a location we're about
+   to overwrite with a store, for future rollback. */
+static void do_rollback_Store(FCEnv* fce,  IREndness end, IRAtom *addr,
+			      IRAtom *data) {
+   void*    helper = NULL;
+   const HChar*    hname = NULL;
+   IRDirty *di;
+
+   tl_assert(end == Iend_LE || end == Iend_BE);
+
+   switch (typeOfIRExpr(fce->sb->tyenv, data)) {
+   case Ity_F64:
+   case Ity_I64: helper = &FC_(save_for_rollback64);
+                  hname = "FC_(save_for_rollback64)";
+      break;
+   case Ity_F32:
+   case Ity_I32: helper = &FC_(save_for_rollback32);
+                  hname = "FC_(save_for_rollback32)";
+      break;
+   case Ity_I16: helper = &FC_(save_for_rollback16);
+                  hname = "FC_(save_for_rollback16)";
+      break;
+   case Ity_I8:  helper = &FC_(save_for_rollback8);
+                  hname = "FC_(save_for_rollback8)";
+      break;
+   case Ity_V128: helper = &FC_(save_for_rollback128);
+                  hname = "FC_(save_for_rollback128)";
+      break;
+   default:      VG_(tool_panic)("Unhandled size in do_rollback_Store");
+   }
+
+   di = unsafeIRDirty_0_N(1/*regparms*/, 
+			  hname, VG_(fnptr_to_fnentry)(helper), 
+			  mkIRExprVec_1(addr));
+   di->guard = mkexpr(fce->rollbackModeTmp1);
+   stmt('V', fce, IRStmt_Dirty(di) );
+}
 
 /* Generate a shadow store.  |addr| is always the original address
    atom.  You can pass in either originals or V-bits for the data
@@ -6336,6 +6355,7 @@
    Int     i, j, first_stmt;
    IRStmt* st;
    FCEnv   fce;
+   TraceEnv trace_env;
    IRSB*   sb_out;
 
    if (gWordTy != hWordTy) {
@@ -6390,10 +6410,22 @@
       ent.kind    = Orig;
       ent.shadowV = IRTemp_INVALID;
       ent.shadowB = IRTemp_INVALID;
+      ent.shadowT = IRTemp_INVALID;
       VG_(addToXA)( fce.tmpMap, &ent );
    }
    tl_assert( VG_(sizeXA)( fce.tmpMap ) == sb_in->tyenv->types_used );
 
+   fce.trace_env = &trace_env;
+
+   if (FC_(shadow_guest_offset) == 0) {
+      tl_assert(FC_(tags_guest_offset) == 0);
+      FC_(tags_guest_offset) = layout->total_sizeB;
+      FC_(shadow_guest_offset) = layout->total_sizeB;
+   } else {
+      tl_assert(FC_(tags_guest_offset) == layout->total_sizeB);
+      tl_assert(FC_(shadow_guest_offset) == layout->total_sizeB);
+   }
+
    if (FC_(clo_expensive_definedness_checks)) {
       /* For expensive definedness checking skip looking for bogus
          literals. */
@@ -6424,6 +6456,30 @@
       fce.bogusLiterals = bogus;
    }
 
+   {
+      UInt loc = (UInt)&FC_(enclosure_mode);
+      fce.enclosureModeTmp32 = newTemp(&fce,Ity_I32, Orig);
+      //fce.enclosureModeTmp32 = newIRTemp(sb_in->tyenv, Ity_I32);
+      fce.enclosureModeTmp8 = newTemp(&fce,Ity_I8, Orig);
+      /* XXX Iend_LE s/b host endianness, U32 s/b host int width */
+      assign('V', &fce, fce.enclosureModeTmp32,
+           IRExpr_Load(Iend_LE, Ity_I32, mkU32(loc)));
+      assign('V', &fce, fce.enclosureModeTmp8,
+           unop(Iop_32to8, mkexpr(fce.enclosureModeTmp32)));
+   }
+           
+   {
+      UInt loc = (UInt)&FC_(rollback_mode);
+      fce.rollbackModeTmp32 = newTemp(&fce,Ity_I32, Orig);
+      fce.rollbackModeTmp1 = newTemp(&fce,Ity_I1, Orig);
+      /* XXX Iend_LE s/b host endianness, U32 s/b host int width */
+      assign('V', &fce, fce.rollbackModeTmp32,
+	     IRExpr_Load(Iend_LE, Ity_I32, mkU32(loc)));
+      assign('V', &fce, fce.rollbackModeTmp1,
+           unop(Iop_32to1, mkexpr(fce.rollbackModeTmp32)));
+   }
+
+
    /* Copy verbatim any IR preamble preceding the first IMark */
 
    tl_assert(fce.sb == sb_out);
@@ -6512,9 +6568,32 @@
       switch (st->tag) {
 
          case Ist_WrTmp:
-            assign( 'V', &fce, findShadowTmpV(&fce, st->Ist.WrTmp.tmp), 
-                               expr2vbits( &fce, st->Ist.WrTmp.data) );
-            break;
+	    if (FC_(clo_taint_all_enclosed)) {
+	       IRType type = shadowTypeV(typeOfIRExpr(sb_in->tyenv,
+						     st->Ist.WrTmp.data));
+	       IRTemp precise_vbits = newTemp(&fce, type, VSh);
+	       assign('V', &fce, precise_vbits, expr2vbits(&fce, st->Ist.WrTmp.data));
+	       if (type == Ity_I1) {
+		  /* compensate for VEX by using a different but
+		     equivalent translation here */
+		  IRTemp t2 = newTemp(&fce,Ity_I32, VSh);
+		  IRTemp t3 = newTemp(&fce,Ity_I32, VSh);
+		  assign('V', &fce, t2, unop(Iop_1Sto32, mkexpr(precise_vbits)));
+		  assign('V', &fce, t3, binop(Iop_Or32, mkexpr(t2),
+				       mkexpr(fce.enclosureModeTmp32)));
+		  assign('V', &fce, findShadowTmpV(&fce, st->Ist.WrTmp.tmp),
+			 unop(Iop_CmpNEZ32, mkexpr(t3)));
+	       } else {
+		  assign('V', &fce, findShadowTmpV(&fce, st->Ist.WrTmp.tmp),
+			 IRExpr_ITE(mkexpr(fce.enclosureModeTmp8),
+				      undefinedOfType(type),
+				      mkexpr(precise_vbits)));	  
+	       }
+	    } else {
+	       assign( 'V', &fce, findShadowTmpV(&fce, st->Ist.WrTmp.tmp), 
+		       expr2vbits( &fce, st->Ist.WrTmp.data) );
+	    }
+	    break;
 
          case Ist_Put:
             do_shadow_PUT( &fce, 
@@ -6528,6 +6607,8 @@
             break;
 
          case Ist_Store:
+	    do_rollback_Store(&fce, st->Ist.Store.end, st->Ist.Store.addr,
+			    	   st->Ist.Store.data);
             do_shadow_Store( &fce, st->Ist.Store.end,
                                    st->Ist.Store.addr, 0/* addr bias */,
                                    st->Ist.Store.data,
@@ -6548,6 +6629,7 @@
             break;
 
          case Ist_IMark:
+ 	    trace_env.location = (ULong)st->Ist.IMark.addr << 16;
             break;
 
          case Ist_NoOp:
@@ -6590,6 +6672,9 @@
 
       } /* switch (st->tag) */
 
+      if (FC_(clo_trace_secret_graph))
+	 trace_translate_stmt(&fce, &trace_env, st);
+
       if (0 && verboze) {
          for (j = first_stmt; j < sb_out->stmts_used; j++) {
             VG_(printf)("   ");
@@ -6604,6 +6689,8 @@
          above. */
       if (st->tag != Ist_CAS)
          stmt('C', &fce, st);
+      trace_env.location = (trace_env.location & ~0xff) + 0x100;
+
    }
 
    /* Now we need to complain if the jump target is undefined. */
@@ -6617,6 +6704,9 @@
 
    complainIfUndefined( &fce, sb_in->next, NULL );
 
+   do_shadow_jump_trace(&fce, &trace_env, sb_in->jumpkind, sb_in->next,
+			IRExpr_Const(IRConst_U1(1)));
+
    if (0 && verboze) {
       for (j = first_stmt; j < sb_out->stmts_used; j++) {
          VG_(printf)("   ");
@@ -6635,7 +6725,6 @@
    return sb_out;
 }
 
-
 /*------------------------------------------------------------*/
 /*--- Post-tree-build final tidying                        ---*/
 /*------------------------------------------------------------*/
@@ -6654,69 +6743,17 @@
    reference, which is kinda pointless.  FC_(final_tidy) therefore
    looks for such repeated calls and removes all but the first. */
 
-
-/* With some testing on perf/bz2.c, on amd64 and x86, compiled with
-   gcc-5.3.1 -O2, it appears that 16 entries in the array are enough to
-   get almost all the benefits of this transformation whilst causing
-   the slide-back case to just often enough to be verifiably
-   correct.  For posterity, the numbers are:
-
-   bz2-32
-
-   1   4,336 (112,212 -> 1,709,473; ratio 15.2)
-   2   4,336 (112,194 -> 1,669,895; ratio 14.9)
-   3   4,336 (112,194 -> 1,660,713; ratio 14.8)
-   4   4,336 (112,194 -> 1,658,555; ratio 14.8)
-   5   4,336 (112,194 -> 1,655,447; ratio 14.8)
-   6   4,336 (112,194 -> 1,655,101; ratio 14.8)
-   7   4,336 (112,194 -> 1,654,858; ratio 14.7)
-   8   4,336 (112,194 -> 1,654,810; ratio 14.7)
-   10  4,336 (112,194 -> 1,654,621; ratio 14.7)
-   12  4,336 (112,194 -> 1,654,678; ratio 14.7)
-   16  4,336 (112,194 -> 1,654,494; ratio 14.7)
-   32  4,336 (112,194 -> 1,654,602; ratio 14.7)
-   inf 4,336 (112,194 -> 1,654,602; ratio 14.7)
-
-   bz2-64
-
-   1   4,113 (107,329 -> 1,822,171; ratio 17.0)
-   2   4,113 (107,329 -> 1,806,443; ratio 16.8)
-   3   4,113 (107,329 -> 1,803,967; ratio 16.8)
-   4   4,113 (107,329 -> 1,802,785; ratio 16.8)
-   5   4,113 (107,329 -> 1,802,412; ratio 16.8)
-   6   4,113 (107,329 -> 1,802,062; ratio 16.8)
-   7   4,113 (107,329 -> 1,801,976; ratio 16.8)
-   8   4,113 (107,329 -> 1,801,886; ratio 16.8)
-   10  4,113 (107,329 -> 1,801,653; ratio 16.8)
-   12  4,113 (107,329 -> 1,801,526; ratio 16.8)
-   16  4,113 (107,329 -> 1,801,298; ratio 16.8)
-   32  4,113 (107,329 -> 1,800,827; ratio 16.8)
-   inf 4,113 (107,329 -> 1,800,827; ratio 16.8)
-*/
-
-/* Structs for recording which (helper, guard) pairs we have already
+/* A struct for recording which (helper, guard) pairs we have already
    seen. */
-
-#define N_TIDYING_PAIRS 16
-
 typedef
    struct { void* entry; IRExpr* guard; }
    Pair;
 
-typedef
-   struct {
-      Pair pairs[N_TIDYING_PAIRS +1/*for bounds checking*/];
-      UInt pairsUsed;
-   }
-   Pairs;
-
-
 /* Return True if e1 and e2 definitely denote the same value (used to
    compare guards).  Return False if unknown; False is the safe
    answer.  Since guest registers and guest memory do not have the
    SSA property we must return False if any Gets or Loads appear in
-   the expression.  This implicitly assumes that e1 and e2 have the
-   same IR type, which is always true here -- the type is Ity_I1. */
+   the expression. */
 
 static Bool sameIRValue ( IRExpr* e1, IRExpr* e2 )
 {
@@ -6765,98 +6802,45 @@
    True if so.  If not, add an entry. */
 
 static 
-Bool check_or_add ( Pairs* tidyingEnv, IRExpr* guard, void* entry )
+Bool check_or_add ( XArray* /*of Pair*/ pairs, IRExpr* guard, void* entry )
 {
-   UInt i, n = tidyingEnv->pairsUsed;
-   tl_assert(n <= N_TIDYING_PAIRS);
+   Pair  p;
+   Pair* pp;
+   Int   i, n = VG_(sizeXA)( pairs );
    for (i = 0; i < n; i++) {
-      if (tidyingEnv->pairs[i].entry == entry
-          && sameIRValue(tidyingEnv->pairs[i].guard, guard))
+      pp = VG_(indexXA)( pairs, i );
+      if (pp->entry == entry && sameIRValue(pp->guard, guard))
          return True;
    }
-   /* (guard, entry) wasn't found in the array.  Add it at the end.
-      If the array is already full, slide the entries one slot
-      backwards.  This means we will lose to ability to detect
-      duplicates from the pair in slot zero, but that happens so
-      rarely that it's unlikely to have much effect on overall code
-      quality.  Also, this strategy loses the check for the oldest
-      tracked exit (memory reference, basically) and so that is (I'd
-      guess) least likely to be re-used after this point. */
-   tl_assert(i == n);
-   if (n == N_TIDYING_PAIRS) {
-      for (i = 1; i < N_TIDYING_PAIRS; i++) {
-         tidyingEnv->pairs[i-1] = tidyingEnv->pairs[i];
-      }
-      tidyingEnv->pairs[N_TIDYING_PAIRS-1].entry = entry;
-      tidyingEnv->pairs[N_TIDYING_PAIRS-1].guard = guard;
-   } else {
-      tl_assert(n < N_TIDYING_PAIRS);
-      tidyingEnv->pairs[n].entry = entry;
-      tidyingEnv->pairs[n].guard = guard;
-      n++;
-      tidyingEnv->pairsUsed = n;
-   }
+   p.guard = guard;
+   p.entry = entry;
+   VG_(addToXA)( pairs, &p );
    return False;
 }
 
 static Bool is_helperc_value_checkN_fail ( const HChar* name )
 {
-   /* This is expensive because it happens a lot.  We are checking to
-      see whether |name| is one of the following 8 strings:
-
-         FC_(helperc_value_check8_fail_no_o)
-         FC_(helperc_value_check4_fail_no_o)
-         FC_(helperc_value_check0_fail_no_o)
-         FC_(helperc_value_check1_fail_no_o)
-         FC_(helperc_value_check8_fail_w_o)
-         FC_(helperc_value_check0_fail_w_o)
-         FC_(helperc_value_check1_fail_w_o)
-         FC_(helperc_value_check4_fail_w_o)
-
-      To speed it up, check the common prefix just once, rather than
-      all 8 times.
-   */
-   const HChar* prefix = "FC_(helperc_value_check";
-
-   HChar n, p;
-   while (True) {
-      n = *name;
-      p = *prefix;
-      if (p == 0) break; /* ran off the end of the prefix */
-      /* We still have some prefix to use */
-      if (n == 0) return False; /* have prefix, but name ran out */
-      if (n != p) return False; /* have both pfx and name, but no match */
-      name++;
-      prefix++;
-   }
-
-   /* Check the part after the prefix. */
-   tl_assert(*prefix == 0 && *name != 0);
-   return    0==VG_(strcmp)(name, "8_fail_no_o)")
-          || 0==VG_(strcmp)(name, "4_fail_no_o)")
-          || 0==VG_(strcmp)(name, "0_fail_no_o)")
-          || 0==VG_(strcmp)(name, "1_fail_no_o)")
-          || 0==VG_(strcmp)(name, "8_fail_w_o)")
-          || 0==VG_(strcmp)(name, "4_fail_w_o)")
-          || 0==VG_(strcmp)(name, "0_fail_w_o)")
-          || 0==VG_(strcmp)(name, "1_fail_w_o)");
+   return
+      0==VG_(strcmp)(name, "FC_(helperc_value_check0_fail_no_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check1_fail_no_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check4_fail_no_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check8_fail_no_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check0_fail_w_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check1_fail_w_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check4_fail_w_o)")
+      || 0==VG_(strcmp)(name, "FC_(helperc_value_check8_fail_w_o)");
 }
 
 IRSB* FC_(final_tidy) ( IRSB* sb_in )
 {
-   Int       i;
+   Int i;
    IRStmt*   st;
    IRDirty*  di;
    IRExpr*   guard;
    IRCallee* cee;
    Bool      alreadyPresent;
-   Pairs     pairs;
-
-   pairs.pairsUsed = 0;
-
-   pairs.pairs[N_TIDYING_PAIRS].entry = (void*)0x123;
-   pairs.pairs[N_TIDYING_PAIRS].guard = (IRExpr*)0x456;
-
+   XArray*   pairs = VG_(newXA)( VG_(malloc), "mc.ft.1",
+                                 VG_(free), sizeof(Pair) );
    /* Scan forwards through the statements.  Each time a call to one
       of the relevant helpers is seen, check if we have made a
       previous call to the same helper using the same guard
@@ -6877,21 +6861,16 @@
           guard 'guard'.  Check if we have already seen a call to this
           function with the same guard.  If so, delete it.  If not,
           add it to the set of calls we do know about. */
-      alreadyPresent = check_or_add( &pairs, guard, cee->addr );
+      alreadyPresent = check_or_add( pairs, guard, cee->addr );
       if (alreadyPresent) {
          sb_in->stmts[i] = IRStmt_NoOp();
          if (0) VG_(printf)("XX\n");
       }
    }
-
-   tl_assert(pairs.pairs[N_TIDYING_PAIRS].entry == (void*)0x123);
-   tl_assert(pairs.pairs[N_TIDYING_PAIRS].guard == (IRExpr*)0x456);
-
+   VG_(deleteXA)( pairs );
    return sb_in;
 }
 
-#undef N_TIDYING_PAIRS
-
 
 /*------------------------------------------------------------*/
 /*--- Origin tracking stuff                                ---*/
@@ -7647,62 +7626,6 @@
 }
 
 
-/*------------------------------------------------------------*/
-/*--- Startup assertion checking                           ---*/
-/*------------------------------------------------------------*/
-
-void FC_(do_instrumentation_startup_checks)( void )
-{
-   /* Make a best-effort check to see that is_helperc_value_checkN_fail
-      is working as we expect. */
-
-#  define CHECK(_expected, _string) \
-      tl_assert((_expected) == is_helperc_value_checkN_fail(_string))
-
-   /* It should identify these 8, and no others, as targets. */
-   CHECK(True, "FC_(helperc_value_check8_fail_no_o)");
-   CHECK(True, "FC_(helperc_value_check4_fail_no_o)");
-   CHECK(True, "FC_(helperc_value_check0_fail_no_o)");
-   CHECK(True, "FC_(helperc_value_check1_fail_no_o)");
-   CHECK(True, "FC_(helperc_value_check8_fail_w_o)");
-   CHECK(True, "FC_(helperc_value_check0_fail_w_o)");
-   CHECK(True, "FC_(helperc_value_check1_fail_w_o)");
-   CHECK(True, "FC_(helperc_value_check4_fail_w_o)");
-
-   /* Ad-hoc selection of other strings gathered via a quick test. */
-   CHECK(False, "amd64g_dirtyhelper_CPUID_avx2");
-   CHECK(False, "amd64g_dirtyhelper_RDTSC");
-   CHECK(False, "FC_(helperc_b_load1)");
-   CHECK(False, "FC_(helperc_b_load2)");
-   CHECK(False, "FC_(helperc_b_load4)");
-   CHECK(False, "FC_(helperc_b_load8)");
-   CHECK(False, "FC_(helperc_b_load16)");
-   CHECK(False, "FC_(helperc_b_load32)");
-   CHECK(False, "FC_(helperc_b_store1)");
-   CHECK(False, "FC_(helperc_b_store2)");
-   CHECK(False, "FC_(helperc_b_store4)");
-   CHECK(False, "FC_(helperc_b_store8)");
-   CHECK(False, "FC_(helperc_b_store16)");
-   CHECK(False, "FC_(helperc_b_store32)");
-   CHECK(False, "FC_(helperc_LOADV8)");
-   CHECK(False, "FC_(helperc_LOADV16le)");
-   CHECK(False, "FC_(helperc_LOADV32le)");
-   CHECK(False, "FC_(helperc_LOADV64le)");
-   CHECK(False, "FC_(helperc_LOADV128le)");
-   CHECK(False, "FC_(helperc_LOADV256le)");
-   CHECK(False, "FC_(helperc_STOREV16le)");
-   CHECK(False, "FC_(helperc_STOREV32le)");
-   CHECK(False, "FC_(helperc_STOREV64le)");
-   CHECK(False, "FC_(helperc_STOREV8)");
-   CHECK(False, "track_die_mem_stack_8");
-   CHECK(False, "track_new_mem_stack_8_w_ECU");
-   CHECK(False, "FC_(helperc_MAKE_STACK_UNINIT_w_o)");
-   CHECK(False, "VG_(unknown_SP_update_w_ECU)");
-
-#  undef CHECK
-}
-
-
 /*--------------------------------------------------------------------*/
 /*--- end                                           fc_translate.c ---*/
 /*--------------------------------------------------------------------*/
diff -urN exp-flowcheck-base/fc_translate.h exp-flowcheck/fc_translate.h
--- exp-flowcheck-base/fc_translate.h	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/fc_translate.h	2016-08-30 15:23:14.000352203 -0500
@@ -0,0 +1,103 @@
+/*--------------------------------------------------------------------*/
+/*--- Instrument IR to perform memory checking operations.         ---*/
+/*---                                               fc_translate.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
+
+   Copyright (C) 2000-2007 Julian Seward
+      jseward@acm.org
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+typedef struct _TraceEnv TraceEnv;
+
+/* Carries around state during flowcheck instrumentation. */
+typedef
+   struct _FCEnv {
+      /* MODIFIED: the superblock being constructed.  IRStmts are
+         added. */
+      IRSB* sb;
+      Bool  trace;
+
+      /* MODIFIED: a table [0 .. #temps_in_sb-1] which gives the
+         current kind and possibly shadow temps for each temp in the
+         IRSB being constructed.  Note that it does not contain the
+         type of each tmp.  If you want to know the type, look at the
+         relevant entry in sb->tyenv.  It follows that at all times
+         during the instrumentation process, the valid indices for
+         tmpMap and sb->tyenv are identical, being 0 .. N-1 where N is
+         total number of Orig, V- and B- temps allocated so far.
+
+         The reason for this strange split (types in one place, all
+         other info in another) is that we need the types to be
+         attached to sb so as to make it possible to do
+         "typeOfIRExpr(fce->bb->tyenv, ...)" at various places in the
+         instrumentation process. */
+      XArray* /* of TempMapEnt */ tmpMap;
+
+      IRTemp enclosureModeTmp8;
+      IRTemp enclosureModeTmp32;
+
+      IRTemp rollbackModeTmp1;
+      IRTemp rollbackModeTmp32;
+
+      /* MODIFIED: indicates whether "bogus" literals have so far been
+         found.  Starts off False, and may change to True. */
+      Bool bogusLiterals;
+
+      /* READONLY: indicates whether we should use expensive
+         interpretations of integer adds, since unfortunately LLVM
+         uses them to do ORs in some circumstances.  Defaulted to True
+         on MacOS and False everywhere else. */
+      Bool useLLVMworkarounds;
+
+      /* READONLY: the guest layout.  This indicates which parts of
+         the guest state should be regarded as 'always defined'. */
+      const VexGuestLayout* layout;
+
+      /* READONLY: the host word type.  Needed for constructing
+         arguments of type 'HWord' to be passed to helper functions.
+         Ity_I32 or Ity_I64 only. */
+      IRType hWordTy;
+      TraceEnv *trace_env;
+
+   }
+   FCEnv;
+
+
+typedef  IRExpr  IRAtom;
+
+IRExpr* expr2vbits ( struct _FCEnv* fce, IRExpr* e );
+Bool isShadowAtom ( FCEnv* fce, IRAtom* a1 );
+IRTemp findShadowTmpV ( FCEnv* fce, IRTemp orig );
+IRTemp findShadowTmpT ( FCEnv* fce, IRTemp orig );
+IRTemp newTempT( FCEnv* fce );
+IRAtom* assignNew ( HChar cat, FCEnv* fce, IRType ty, IRExpr* e );
+
+/* The offset of the shadow area, within the guest state. */
+Int FC_(shadow_guest_offset);
+
+/* Offset of the tags shadows within the shadow guest state. */
+Int FC_(tags_guest_offset);
diff -urN exp-flowcheck-base/flowcheck.h exp-flowcheck/flowcheck.h
--- exp-flowcheck-base/flowcheck.h	2016-08-30 16:12:49.944947853 -0500
+++ exp-flowcheck/flowcheck.h	2016-08-30 15:23:14.003743998 -0500
@@ -11,7 +11,10 @@
    ----------------------------------------------------------------
 
    This file is part of FlowCheck, a heavyweight Valgrind tool for
-   detecting memory errors.
+   detecting leakage of secret information.
+
+   Based on MemCheck, a heavyweight Valgrind tool for detecting memory
+   errors.
 
    Copyright (C) 2000-2015 Julian Seward.  All rights reserved.
 
@@ -93,6 +96,28 @@
 
       VG_USERREQ__MAKE_MEM_DEFINED_IF_ADDRESSABLE,
 
+      /* This is the place in the numbering where we got to end of the
+	 Memcheck client requests that existed as of Flowcheck 1.00,
+	 so it's where the Flowcheck-specific requests start. */
+
+      VG_USERREQ__PUSH_ENCLOSE,
+      VG_USERREQ__POP_ENCLOSE,
+
+      VG_USERREQ__LEAK_WORD,
+      VG_USERREQ__MAYBE_LEAK_WORD,
+      VG_USERREQ__TAINT_WORD,
+      VG_USERREQ__UNTAINT_WORD,
+
+      VG_USERREQ__MD5SUM_BLOCK,
+
+      VG_USERREQ__PREPARE_ROLLBACK,
+      VG_USERREQ__PREPARE_ESCAPEE,
+      VG_USERREQ__DO_ROLLBACK,
+
+      VG_USERREQ__NOTE_ITERATION,
+
+      /* Back to Memcheck requests, more recently added */
+
       /* Not next to VG_USERREQ__COUNT_LEAKS because it was added later. */
       VG_USERREQ__COUNT_LEAK_BLOCKS,
 
@@ -298,5 +323,230 @@
        VG_USERREQ__ENABLE_ADDR_ERROR_REPORTING_IN_RANGE,       \
        (_qzz_addr), (_qzz_len), 0, 0, 0)
 
+
+#define FC_PUSH_ENCLOSE(zzident, zzespoff, zzebpoff)		 \
+   {VALGRIND_DO_CLIENT_REQUEST_STMT(                     	 \
+                            VG_USERREQ__PUSH_ENCLOSE,            \
+			    zzident, zzespoff, zzebpoff, 0, 0);  \
+   }
+
+#define FC_POP_ENCLOSE(zzident)					 \
+   {                                 				 \
+    VALGRIND_DO_CLIENT_REQUEST_STMT(		                 \
+                            VG_USERREQ__POP_ENCLOSE,             \
+                            zzident, 0, 0, 0, 0);                \
+   }
+
+#define FC_LEAK_WORD(zzvar)					 \
+   (__extension__({unsigned int _qzz_res = zzvar;                \
+       VALGRIND_DO_CLIENT_REQUEST(_qzz_res, zzvar,		 \
+                            VG_USERREQ__LEAK_WORD,               \
+                            &zzvar, 0, 0, 0, 0);                 \
+    _qzz_res;                                                    \
+     }))
+
+#define FC_MAYBE_LEAK_WORD(zzvar)				 \
+   (__extension__({unsigned int _qzz_res;			 \
+       VALGRIND_DO_CLIENT_REQUEST(_qzz_res, zzvar,		 \
+				  VG_USERREQ__MAYBE_LEAK_WORD,	 \
+				  &zzvar, 0, 0, 0, 0);		 \
+       zzvar;							 \
+     }))
+
+#define FC_TAINT_WORD(zzvarref)					 \
+   (__extension__({unsigned int _qzz_res = 0;                    \
+    VALGRIND_DO_CLIENT_REQUEST(_qzz_res, 0,                      \
+                            VG_USERREQ__TAINT_WORD,              \
+			    zzvarref, 0, 0, 0, 0);		 \
+    _qzz_res;                                                    \
+     }))
+
+#define FC_UNTAINT_WORD(zzvarref)				 \
+   (__extension__({unsigned int _qzz_res = 0;                    \
+    VALGRIND_DO_CLIENT_REQUEST(_qzz_res, 0,                      \
+                            VG_USERREQ__UNTAINT_WORD,            \
+			    zzvarref, 0, 0, 0, 0);		 \
+    _qzz_res;                                                    \
+     }))
+
+#define FC_MD5SUM_BLOCK(zzinptr, zzinlen, zzoutptr)		 \
+   (__extension__({unsigned int _qzz_res = 0;                    \
+    VALGRIND_DO_CLIENT_REQUEST(_qzz_res, 0,                      \
+                            VG_USERREQ__MD5SUM_BLOCK,            \
+			    zzinptr, zzinlen, zzoutptr, 0, 0);	 \
+    _qzz_res;                                                    \
+     }))
+
+#define FC_PREPARE_ROLLBACK(zzident, zzvarptr, zzlen)			\
+  {									\
+    VALGRIND_DO_CLIENT_REQUEST_STMT(					\
+			       VG_USERREQ__PREPARE_ROLLBACK,		\
+			       zzident, zzvarptr, zzlen, 0, 0);		\
+  }
+
+#define FC_PREPARE_ESCAPEE(zzvarptr, zzlen)				\
+  {									\
+    VALGRIND_DO_CLIENT_REQUEST_STMT(					\
+			       VG_USERREQ__PREPARE_ESCAPEE,		\
+			       zzvarptr, zzlen, 0, 0, 0);		\
+  }
+
+#define FC_PREPARE_ROLLBACK_VAR(ident, var)		\
+  FC_PREPARE_ROLLBACK(ident, &(var), sizeof(var))
+
+#define FC_PREPARE_ESCAPEE_VAR(var)		\
+  FC_PREPARE_ESCAPEE(&(var), sizeof(var))
+
+#define FC_DO_ROLLBACK(zzident)					 \
+  {								 \
+    VALGRIND_DO_CLIENT_REQUEST_STMT(    	                 \
+			       VG_USERREQ__DO_ROLLBACK,		 \
+			       zzident, 0, 0, 0, 0);	       	 \
+  }
+
+/* When entering an enclosure region, you can specify any combination
+   of regions and variables as the output. Regions are passed as two
+   arguments, a start address and a length. Variable are passed as a
+   single argument, where a variable "v" is equivalent to the region
+   (&v, sizeof(v)). The first one is passed to the _ROLLBACK requests,
+   and any subsequent ones to the _ESCAPEE ones. */
+
+#define FC_ENTER_ENCLOSE_R(ident, preserve, len)		\
+  {								\
+    FC_PUSH_ENCLOSE(ident, 0, 0);				\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);			\
+  }
+
+#define FC_ENTER_ENCLOSE_V(ident, var)				\
+  {								\
+    FC_PUSH_ENCLOSE(ident, 0, 0);				\
+    FC_PREPARE_ROLLBACK_VAR(ident, var);			\
+  }
+
+#define FC_ENTER_ENCLOSE_RR(ident, preserve, len, p2, l2)	\
+  {								\
+    FC_PUSH_ENCLOSE(ident, 0, 0);				\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);			\
+    FC_PREPARE_ESCAPEE(p2, l2);					\
+  }
+
+#define FC_ENTER_ENCLOSE_RV(ident, preserve, len, var)		\
+  {								\
+    FC_PUSH_ENCLOSE(ident, 0, 0);				\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);			\
+    FC_PREPARE_ESCAPEE_VAR(var);				\
+  }
+
+#define FC_ENTER_ENCLOSE_VV(ident, var1, var2)	\
+  {								\
+    FC_PUSH_ENCLOSE(ident, 0, 0);				\
+    FC_PREPARE_ROLLBACK_VAR(ident, var1);			\
+    FC_PREPARE_ESCAPEE_VAR(var2);				\
+  }
+
+#define FC_ENTER_ENCLOSE_RRR(ident, preserve, len, p2, l2, p3, l3)	\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE(p2, l2);						\
+    FC_PREPARE_ESCAPEE(p3, l3);						\
+  }
+
+#define FC_ENTER_ENCLOSE_RRV(ident, preserve, len, p2, l2, var)		\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE(p2, l2);						\
+    FC_PREPARE_ESCAPEE_VAR(var);					\
+  }
+
+#define FC_ENTER_ENCLOSE_RVV(ident, preserve, len, var1, var2)		\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE_VAR(var1);					\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+  }
+
+#define FC_ENTER_ENCLOSE_VVV(ident, var1, var2, var3)			\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK_VAR(ident, var1);				\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+  }
+
+#define FC_ENTER_ENCLOSE_RVVV(ident, preserve, len, var1, var2, var3)	\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE_VAR(var1);					\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+  }
+
+#define FC_ENTER_ENCLOSE_VVVV(ident, var1, var2, var3, var4)		\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK_VAR(ident, var1);				\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+    FC_PREPARE_ESCAPEE_VAR(var4);					\
+  }
+
+#define FC_ENTER_ENCLOSE_RVVVV(ident, preserve, len, var1, var2, var3, var4) \
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE_VAR(var1);					\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+    FC_PREPARE_ESCAPEE_VAR(var4);					\
+  }
+
+#define FC_ENTER_ENCLOSE_VVVVV(ident, var1, var2, var3, var4, var5)	\
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK_VAR(ident, var1);				\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+    FC_PREPARE_ESCAPEE_VAR(var4);					\
+    FC_PREPARE_ESCAPEE_VAR(var5);					\
+  }
+
+#define FC_ENTER_ENCLOSE_RVVVVV(ident, preserve, len, var1, var2, var3, var4, var5) \
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK(ident, preserve, len);				\
+    FC_PREPARE_ESCAPEE_VAR(var1);					\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+    FC_PREPARE_ESCAPEE_VAR(var4);					\
+    FC_PREPARE_ESCAPEE_VAR(var5);					\
+  }
+
+#define FC_ENTER_ENCLOSE_VVVVVV(ident, var1, var2, var3, var4, var5, var6) \
+  {									\
+    FC_PUSH_ENCLOSE(ident, 0, 0);					\
+    FC_PREPARE_ROLLBACK_VAR(ident, var1);				\
+    FC_PREPARE_ESCAPEE_VAR(var2);					\
+    FC_PREPARE_ESCAPEE_VAR(var3);					\
+    FC_PREPARE_ESCAPEE_VAR(var4);					\
+    FC_PREPARE_ESCAPEE_VAR(var5);					\
+    FC_PREPARE_ESCAPEE_VAR(var6);					\
+  }
+
+#define FC_LEAVE_ENCLOSE(ident)				\
+  {							\
+    FC_DO_ROLLBACK(ident);				\
+    FC_POP_ENCLOSE(ident);				\
+  }
+
+#define FC_NOTE_ITERATION()						\
+  {									\
+    VALGRIND_DO_CLIENT_REQUEST_STMT(					\
+			       VG_USERREQ__NOTE_ITERATION,		\
+			       0, 0, 0, 0, 0);				\
+  }
 #endif
 
diff -urN exp-flowcheck-base/fold.c exp-flowcheck/fold.c
--- exp-flowcheck-base/fold.c	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/fold.c	2016-08-30 15:23:14.006907859 -0500
@@ -0,0 +1,336 @@
+/*--------------------------------------------------------------------*/
+/*--- Collapse a large flow graph into a compressed one that       ---*/
+/*--- allows the same flows, by adding together edges frome the    ---*/
+/*--- same program location and merging nodes with a union-find    ---*/
+/*--- data structure.                                              ---*/
+/*---                                                       fold.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_tool_basics.h"
+#include "pub_tool_aspacemgr.h"
+#include "pub_tool_hashtable.h"     // For fc_include.h
+#include "pub_tool_debuginfo.h"
+#include "pub_tool_machine.h"
+#include "pub_tool_mallocfree.h"
+#include "pub_tool_oset.h"
+#include "pub_tool_poolalloc.h"
+#include "pub_tool_threadstate.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_vki.h"
+#include "pub_tool_libcbase.h"
+#include "pub_tool_libcfile.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_libcproc.h"
+#include "pub_tool_libcassert.h"
+#include "fc_include.h"
+#include "fold.h"
+#include "trace_runtime.h"
+
+struct uf_object {
+   struct uf_object *parent;
+   UShort rank;
+   Char free;
+   Char mark;
+};
+
+#define NUM_TAGS 10000000
+/* #define NUM_TAGS 150000 */
+
+static int tags_left = NUM_TAGS - 2;
+
+static struct uf_object tags[NUM_TAGS];
+
+static struct uf_object *uf_find(struct uf_object *o) {
+   struct uf_object *root, *next;
+
+   for (root = o; root->parent != root; root = root->parent)
+      ;
+
+   for (next = o->parent; next != root; o = next, next = o->parent)
+      o->parent = root;
+
+   return root;
+}
+
+static void uf_make_set(struct uf_object *o) {
+   o->parent = o;
+   o->rank = 0;
+}
+
+static UWord uf_set_num(struct uf_object *o) {
+   return o - tags;
+}
+
+static struct uf_object *uf_union(struct uf_object *o1, struct uf_object *o2) {
+   struct uf_object *rep1 = uf_find(o1);
+   struct uf_object *rep2 = uf_find(o2);
+
+   if (rep1 == rep2)
+      return rep1;
+
+   /*VG_(printf)("Merging %d with %d\n", uf_set_num(o1), uf_set_num(o2));*/
+   if (rep1->rank < rep2->rank) {
+      rep1->parent = rep2;
+      return rep2;
+   } else {
+      /* rep1->rank >= rep2->rank */
+      rep2->parent = rep1;
+      if (rep1->rank == rep2->rank)
+	 rep1->rank++;
+      return rep1;
+   }
+}
+
+void fold_init(void) {
+   UWord i;
+   for (i = FIRST_TAG; i < NUM_TAGS; i++) {
+      tags[i].free = 1;
+   }
+}
+
+void fold_new_tag(ULong tag) {
+   tl_assert(tag < NUM_TAGS);
+   uf_make_set(&tags[tag]);
+}
+
+void mark_tag_in_use(UWord tag) {
+   tl_assert(tag != 0);
+   tl_assert(tag != SOURCE_TAG);
+   tl_assert(tag != SINK_TAG);
+   tl_assert(tag != NO_TAG);
+   tl_assert(tag < NUM_TAGS);
+   tags[tag].mark = 1;
+}
+
+static int next_tag = FIRST_TAG;
+
+/* XXX This is core-only, but I get an assertion failure if I don't
+   check it in the for-all-threads loop below. */
+Bool VG_(is_valid_tid)(ThreadId tid);
+
+static void gc_tags(void) {
+   Word i;
+   Word new_free = 0;
+   struct edge_info *edge;
+   for (i = 0; i < NUM_TAGS; i++) {
+      tags[i].mark = 0;
+   }
+   mark_memory_tags();
+   VG_(OSetGen_ResetIter)(edges);
+   while ((edge = VG_(OSetGen_Next)(edges))) {
+      struct uf_object *src_rep = uf_find(&tags[edge->src_tag]);
+      struct uf_object *dest_rep = uf_find(&tags[edge->dest_tag]);
+      src_rep->mark = 1;
+      edge->src_tag = uf_set_num(src_rep);
+      dest_rep->mark = 1;
+      edge->dest_tag = uf_set_num(dest_rep);
+   }
+   {
+      ThreadId tid;
+#define GUEST_SIZE 320 /* XXX figure out how to not hard code this*/
+      ULong guest_tags[GUEST_SIZE];
+      for (tid = 1; tid < VG_N_THREADS; tid++) {
+	 int reg;
+	 if (!VG_(is_valid_tid)(tid))
+	   continue;
+	 VG_(get_shadow_regs_area)(tid, (UChar*)&guest_tags, 0, GUEST_SIZE, 8 * GUEST_SIZE);
+	 for (reg = 0; reg < GUEST_SIZE; reg++) {
+	    if (guest_tags[reg] != 0) {
+	       /*VG_(printf)("Register at offset %d has tag %lld\n",
+		 reg, guest_tags[reg]);*/
+	       mark_tag_in_use(guest_tags[reg]);
+	    }
+	 }
+      }
+   }
+   for (i = FIRST_TAG; i < NUM_TAGS; i++) {
+      /*VG_(printf)("Tag %d: %s\n", i, tags[i].mark ? "in use" : "free");*/
+      if (tags[i].mark) {
+	 tags[i].free = 0;
+      } else {
+	 tags[i].free = 1;
+	 new_free++;
+      }
+   }
+   VG_(printf)("%ld tags freed by garbage collection\n", new_free);
+   next_tag = FIRST_TAG;
+   tags_left = new_free;
+   tl_assert(tags_left);
+}
+
+void check_enough_tags(Word needed) {
+   if (!FC_(clo_folding_level))
+      return;
+   /* 100000 is a fudge factor to save from figuring out all the places
+      "needed" needs to be bigger. */
+   if (needed + 100000 > tags_left) {
+      gc_tags();
+      if (needed > tags_left) {
+	 VG_(printf)("Out of tag space in request for %ld tags\n", needed);
+	 tl_assert(needed <= tags_left);
+      }
+   }
+}
+
+UWord get_free_tag(void) {
+   UWord tag;
+   while (next_tag < NUM_TAGS && !tags[next_tag].free)
+      next_tag++;
+   tl_assert(next_tag < NUM_TAGS);
+   tags_left--;
+   tag = next_tag++;
+   uf_make_set(&tags[tag]);
+   return tag;
+}
+
+void fold_new_edge(ULong src, ULong dest, int capacity,
+		   struct edge_info *edge) {
+   if (edge->src_tag == NO_TAG) {
+      edge->src_tag = src;
+   } else {
+      uf_union(&tags[src], &tags[edge->src_tag]);
+   }
+
+   if (edge->dest_tag == NO_TAG) {
+      edge->dest_tag = dest;
+   } else {
+      uf_union(&tags[dest], &tags[edge->dest_tag]);
+   }
+
+   {
+      ULong old_total = edge->total_capacity;
+      edge->total_capacity += capacity;
+      tl_assert(edge->total_capacity >= old_total);
+   }
+}
+
+static void output_folded_edges_to_fd(int fd, Bool long_form) {
+   struct uf_object *source_rep = uf_find(&tags[SOURCE_TAG]);
+   struct uf_object *sink_rep = uf_find(&tags[SINK_TAG]);
+   struct edge_info *edge;
+
+   VG_(OSetGen_ResetIter)(edges);
+   while ((edge = VG_(OSetGen_Next)(edges))) {
+      struct uf_object *src_rep = uf_find(&tags[edge->src_tag]);
+      struct uf_object *dest_rep = uf_find(&tags[edge->dest_tag]);
+      Long src_num, dest_num;
+      if (src_rep == source_rep)
+	 src_num = 0;
+      else
+	 src_num = uf_set_num(src_rep);
+      if (dest_rep == sink_rep)
+	 dest_num = -1;
+      else
+	 dest_num = uf_set_num(dest_rep);
+      if (fd == 2) {
+	 /* NB: Valgrind's "printf" goes to stderr */
+	 if (long_form) {
+	    VG_(printf)("%lld %lld %lld %llx:%llx %s (%s)\n",
+			src_num, dest_num, edge->total_capacity,
+			edge->location, edge->context,
+			edge->eip_descr, edge->type_descr);
+	 } else {
+	    VG_(printf)("%lld %lld %lld\n", src_num, dest_num,
+			edge->total_capacity);
+	 }
+      } else {
+	 if (long_form) {
+	    fdprintf(fd, "%lld %lld %lld %llx:%llx %s (%s)\n",
+		     src_num, dest_num, edge->total_capacity,
+		     edge->location, edge->context,
+		     edge->eip_descr, edge->type_descr);
+	 } else {
+	    fdprintf(fd, "%lld %lld %lld\n",
+		     src_num, dest_num, edge->total_capacity);
+	 }
+      }
+
+   }
+}
+
+void output_folded_edges_stderr(void) {
+   output_folded_edges_to_fd(2, True);
+}
+
+void run_max_flow_program(const char *fname) {
+   int len;
+   char *flow_fname;
+   char cmd_buf[4096];
+   len = VG_(strlen)(fname);
+   flow_fname = VG_(malloc)("fc.rmfp", len + 6);
+   VG_(strcpy)(flow_fname, fname);
+   if (fname[len-2] == '.' && fname[len-1] == 'g') {
+      VG_(strcpy)(flow_fname + len - 2, ".flow");
+   } else {
+      VG_(strcpy)(flow_fname + len, ".flow");
+   }
+   /* XXX avoid the shell and fixed-size buffer here */
+   VG_(snprintf)(cmd_buf, sizeof cmd_buf, "%s <%s >%s",
+		 FC_(clo_max_flow_program), fname, flow_fname);
+   VG_(system)(cmd_buf);
+   VG_(free)(flow_fname);
+}
+
+void output_folded_edges_filename(const char *fname) {
+   SysRes sr;
+   int fd;
+   sr = VG_(open)(fname, VKI_O_WRONLY|VKI_O_CREAT|VKI_O_TRUNC,
+		  0666);
+   tl_assert(!sr_isError(sr));
+   fd = (int) sr_Res(sr);
+   output_folded_edges_to_fd(fd, True);
+   VG_(close)(fd);
+   if (FC_(clo_max_flow_program)) {
+      run_max_flow_program(fname);
+   }
+}
+
+static UInt last_millis = 0;
+
+void run_incremental_max_flow(void) {
+   int fd;
+   UInt millis;
+   SysRes sr;
+   millis = VG_(read_millisecond_timer)();
+   if (millis - last_millis > 1000) {
+      char cmd_buf[4096];
+      sr = VG_(open)("flowcheck.g", VKI_O_WRONLY|VKI_O_CREAT|VKI_O_TRUNC,
+		     0666);
+      tl_assert(!sr_isError(sr));
+      fd = (int)sr_Res(sr);
+      output_folded_edges_to_fd(fd, False);
+      VG_(close)(fd);
+      if (FC_(clo_max_flow_program)) {
+	 VG_(snprintf)(cmd_buf, sizeof cmd_buf, "%s <%s >%s",
+		       FC_(clo_max_flow_program), "flowcheck.g",
+		       "flowcheck.flow");
+	 VG_(system)(cmd_buf);
+      }
+      last_millis = millis;
+   }
+}
diff -urN exp-flowcheck-base/fold.h exp-flowcheck/fold.h
--- exp-flowcheck-base/fold.h	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/fold.h	2016-08-30 15:23:14.009608591 -0500
@@ -0,0 +1,21 @@
+#ifndef FOLD_H
+#define FOLD_H
+
+struct edge_info;
+
+void fold_init(void);
+
+void fold_new_tag(ULong tag);
+
+UWord get_free_tag(void);
+void check_enough_tags(Word needed);
+void mark_tag_in_use(UWord tag);
+
+void fold_new_edge(ULong src, ULong dest, int capacity,
+		   struct edge_info *edge);
+void output_folded_edges_stderr(void);
+void output_folded_edges_filename(const char *);
+void run_max_flow_program(const char *fname);
+void run_incremental_max_flow(void);
+
+#endif
diff -urN exp-flowcheck-base/Makefile.am exp-flowcheck/Makefile.am
--- exp-flowcheck-base/Makefile.am	2016-08-30 16:12:49.880946802 -0500
+++ exp-flowcheck/Makefile.am	2016-08-30 15:23:13.950282444 -0500
@@ -1,6 +1,6 @@
 include $(top_srcdir)/Makefile.tool.am
 
-EXTRA_DIST = docs/mc-manual.xml docs/mc-tech-docs.xml
+EXTRA_DIST =
 
 #----------------------------------------------------------------------------
 # Headers
@@ -10,63 +10,72 @@
 	flowcheck.h
 
 noinst_HEADERS = \
-	fc_include.h
+	fc_include.h      \
+	fc_translate.h    \
+	trace_translate.h \
+	trace_runtime.h \
+	popcount.h \
+	fold.h
 
 #----------------------------------------------------------------------------
-# flowcheck-<platform>
+# exp-flowcheck-<platform>
 #----------------------------------------------------------------------------
 
-noinst_PROGRAMS  = flowcheck-@VGCONF_ARCH_PRI@-@VGCONF_OS@
+noinst_PROGRAMS  = exp-flowcheck-@VGCONF_ARCH_PRI@-@VGCONF_OS@
 if VGCONF_HAVE_PLATFORM_SEC
-noinst_PROGRAMS += flowcheck-@VGCONF_ARCH_SEC@-@VGCONF_OS@
+noinst_PROGRAMS += exp-flowcheck-@VGCONF_ARCH_SEC@-@VGCONF_OS@
 endif
 
-FLOWCHECK_SOURCES_COMMON = \
+EXP_FLOWCHECK_SOURCES_COMMON = \
 	fc_leakcheck.c \
 	fc_malloc_wrappers.c \
 	fc_main.c \
 	fc_translate.c \
 	fc_machine.c \
-	fc_errors.c
-
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_SOURCES      = \
-	$(FLOWCHECK_SOURCES_COMMON)
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CPPFLAGS     = \
+	fc_errors.c \
+	trace_translate.c \
+	trace_runtime.c \
+	popcount.c \
+	fold.c
+
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_SOURCES      = \
+	$(EXP_FLOWCHECK_SOURCES_COMMON)
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CPPFLAGS     = \
 	$(AM_CPPFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS       = \
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS       = \
 	$(AM_CFLAGS_@VGCONF_PLATFORM_PRI_CAPS@) -O2
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_DEPENDENCIES = \
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_DEPENDENCIES = \
 	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_PRI_CAPS@)
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDADD        = \
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDADD        = \
 	$(TOOL_LDADD_@VGCONF_PLATFORM_PRI_CAPS@)
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS      = \
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS      = \
 	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
-flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LINK = \
+exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LINK = \
 	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
 	@VALT_LOAD_ADDRESS_PRI@ \
 	$(LINK) \
-	$(flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS) \
-	$(flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS)
+	$(exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS) \
+	$(exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS)
 
 if VGCONF_HAVE_PLATFORM_SEC
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_SOURCES      = \
-	$(FLOWCHECK_SOURCES_COMMON)
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CPPFLAGS     = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_SOURCES      = \
+	$(EXP_FLOWCHECK_SOURCES_COMMON)
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CPPFLAGS     = \
 	$(AM_CPPFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS       = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS       = \
 	$(AM_CFLAGS_@VGCONF_PLATFORM_SEC_CAPS@) -O2
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_DEPENDENCIES = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_DEPENDENCIES = \
 	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_SEC_CAPS@)
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDADD        = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDADD        = \
 	$(TOOL_LDADD_@VGCONF_PLATFORM_SEC_CAPS@)
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS      = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS      = \
 	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
-flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LINK = \
+exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LINK = \
 	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
 	@VALT_LOAD_ADDRESS_SEC@ \
 	$(LINK) \
-	$(flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS) \
-	$(flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS)
+	$(exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS) \
+	$(exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS)
 endif
 
 # fc_main.c contains the helper function for flowcheck that get called
@@ -75,42 +84,42 @@
 fc_main.o: CFLAGS += -fomit-frame-pointer
 
 #----------------------------------------------------------------------------
-# vgpreload_flowcheck-<platform>.so
+# vgpreload_exp-flowcheck-<platform>.so
 #----------------------------------------------------------------------------
 
-noinst_PROGRAMS += vgpreload_flowcheck-@VGCONF_ARCH_PRI@-@VGCONF_OS@.so
+noinst_PROGRAMS += vgpreload_exp-flowcheck-@VGCONF_ARCH_PRI@-@VGCONF_OS@.so
 if VGCONF_HAVE_PLATFORM_SEC
-noinst_PROGRAMS += vgpreload_flowcheck-@VGCONF_ARCH_SEC@-@VGCONF_OS@.so
+noinst_PROGRAMS += vgpreload_exp-flowcheck-@VGCONF_ARCH_SEC@-@VGCONF_OS@.so
 endif
 
 if VGCONF_OS_IS_DARWIN
-noinst_DSYMS = $(noinst_PROGRAMS)
+noinst_DSYMS =- $(noinst_PROGRAMS)
 endif
 
-VGPRELOAD_FLOWCHECK_SOURCES_COMMON = fc_replace_strmem.c
+VGPRELOAD_EXP_FLOWCHECK_SOURCES_COMMON = fc_replace_strmem.c
 
-vgpreload_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_SOURCES      = \
-	$(VGPRELOAD_FLOWCHECK_SOURCES_COMMON)
-vgpreload_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_CPPFLAGS     = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_SOURCES      = \
+	$(VGPRELOAD_EXP_FLOWCHECK_SOURCES_COMMON)
+vgpreload_exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_CPPFLAGS     = \
 	$(AM_CPPFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
-vgpreload_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_CFLAGS       = \
-	$(AM_CFLAGS_PSO_@VGCONF_PLATFORM_PRI_CAPS@) -O2
-vgpreload_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_DEPENDENCIES = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_CFLAGS       = \
+	$(AM_CFLAGS_PSO_@VGCONF_PLATFORM_PRI_CAPS@)
+vgpreload_exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_DEPENDENCIES = \
 	$(LIBREPLACEMALLOC_@VGCONF_PLATFORM_PRI_CAPS@)
-vgpreload_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_LDFLAGS      = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_PRI@_@VGCONF_OS@_so_LDFLAGS      = \
 	$(PRELOAD_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@) \
 	$(LIBREPLACEMALLOC_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
 
 if VGCONF_HAVE_PLATFORM_SEC
-vgpreload_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_SOURCES      = \
-	$(VGPRELOAD_FLOWCHECK_SOURCES_COMMON)
-vgpreload_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_CPPFLAGS     = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_SOURCES      = \
+	$(VGPRELOAD_EXP_FLOWCHECK_SOURCES_COMMON)
+vgpreload_exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_CPPFLAGS     = \
 	$(AM_CPPFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
-vgpreload_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_CFLAGS       = \
-	$(AM_CFLAGS_PSO_@VGCONF_PLATFORM_SEC_CAPS@) -O2
-vgpreload_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_DEPENDENCIES = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_CFLAGS       = \
+	$(AM_CFLAGS_PSO_@VGCONF_PLATFORM_SEC_CAPS@)
+vgpreload_exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_DEPENDENCIES = \
 	$(LIBREPLACEMALLOC_@VGCONF_PLATFORM_SEC_CAPS@)
-vgpreload_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_LDFLAGS      = \
+vgpreload_exp_flowcheck_@VGCONF_ARCH_SEC@_@VGCONF_OS@_so_LDFLAGS      = \
 	$(PRELOAD_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@) \
 	$(LIBREPLACEMALLOC_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
 endif
diff -urN exp-flowcheck-base/popcount.c exp-flowcheck/popcount.c
--- exp-flowcheck-base/popcount.c	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/popcount.c	2016-08-30 15:23:14.012288339 -0500
@@ -0,0 +1,91 @@
+/*--------------------------------------------------------------------*/
+/*--- Count how many of the bits in a value are set to 1.          ---*/
+/*---                                                   popcount.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_tool_basics.h"
+#include "pub_tool_hashtable.h"     // For fc_include.h
+#include "pub_tool_tooliface.h"     // For fc_include.h
+#include "pub_tool_poolalloc.h"     // For fc_include.h
+#include "fc_include.h"
+#include "popcount.h"
+
+static const char popcount_table[] = {
+    0,1,1,2, 1,2,2,3, 1,2,2,3, 2,3,3,4, 1,2,2,3, 2,3,3,4, 2,3,3,4, 3,4,4,5,
+    1,2,2,3, 2,3,3,4, 2,3,3,4, 3,4,4,5, 2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6,
+    1,2,2,3, 2,3,3,4, 2,3,3,4, 3,4,4,5, 2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6,
+    2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6, 3,4,4,5, 4,5,5,6, 4,5,5,6, 5,6,6,7,
+
+    1,2,2,3, 2,3,3,4, 2,3,3,4, 3,4,4,5, 2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6,
+    2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6, 3,4,4,5, 4,5,5,6, 4,5,5,6, 5,6,6,7,
+    2,3,3,4, 3,4,4,5, 3,4,4,5, 4,5,5,6, 3,4,4,5, 4,5,5,6, 4,5,5,6, 5,6,6,7,
+    3,4,4,5, 4,5,5,6, 4,5,5,6, 5,6,6,7, 4,5,5,6, 5,6,6,7, 5,6,6,7, 6,7,7,8
+};
+
+int popcount8(UChar val) {
+    return popcount_table[val];
+}
+
+int popcount16(UShort val) {
+    return popcount_table[ val        & 0xff] +
+	   popcount_table[(val >>  8) & 0xff];
+}
+
+int popcount32(UInt val) {
+    return popcount_table[ val        & 0xff] +
+	   popcount_table[(val >>  8) & 0xff] +
+	   popcount_table[(val >> 16) & 0xff] +
+	   popcount_table[(val >> 24) & 0xff];
+}
+
+int popcount64(ULong val) {
+    return popcount_table[ val        & 0xff] +
+	   popcount_table[(val >>  8) & 0xff] +
+	   popcount_table[(val >> 16) & 0xff] +
+	   popcount_table[(val >> 24) & 0xff] +
+	   popcount_table[(val >> 32) & 0xff] +
+	   popcount_table[(val >> 40) & 0xff] +
+	   popcount_table[(val >> 48) & 0xff] +
+	   popcount_table[(val >> 56) & 0xff];
+}
+
+Int FC_(helperc_POPCOUNT8) (UInt  val) {
+    return popcount8(val & 0xff);
+}
+
+Int FC_(helperc_POPCOUNT16)(UInt  val) {
+    return popcount16(val & 0xffff);
+}
+
+Int FC_(helperc_POPCOUNT32)(UInt  val) {
+    return popcount32(val);
+}
+
+Int FC_(helperc_POPCOUNT64)(ULong val) {
+    return popcount64(val);
+}
diff -urN exp-flowcheck-base/popcount.h exp-flowcheck/popcount.h
--- exp-flowcheck-base/popcount.h	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/popcount.h	2016-08-30 15:23:14.015477004 -0500
@@ -0,0 +1,39 @@
+/*--------------------------------------------------------------------*/
+/*--- Count how many of the bits in a value are set to 1.          ---*/
+/*---                                                   popcount.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+int popcount8 (UChar  val);
+int popcount16(UShort val);
+int popcount32(UInt   val);
+int popcount64(ULong  val);
+
+Int FC_(helperc_POPCOUNT8) (UInt  val);
+Int FC_(helperc_POPCOUNT16)(UInt  val);
+Int FC_(helperc_POPCOUNT32)(UInt  val);
+Int FC_(helperc_POPCOUNT64)(ULong val);
diff -urN exp-flowcheck-base/trace_runtime.c exp-flowcheck/trace_runtime.c
--- exp-flowcheck-base/trace_runtime.c	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/trace_runtime.c	2016-08-30 15:23:14.021603704 -0500
@@ -0,0 +1,871 @@
+/*--------------------------------------------------------------------*/
+/*--- Runtime operations related to constructing a data-flow       ---*/
+/*--- graph of program execution.                                  ---*/
+/*---                                              trace_runtime.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_tool_basics.h"
+#include "pub_tool_aspacemgr.h"
+#include "pub_tool_hashtable.h"     // For fc_include.h
+#include "pub_tool_debuginfo.h"
+#include "pub_tool_machine.h"
+#include "pub_tool_mallocfree.h"
+#include "pub_tool_poolalloc.h"
+#include "pub_tool_threadstate.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_oset.h"
+#include "pub_tool_vki.h"
+#include "pub_tool_libcbase.h"
+#include "pub_tool_libcfile.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_libcassert.h"
+#include "fc_include.h"
+#include "fc_translate.h" /* To get FC_(tags_guest_offset) */
+#include "trace_runtime.h"
+#include "fold.h"
+
+#define MORE_DEBUG
+
+#if VG_WORDSIZE == 4
+#define HIGH_BITS(addr) ((addr) >> 16)
+#define LOW_BITS(addr) ((addr) & 0xffff)
+#define PAGE_SIZE 65536
+#define TABLE_SIZE 65536
+#else
+#error Only 32-bit architectures supported
+#endif
+
+static ULong zero_page[PAGE_SIZE];
+
+static ULong *page_table[TABLE_SIZE];
+
+#define MAX_FLOW_EXCEPTIONS 30
+struct flow_region {
+   Addr start;
+   SizeT size;
+   ULong src_tag;
+   int num_exceptions;
+   Addr exceptions[MAX_FLOW_EXCEPTIONS];
+};
+
+#define MAX_FLOW_REGIONS 40
+static struct flow_region flow_regions[MAX_FLOW_REGIONS];
+static int num_flow_regions;
+
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+
+/* Hash of the calling context, a la "Probabilistic Calling Context",
+   Bond and McKinley OOPSLA 2007 */
+static ULong callstack_hash = 0;
+static Addr last_callsite;
+
+/* Tourist information: graph size before collapsing */
+static ULong total_nodes = 0;
+static ULong total_edges = 0;
+
+/* File descriptor to which graph edges should be output. The value of 2
+   is special, since that's where Valgrind's printf goes. */
+static Int graph_output_fd;
+
+void output_graph_stats(void) {
+   VG_(printf)("Uncollapsed graph had %lld nodes, %lld edges\n",
+	       total_nodes, total_edges);
+   if (graph_output_fd != 2) {
+      VG_(close)(graph_output_fd);
+   }
+}
+
+static Word edge_compare(const void *key, const void *elem) {
+   const ULong *key_llp = key;
+   const struct edge_info *edge = (const struct edge_info *)elem;
+   if (key_llp[0] < edge->location)
+      return -1;
+   else if (key_llp[0] > edge->location)
+      return 1;
+   else if (key_llp[1] < edge->context)
+      return -1;
+   else if (key_llp[1] > edge->context)
+      return 1;
+   else {
+      tl_assert(key_llp[0] == edge->location);
+      tl_assert(key_llp[1] == edge->context);
+      return 0;
+   }
+   /*return (Word)(key_llp[0] - edge->location)
+     || (Word)(key_llp[1] - edge->context);*/
+}
+
+void init_tag_memory(void) {
+   int i;
+   for (i = 0; i < TABLE_SIZE; i++)
+      page_table[i] = zero_page;
+   edges = VG_(OSetGen_Create)(offsetof(struct edge_info, location),
+			       edge_compare, VG_(malloc), "fc.tr.itm1", VG_(free));
+   if (FC_(clo_graph_file)) {
+      SysRes sr;
+      sr = VG_(open)(FC_(clo_graph_file), VKI_O_WRONLY|VKI_O_CREAT|VKI_O_TRUNC,
+		     0666);
+      tl_assert(!sr_isError(sr));
+      graph_output_fd = (int)sr_Res(sr);
+   } else {
+      graph_output_fd = 2;
+   }
+   if (FC_(clo_folding_level)) {
+      fold_init();
+      fold_new_tag(SOURCE_TAG);
+      fold_new_tag(SINK_TAG);
+   }
+}
+
+static ULong get_single_tag(Addr addr) {
+   /*VG_(printf)("tag[%08x] is %lld (flat)\n", addr,
+     page_table[HIGH_BITS(addr)][LOW_BITS(addr)]);*/
+   return page_table[HIGH_BITS(addr)][LOW_BITS(addr)];
+}
+
+static void set_single_tag(Addr addr, ULong tag) {
+   if (page_table[HIGH_BITS(addr)] == zero_page) {
+      int i;
+      ULong *page = VG_(am_shadow_alloc)(PAGE_SIZE * sizeof(ULong));
+      if (!page)
+	 VG_(out_of_memory_NORETURN)("flowcheck: allocate new tag page",
+				     PAGE_SIZE * sizeof(ULong));
+      for (i = 0; i < PAGE_SIZE; i++)
+	 page[i] = 0;
+      page_table[HIGH_BITS(addr)] = page;
+   }
+   /*VG_(printf)("tag[%08x] := %lld (flat)\n", addr, tag);*/
+   page_table[HIGH_BITS(addr)][LOW_BITS(addr)] = tag;
+}
+
+void mark_special_tags(void);
+
+void mark_memory_tags(void) {
+   int i;
+   for (i = 0; i < TABLE_SIZE; i++) {
+      int j;
+      ULong *page = page_table[i];
+      if (page == zero_page)
+	 continue;
+      for (j = 0; j < PAGE_SIZE; j++)
+	 if (page[j] != 0)
+	    mark_tag_in_use(page[j]);
+   }
+   for (i = 0; i < num_flow_regions; i++) {
+      mark_tag_in_use(flow_regions[i].src_tag);
+   }
+   mark_special_tags();
+}
+
+static void new_edge(ULong src, ULong dest, int capacity,
+		     const char *type, ULong location) {
+   tl_assert(src != SINK_TAG && dest != SOURCE_TAG);
+   /* Graph should be a DAG, but node numbers aren't always increasing */
+   /* tl_assert(dest == SINK_TAG || dest > src); */
+   total_edges++;
+   if (capacity == 0) /* can discard immediately */
+      return;
+   if (FC_(clo_folding_level) == 0) {
+      Addr eip = VG_(get_IP)(VG_(get_running_tid)());
+      const HChar *buf;
+      buf = VG_(describe_IP)(eip, NULL);
+      if (src == SOURCE_TAG)
+	 src = 0;
+      if (dest == SINK_TAG)
+	 dest = -1;
+      if (graph_output_fd == 2) {
+	 VG_(printf)("%lld %lld %d %llx:%llx %s (%s)\n", src, dest,
+		     (Int)capacity, location, callstack_hash, buf, type);
+      } else {
+	 fdprintf(graph_output_fd, "%lld %lld %d %llx:%llx %s (%s)\n",
+		  src, dest, (Int)capacity, location,
+		  callstack_hash, buf, type);
+      }
+   } else {
+      ULong key[2];
+      struct edge_info *edge;
+      ULong context;
+      ULong loc;
+      if (FC_(clo_folding_level) <= 10) {
+	 context = callstack_hash;
+      } else if (FC_(clo_folding_level) <= 50) {
+	 context = last_callsite;
+      } else {
+	 context = 0;
+      }
+      /*loc = (location << 8) | ((int)type & 0xff);*/
+      loc = location;
+      key[0] = loc;
+      key[1] = context;
+      edge = VG_(OSetGen_Lookup)(edges, &key);
+      if (!edge) {
+	 Addr eip = VG_(get_IP)(VG_(get_running_tid)());
+	 const HChar *buf;
+	 edge = VG_(OSetGen_AllocNode)(edges, sizeof(struct edge_info));
+	 edge->location = loc;
+	 edge->context = context;
+	 edge->eip = eip;
+	 buf = VG_(describe_IP)(eip, NULL);
+	 edge->eip_descr = VG_(strdup)("fc.tr.ne.1", buf);
+	 edge->type_descr = type;
+	 edge->src_tag = edge->dest_tag = NO_TAG;
+	 edge->total_capacity = 0;
+	 VG_(OSetGen_Insert)(edges, edge);
+      }
+      fold_new_edge(src, dest, capacity, edge);
+   }
+}
+
+/* Used only when FC_(clo_folding_level) == 0. In that case, we assign
+   new tags freely and never garbage collect them. But when we're folding,
+   we need to keep state for each tag; then we keep only a limited number
+   around at once, and reuse indices by garbage collecting. */
+static ULong next_tag = FIRST_TAG;
+
+static ULong get_next_tag(void) {
+   total_nodes++;
+   if (FC_(clo_folding_level))
+      return get_free_tag();
+   else
+      return next_tag++;
+}
+
+static void add_flow_region(Addr start, SizeT size, ULong src_tag,
+			    ULong location) {
+   int i;
+   Addr end = start + size;
+   for (i = 0; i < num_flow_regions; i++) {
+      Addr old_start = flow_regions[i].start;
+      int old_size =  flow_regions[i].size;
+      Addr old_end = old_start + old_size;
+      if (!(end < old_start) && !(old_end < start)) {
+         /* overlapping or adjacent regions: merge */
+         Addr new_end;
+         int j;
+	 ULong new_tag = get_next_tag();
+         flow_regions[i].start = MIN(start, old_start);
+         new_end = MAX(end, old_end);
+         flow_regions[i].size = new_end - flow_regions[i].start;
+         /*if (flow_regions[i].start != old_start || new_end != old_end)
+            VG_(printf)("Expanding flow region %d to 0x%08x to 0x%08x"
+                        " (size %d)\n",
+                        i, flow_regions[i].start, end, flow_regions[i].size);
+	 else
+            VG_(printf)("Renewing flow region %d to 0x%08x to 0x%08x"
+                        " (size %d)\n",
+                        i, flow_regions[i].start, end, flow_regions[i].size);*/
+         for (j = 0; j < flow_regions[i].num_exceptions; j++) {
+            Addr a = flow_regions[i].exceptions[j];
+            if (a >= start && a < end) {
+               Addr old_num = flow_regions[i].num_exceptions;
+               /*VG_(printf)("Removing exception for 0x%08x\n", a);*/
+               flow_regions[i].exceptions[j] =
+                  flow_regions[i].exceptions[old_num - 1];
+               --flow_regions[i].num_exceptions;
+	       j--;
+            }
+         }
+	 new_edge(flow_regions[i].src_tag, new_tag, old_size * 8,
+		     "flow region cascade old", location);
+	 new_edge(src_tag, new_tag, size * 8, "flow region cascade new",
+		     location + 1);
+	 flow_regions[i].src_tag = new_tag;
+         return;
+      }
+   }
+   i = num_flow_regions++;
+   tl_assert(num_flow_regions <= MAX_FLOW_REGIONS);
+   flow_regions[i].start = start;
+   flow_regions[i].size = size;
+   flow_regions[i].num_exceptions = 0;
+   flow_regions[i].src_tag = src_tag;
+   /*VG_(printf)("Adding flow region %d from 0x%08x to 0x%08x (size %d)\n",
+     i, start, end, size);*/
+}
+
+static void partial_give_up_on_flow_region(int i, SizeT size, ULong location) {
+   Addr a;
+   Addr end = flow_regions[i].start + size;
+   Bool partial = (size < flow_regions[i].size);
+   /*const char *partial_str = partial ? "partial " : "";*/
+   int j;
+
+   /*if (partial) {
+      VG_(printf)("Scaling back flow region %d\n", i);
+   } else {
+      VG_(printf)("Destroying flow region %d\n", i);
+   }
+   VG_(printf)("Exceptions were:\n");
+   for (j = 0; j < flow_regions[i].num_exceptions; j++) {
+      Addr addr = flow_regions[i].exceptions[j];
+      VG_(printf)("    0x%08x\n", addr);
+      }*/
+
+   a = flow_regions[i].start;
+   /*VG_(printf)("On %sdestruction, tainting from 0x%08x to 0x%08x\n",
+     partial_str, a, end);*/
+   for (; a < end; a++) {
+      Bool is_exception = False;
+      for (j = 0; j < flow_regions[i].num_exceptions; j++) {
+	 Addr a2 = flow_regions[i].exceptions[j];
+	 tl_assert(a2 >= flow_regions[i].start && a2 < end);
+	 if (a == a2) {
+	    /* VG_(printf)("  except 0x%08x\n", a);*/
+	    is_exception = True;
+	    break;
+	 }
+      }
+      if (!is_exception) {
+	 ULong old_tag = get_single_tag(a);
+	 ULong new_tag = get_next_tag();
+	 if (old_tag)
+	    new_edge(old_tag, new_tag, 8, partial ?
+			"old value in partial give up" :
+			"old value in give up", location + 2 * partial);
+	 new_edge(flow_regions[i].src_tag, new_tag, 8, partial ?
+		     "enclosed output in partial give up" :
+		     "enclosed output in give up", location + 2*partial + 1);
+	 set_single_tag(a, new_tag);
+      }
+   }
+   flow_regions[i].num_exceptions = 0;
+   flow_regions[i].start += size;
+   flow_regions[i].size -= size;
+}
+
+static void give_up_on_flow_region(int i, ULong location) {
+  partial_give_up_on_flow_region(i, flow_regions[i].size, location);
+   flow_regions[i] = flow_regions[--num_flow_regions];
+}
+
+static void add_flow_exception(int i, Addr addr, ULong location) {
+   if (addr == flow_regions[i].start) {
+      flow_regions[i].start++;
+      flow_regions[i].size--;
+      return;
+   }
+   flow_regions[i].exceptions[flow_regions[i].num_exceptions++] = addr;
+   if (flow_regions[i].num_exceptions == MAX_FLOW_EXCEPTIONS) {
+      int j;
+      Addr max_exception = flow_regions[i].exceptions[0];
+      SizeT partial_size;
+      for (j = 1; j < flow_regions[i].num_exceptions; j++) {
+	 if (flow_regions[i].exceptions[j] > max_exception)
+	    max_exception = flow_regions[i].exceptions[j];
+      }
+      /* VG_(printf)("Largest exeception is 0x%08x\n", max_exception);*/
+      partial_size = max_exception - flow_regions[i].start + 1;
+      if (2 * partial_size < flow_regions[i].size)
+	 partial_give_up_on_flow_region(i, partial_size, location);
+      else
+	 give_up_on_flow_region(i, location);
+   }
+}
+
+ULong get_tag(Addr addr, ULong location) {
+   int i;
+   for (i = 0; i < num_flow_regions; i++) {
+      /*VG_(printf)("Checking 0x%08x versus 0x%08x and 0x%08x (flow)\n",
+		  addr, flow_regions[i].start,
+		  flow_regions[i].start + flow_regions[i].size);*/
+      if (addr >= flow_regions[i].start &&
+	  addr < flow_regions[i].start + flow_regions[i].size) {
+	 int j;
+	 ULong old_tag, new_tag;
+	 for (j = 0; j < flow_regions[i].num_exceptions; j++)
+	    if (flow_regions[i].exceptions[j] == addr) {
+	       /*VG_(printf)("0x%08x is an exception\n", addr);*/
+	       return get_single_tag(addr);
+	    }
+	 old_tag = get_single_tag(addr);
+	 new_tag = get_next_tag();
+	 if (old_tag)
+	    new_edge(old_tag, new_tag, 8,
+			"old value of enclosed output region", location+31);
+	 new_edge(flow_regions[i].src_tag, new_tag, 8,
+		     "enclosed output in region", location+32);
+	 set_single_tag(addr, new_tag);
+	 if (flow_regions[i].num_exceptions < MAX_FLOW_EXCEPTIONS / 2)
+	    add_flow_exception(i, addr, location);
+	 return new_tag;
+      }
+   }
+   return get_single_tag(addr);
+}
+
+void set_tag(Addr addr, ULong tag, ULong location) {
+   int i;
+   for (i = 0; i < num_flow_regions; i++) {
+      if (addr >= flow_regions[i].start &&
+	  addr < flow_regions[i].start + flow_regions[i].size) {
+	 int j;
+	 for (j = 0; j < flow_regions[i].num_exceptions; j++)
+	    if (flow_regions[i].exceptions[j] == addr) {
+	       /*VG_(printf)("0x%08x is already an exception\n", addr);*/
+	       return set_single_tag(addr, tag);
+	    }
+	 add_flow_exception(i, addr, location);
+	 return set_single_tag(addr, tag);
+      }
+   }
+   return set_single_tag(addr, tag);
+}
+
+void make_freshly_tagged_input(Addr start, SizeT len, ULong location) {
+   SizeT i;
+   check_enough_tags(len);
+   for (i = 0; i < len; i++) {
+      ULong tag = get_next_tag();
+      set_tag(start + i, tag, location);
+      new_edge(SOURCE_TAG, tag, 8, "input byte", location);
+   }
+}
+
+void make_freshly_tagged_register(ThreadId tid, OffT offset, int capacity,
+				  ULong location) {
+   ULong tag;
+   OffT tag_offset;
+   check_enough_tags(1);
+   tag = get_next_tag();
+   tag_offset = FC_(tags_guest_offset) + 8 * offset;
+   VG_(set_shadow_regs_area)(tid, 0, tag_offset, sizeof(ULong), (UChar*)&tag);
+   new_edge(SOURCE_TAG, tag, capacity, "input register", location);
+}
+
+static ULong leak_cascade = 0;
+
+void count_leaked_tags(Addr start, SizeT len, ULong location) {
+   SizeT i;
+   ULong next_cascade = 0;
+   check_enough_tags(2 * len);
+   for (i = 0; i < len; i++) {
+      ULong tag = get_tag(start + i, location);
+      if (tag) {
+	 if (leak_cascade) {
+	    ULong join_node = get_next_tag();
+	    if (!next_cascade) {
+	       next_cascade = get_next_tag();
+	       new_edge(leak_cascade, next_cascade, FC_(total_bits_leaked),
+			   "global leak cascade", location);
+	    }
+	    new_edge(tag, join_node, 8, "output byte to join", location+1);
+	    new_edge(leak_cascade, join_node, 8, "global leak to output",
+			location+2);
+	    tag = join_node;
+	 }
+	 new_edge(tag, SINK_TAG, 8, "output byte", location+3);
+      } else if (leak_cascade) {
+	new_edge(leak_cascade, SINK_TAG, 8, "global leak on public output",
+		    location+4);
+      }
+   }
+   if (next_cascade) {
+      leak_cascade = next_cascade;
+   }
+   if (FC_(clo_incremental))
+      run_incremental_max_flow();
+}
+
+static ULong combine2(ULong tag1, ULong tag2, ULong location) {
+   if (tag1 == tag2) {
+      return tag1;
+   } else if (!tag1) {
+      return tag2;
+   } else if (!tag2) {
+      return tag1;
+   } else {
+      ULong new_tag = get_next_tag();
+      new_edge(tag1, new_tag, 8, "combine2", location + 1);
+      new_edge(tag2, new_tag, 8, "combine2", location + 2);
+      return new_tag;
+   }
+}
+
+static ULong merge2(ULong tag1, ULong tag2, ULong location) {
+   if (tag1 == tag2)
+      return tag1;
+   else
+      return combine2(tag1, tag2, location);
+}
+
+static ULong combine4(ULong tag1, ULong tag2, ULong tag3, ULong tag4,
+		      ULong location) {
+   if (tag1 == tag2 && tag2 == tag3 && tag3 == tag4) {
+      return tag1;
+   } else if (!tag1 && !tag2 && !tag3) {
+      return tag4;
+   } else if (!tag2 && !tag3 && !tag4) {
+      return tag1;
+   } else if (!tag1 && !tag3 && !tag4) {
+      return tag2;
+   } else if (!tag1 && !tag2 && !tag4) {
+      return tag3;
+   } else {
+      ULong new_tag = get_next_tag();
+      if (tag1)
+	 new_edge(tag1, new_tag, 8, "combine4", location);
+      if (tag2)
+	 new_edge(tag2, new_tag, 8, "combine4", location);
+      if (tag3)
+	 new_edge(tag3, new_tag, 8, "combine4", location);
+      if (tag4)
+	 new_edge(tag4, new_tag, 8, "combine4", location);
+      return new_tag;
+   }
+}
+
+static ULong merge4(ULong tag1, ULong tag2, ULong tag3, ULong tag4,
+		    ULong location) {
+   if (tag1 == tag2 && tag2 == tag3 && tag3 == tag4)
+      return tag1;
+   else
+      return combine4(tag1, tag2, tag3, tag4, location);
+}
+
+ULong FC_(helperc_RESULT_TAG)(ULong tag, UInt width, ULong location) {
+   if (width && !tag) {
+      VG_(printf)("Missing tag for result at %llx\n", location);
+      tl_assert(!width || tag);
+   }
+   if (!tag) {
+      return tag;
+   } else {
+      ULong new_tag;
+      check_enough_tags(1);
+      new_tag = get_next_tag();
+      new_edge(tag, new_tag, width, "result", location + 10);
+      return new_tag;
+   }
+}
+
+ULong FC_(helperc_UNARY_TAG)(ULong tag, UInt width, ULong location) {
+   if (!tag) {
+      return tag;
+   } else {
+      ULong new_tag;
+      check_enough_tags(1);
+      new_tag = get_next_tag();
+      new_edge(tag, new_tag, width, "unary", location + 20);
+      return new_tag;
+   }
+}
+
+ULong FC_(helperc_BINARY_TAGS)(ULong tag1, ULong tag2,
+			       UInt width1, UInt width2,
+			       ULong location) {
+   ULong new_tag;
+   tl_assert(!width1 || tag1);
+   if (width2 && !tag2) {
+      VG_(printf)("Missing tag for second arg at %llx\n", location);
+      tl_assert(!width2 || tag2);
+   }
+   if (!tag1 && !tag2)
+      return 0;
+   check_enough_tags(1);
+   new_tag = get_next_tag();
+   if (tag1)
+      new_edge(tag1, new_tag, width1, "binary arg 1", location + 1);
+   if (tag2)
+      new_edge(tag2, new_tag, width2, "binary arg 2", location + 2);
+   return new_tag;
+}
+
+ULong FC_(helperc_TERNARY_TAGS)(ULong tag1, ULong tag2, ULong tag3,
+				UInt width1, UInt width2, UInt width3,
+				ULong location) {
+   ULong new_tag;
+   if (!tag1 && !tag2 && !tag3)
+      return 0;
+   check_enough_tags(1);
+   new_tag = get_next_tag();
+   if (tag1)
+      new_edge(tag1, new_tag, width1, "ternary arg 1", location + 1);
+   if (tag2)
+      new_edge(tag2, new_tag, width2, "ternary arg 2", location + 2);
+   if (tag3)
+      new_edge(tag3, new_tag, width3, "ternary arg 3", location + 3);
+   return new_tag;
+}
+
+ULong FC_(helperc_FOURARY_TAGS)(ULong tag1, ULong tag2, ULong tag3, ULong tag4,
+				UInt width1, UInt width2, UInt width3,
+				UInt width4, ULong location) {
+   ULong new_tag;
+   if (!tag1 && !tag2 && !tag3 && !tag4)
+      return 0;
+   check_enough_tags(1);
+   new_tag = get_next_tag();
+   if (tag1)
+      new_edge(tag1, new_tag, width1, "fourary arg 1", location + 1);
+   if (tag2)
+      new_edge(tag2, new_tag, width2, "fourary arg 2", location + 2);
+   if (tag3)
+      new_edge(tag3, new_tag, width3, "fourary arg 3", location + 3);
+   if (tag4)
+      new_edge(tag4, new_tag, width4, "fourary arg 4", location + 4);
+   return new_tag;
+}
+
+ULong FC_(helperc_FIVEARY_TAGS)(ULong tag1, ULong tag2, ULong tag3, ULong tag4,
+				ULong tag5,
+				UInt width1, UInt width2, UInt width3,
+				UInt width4, UInt width5, ULong location) {
+   ULong new_tag;
+   if (!tag1 && !tag2 && !tag3 && !tag4 && !tag5)
+      return 0;
+   check_enough_tags(1);
+   new_tag = get_next_tag();
+   if (tag1)
+      new_edge(tag1, new_tag, width1, "fiveary arg 1", location + 1);
+   if (tag2)
+      new_edge(tag2, new_tag, width2, "fiveary arg 2", location + 2);
+   if (tag3)
+      new_edge(tag3, new_tag, width3, "fiveary arg 3", location + 3);
+   if (tag4)
+      new_edge(tag4, new_tag, width4, "fiveary arg 4", location + 4);
+   if (tag5)
+      new_edge(tag5, new_tag, width5, "fiveary arg 5", location + 5);
+   return new_tag;
+}
+
+ULong FC_(helper_LOAD_TAG_1) (Addr addr, ULong addrTag, ULong location) {
+   ULong tag = get_tag(addr, location);
+#ifdef EVEN_MORE_DEBUG
+   Bool ok;
+   UChar vbits8;
+   ok = get_vbits8(addr, &vbits8);
+   if (ok && vbits8 && !tag) {
+      VG_(printf)("Missing tag for loaded byte at %llx\n", location);
+      tl_assert(!vbits8 || tag);
+   }
+#endif
+   return tag;
+}
+
+ULong FC_(helper_LOAD_TAG_2) (Addr addr, ULong addrTag, ULong location) {
+   check_enough_tags(3);
+   return merge2(get_tag(addr, location), get_tag(addr + 1, location),
+		 location);
+}
+
+ULong FC_(helper_LOAD_TAG_4) (Addr addr, ULong addrTag, ULong location) {
+   check_enough_tags(5);
+   return merge4(get_tag(addr, location),     get_tag(addr + 1, location),
+		 get_tag(addr + 2, location), get_tag(addr + 3, location),
+		 location);
+}
+
+ULong FC_(helper_LOAD_TAG_8) (Addr addr, ULong addrTag, ULong location) {
+   check_enough_tags(13);
+   return merge4(merge2(get_tag(addr, location),
+			get_tag(addr + 1, location), location),
+		 merge2(get_tag(addr + 2, location),
+			get_tag(addr + 3, location), location + 4),
+		 merge2(get_tag(addr + 4, location),
+			get_tag(addr + 5, location), location + 8),
+		 merge2(get_tag(addr + 6, location),
+			get_tag(addr + 7, location), location + 12),
+		 location + 16);
+}
+
+ULong FC_(helper_LOAD_TAG_16)(Addr addr, ULong addrTag, ULong location) {
+   check_enough_tags(21);
+   return
+      merge4
+      (merge4(get_tag(addr, location),      get_tag(addr + 1, location),
+	      get_tag(addr + 2, location),  get_tag(addr + 3, location),
+	      location),
+       merge4(get_tag(addr + 4, location),  get_tag(addr + 5, location),
+	      get_tag(addr + 6, location),  get_tag(addr + 7, location),
+	      location),
+       merge4(get_tag(addr + 8, location),  get_tag(addr + 9, location),
+	      get_tag(addr + 10, location), get_tag(addr + 11, location),
+	      location),
+       merge4(get_tag(addr + 12, location), get_tag(addr + 13, location),
+	      get_tag(addr + 14, location), get_tag(addr + 15, location),
+	      location),
+       location);
+}
+
+static void store_tag(Addr addr, ULong dataTag, ULong location,
+		      Word in_rollback) {
+   if (in_rollback && !FC_(is_escaping)(addr)) {
+      dataTag = combine2(dataTag, get_tag(addr, location), location);
+   }
+   set_tag(addr, dataTag, location);
+}
+
+void FC_(helper_STORE_TAG_1) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback) {
+#ifdef EVEN_MORE_DEBUG
+   /* Assuming the V bits get written first */
+   Bool ok;
+   UChar vbits8;
+   ok = get_vbits8(addr, &vbits8);
+   if (ok && vbits8 && !dataTag) {
+      VG_(printf)("Missing tag for stored byte at %llx\n", location);
+      tl_assert(!vbits8 || dataTag);
+   }
+#endif
+   store_tag(addr, dataTag, location, in_rollback);
+}
+
+void FC_(helper_STORE_TAG_2) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback) {
+   store_tag(addr,     dataTag, location, in_rollback);
+   store_tag(addr + 1, dataTag, location, in_rollback);
+}
+
+void FC_(helper_STORE_TAG_4) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback) {
+   store_tag(addr,     dataTag, location, in_rollback);
+   store_tag(addr + 1, dataTag, location, in_rollback);
+   store_tag(addr + 2, dataTag, location, in_rollback);
+   store_tag(addr + 3, dataTag, location, in_rollback);
+}
+
+void FC_(helper_STORE_TAG_8) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback) {
+   store_tag(addr,     dataTag, location, in_rollback);
+   store_tag(addr + 1, dataTag, location, in_rollback);
+   store_tag(addr + 2, dataTag, location, in_rollback);
+   store_tag(addr + 3, dataTag, location, in_rollback);
+   store_tag(addr + 4, dataTag, location, in_rollback);
+   store_tag(addr + 5, dataTag, location, in_rollback);
+   store_tag(addr + 6, dataTag, location, in_rollback);
+   store_tag(addr + 7, dataTag, location, in_rollback);
+}
+
+void FC_(helper_STORE_TAG_16)(Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback) {
+   int i;
+   for (i = 0; i < 16; i++)
+      store_tag(addr + i, dataTag, location, in_rollback);
+}
+
+static ULong enclose_node = 0;
+
+void mark_special_tags(void) {
+   if (leak_cascade)
+      mark_tag_in_use(leak_cascade);
+   if (enclose_node)
+      mark_tag_in_use(enclose_node);
+}
+
+void trace_enter_enclose() {
+   enclose_node = 0;
+}
+
+void trace_leak(ULong tag, Int width, ULong location) {
+   tl_assert(tag);
+   check_enough_tags(1);
+   if (!leak_cascade)
+      leak_cascade = get_next_tag();
+   new_edge(tag, leak_cascade, width, "global leak", location);
+}
+
+void trace_leak_enclosed(ULong tag, Int width, ULong location) {
+   tl_assert(width > 0);
+   if (!tag) {
+      VG_(printf)("Leak (%d bits) with no tag\n", width);
+      tl_assert(0);
+   }
+
+   if (!enclose_node) {
+      check_enough_tags(1);
+      enclose_node = get_next_tag();
+   }
+
+   new_edge(tag, enclose_node, width, "enclosed leak", location + 41);
+}
+
+void trace_enclosed_output(Addr addr, ULong location) {
+   ULong old_tag = get_tag(addr, location + 1);
+   ULong new_tag;
+   check_enough_tags(1);
+   new_tag = get_next_tag();
+   if (!enclose_node) {
+      VG_(printf)("Enclosed output at %p with no node\n", (void *)addr);
+      tl_assert(0);
+   }
+   if (old_tag)
+      new_edge(old_tag, new_tag, 8, "old value of enclosed output",
+	       location + 2);
+   new_edge(enclose_node, new_tag, 8, "enclosed output", location + 3);
+   set_single_tag(addr, new_tag);
+}
+
+void trace_enclosed_output_region(Addr addr, Int size, ULong location) {
+   if (!enclose_node) {
+      VG_(printf)("Enclosed output at %p with no node\n", (void *)addr);
+      tl_assert(0);
+   }
+   check_enough_tags(size);
+   add_flow_region(addr, size, enclose_node, location);
+}
+
+#define CALL_DEPTH 4096
+static Addr shadow_call_stack[CALL_DEPTH];
+static ULong shadow_stack_hashes[CALL_DEPTH];
+static int call_level = 0;
+
+void FC_(helper_WATCH_CALL)(Addr addr) {
+   /*int i;*/
+   Addr eip = VG_(get_IP)(VG_(get_running_tid)());
+   /*for (i = 0; i < call_level; i++)
+      VG_(printf)(" ");
+      VG_(printf)("Saw call to 0x%llx at 0x%llx\n", (ULong)addr, (ULong)eip);*/
+   last_callsite = eip;
+   shadow_call_stack[call_level] = eip;
+   shadow_stack_hashes[call_level] = callstack_hash;
+   call_level++;
+   callstack_hash = 3 * callstack_hash + eip;
+   /*VG_(printf)("Call stack hash is 0x%llx\n", callstack_hash);*/
+   tl_assert(call_level < CALL_DEPTH);
+}
+
+void FC_(helper_WATCH_RET)(void) {
+   /*int i;*/
+   Addr old_callsite;
+   --call_level;
+   if (call_level < 0)
+      call_level = 0;
+   old_callsite = shadow_call_stack[call_level];
+   callstack_hash = shadow_stack_hashes[call_level];
+   /*for (i = 0; i < call_level; i++)
+      VG_(printf)(" ");
+      VG_(printf)("Saw return, popping 0x%llx\n", (ULong)old_callsite);*/
+   last_callsite = old_callsite;
+   /*VG_(printf)("Call stack hash is 0x%llx\n", callstack_hash);*/
+}
+
+void shadow_stack_iterate(void) {
+   callstack_hash += 541; /* perturb by a random value */
+   /*VG_(printf)("Iterating call stack hash to 0x%llx\n", callstack_hash);*/
+}
diff -urN exp-flowcheck-base/trace_runtime.h exp-flowcheck/trace_runtime.h
--- exp-flowcheck-base/trace_runtime.h	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/trace_runtime.h	2016-08-30 15:23:14.025201304 -0500
@@ -0,0 +1,113 @@
+/*--------------------------------------------------------------------*/
+/*--- Runtime operations related to constructing a data-flow       ---*/
+/*--- graph of program execution.                                  ---*/
+/*---                                              trace_runtime.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef TRACE_RUNTIME_H
+#define TRACE_RUNTIME_H
+
+#include "pub_tool_oset.h"
+
+struct edge_info {
+   ULong location;
+   ULong context;
+   UWord eip;
+   char *eip_descr;
+   const char *type_descr;
+   UWord src_tag;
+   UWord dest_tag;
+   ULong total_capacity;
+};
+
+#define SOURCE_TAG  0
+#define SINK_TAG    1
+#define FIRST_TAG   2
+#define NO_TAG     -1
+
+OSet *edges;
+
+void init_tag_memory(void);
+void mark_memory_tags(void);
+
+ULong get_tag(Addr addr, ULong location);
+void set_tag(Addr addr, ULong tag, ULong location);
+
+void make_freshly_tagged_input(Addr start, SizeT len, ULong location);
+void make_freshly_tagged_register(ThreadId tid, OffT offset, int capacity,
+				  ULong location);
+void count_leaked_tags(Addr start, SizeT len, ULong location);
+
+void trace_enter_enclose(void);
+void trace_leak(ULong tag, Int width, ULong location);
+void trace_leak_enclosed(ULong tag, Int width, ULong location);
+void trace_enclosed_output(Addr addr, ULong location);
+void trace_enclosed_output_region(Addr addr, Int size, ULong location);
+
+void output_graph_stats(void);
+
+ULong FC_(helperc_RESULT_TAG)  (ULong, UInt, ULong);
+ULong FC_(helperc_UNARY_TAG)   (ULong, UInt, ULong);
+ULong FC_(helperc_BINARY_TAGS) (ULong, ULong, UInt, UInt, ULong);
+ULong FC_(helperc_TERNARY_TAGS)(ULong, ULong, ULong, UInt, UInt, UInt, ULong);
+ULong FC_(helperc_FOURARY_TAGS)(ULong, ULong, ULong, ULong,
+				UInt,  UInt,  UInt,  UInt, ULong);
+ULong FC_(helperc_FIVEARY_TAGS)(ULong, ULong, ULong, ULong, ULong,
+				UInt,  UInt,  UInt,  UInt,  UInt, ULong);
+
+ULong FC_(helper_LOAD_TAG_1) (Addr addr, ULong addrTag, ULong location);
+ULong FC_(helper_LOAD_TAG_2) (Addr addr, ULong addrTag, ULong location);
+ULong FC_(helper_LOAD_TAG_4) (Addr addr, ULong addrTag, ULong location);
+ULong FC_(helper_LOAD_TAG_8) (Addr addr, ULong addrTag, ULong location);
+ULong FC_(helper_LOAD_TAG_16)(Addr addr, ULong addrTag, ULong location);
+
+void FC_(helper_STORE_TAG_1) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback);
+void FC_(helper_STORE_TAG_2) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback);
+void FC_(helper_STORE_TAG_4) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback);
+void FC_(helper_STORE_TAG_8) (Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback);
+void FC_(helper_STORE_TAG_16)(Addr addr, ULong addrTag, ULong dataTag,
+			      ULong location, Word in_rollback);
+
+void FC_(helper_BRANCH_TAG)(ULong tag);
+
+void FC_(helper_WATCH_CALL)(UWord addr);
+void FC_(helper_WATCH_RET)(void);
+void shadow_stack_iterate(void);
+
+#define fdprintf(fd, format, args...) ({			\
+         const int _buf_size = 4096;				\
+	 char _buf[_buf_size];					\
+	 VG_(snprintf)(_buf, _buf_size, format, ##args);	\
+	 VG_(write)(fd, (void*)_buf, VG_(strlen)(_buf));	\
+      })
+
+#endif /* TRACE_RUNTIME_H */
diff -urN exp-flowcheck-base/trace_translate.c exp-flowcheck/trace_translate.c
--- exp-flowcheck-base/trace_translate.c	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/trace_translate.c	2016-08-30 15:23:14.028134946 -0500
@@ -0,0 +1,725 @@
+/*--------------------------------------------------------------------*/
+/*--- VEX IR transformations related to constructing a data-flow   ---*/
+/*--- graph of program execution.                                  ---*/
+/*---                                            trace_translate.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_tool_basics.h"
+#include "pub_tool_hashtable.h"     // For fc_include.h
+#include "pub_tool_libcassert.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_poolalloc.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_machine.h"     // VG_(fnptr_to_fnentry)
+#include "pub_tool_xarray.h"
+#include "fc_include.h"
+#include "fc_translate.h"
+#include "trace_translate.h"
+#include "trace_runtime.h"
+#include "popcount.h"
+#include "libvex_guest_offsets.h"
+#include "libvex_ir.h"
+
+
+/* Find the tmp currently shadowing the given original tmp.  If none
+   so far exists, allocate one.  */
+static IRTemp
+findShadowTmp_trace(FCEnv *fce, TraceEnv *trace_env, IRTemp orig) {
+   (void)trace_env;
+   return findShadowTmpT(fce, orig);
+}
+
+/* assign value to tmp */
+#define assign(_bb,_tmp,_expr)   \
+   addStmtToIRSB((_bb), IRStmt_WrTmp((_tmp),(_expr)))
+
+/* add stmt to a bb */
+#define stmt(_bb,_stmt)    \
+   addStmtToIRSB((_bb), (_stmt))
+
+/* build various kinds of expressions */
+#define binop(_op, _arg1, _arg2) IRExpr_Binop((_op),(_arg1),(_arg2))
+#define unop(_op, _arg)          IRExpr_Unop((_op),(_arg))
+#define mkU8(_n)                 IRExpr_Const(IRConst_U8(_n))
+#define mkU16(_n)                IRExpr_Const(IRConst_U16(_n))
+#define mkU32(_n)                IRExpr_Const(IRConst_U32(_n))
+#define mkU64(_n)                IRExpr_Const(IRConst_U64(_n))
+#define mkV128(_n)               IRExpr_Const(IRConst_V128(_n))
+#define mkexpr(_tmp)             IRExpr_RdTmp((_tmp))
+
+/* bind the given expression to a new temporary, and return the
+   temporary.  This effectively converts an arbitrary expression into
+   an atom. */
+static IRAtom* assignNewT ( FCEnv* fce, IRType ty, IRExpr* e ) {
+   return assignNew('T', fce, ty, e);
+}
+
+/* (used for sanity checks only): is this an atom which looks
+   like it's from original code? */
+static Bool isOriginalAtom_trace(FCEnv* fce, TraceEnv *trace_env, IRAtom* a1) {
+   if (a1->tag == Iex_Const)
+      return True;
+   if (a1->tag == Iex_RdTmp && a1->Iex.RdTmp.tmp <  fce->sb->tyenv->types_used)
+      return True;
+   return False;
+}
+
+/* (used for sanity checks only): is this an atom which looks
+   like it's from shadow code? */
+static Bool isShadowAtom_trace(FCEnv* fce, TraceEnv *trace_env, IRAtom* a1) {
+   return isShadowAtom(fce, a1);
+}
+
+static IRExpr *
+shadow_Get_trace(FCEnv *fce, TraceEnv *trace_env, Int offset, IRType ty) {
+   tl_assert(ty != Ity_I1);
+
+   return IRExpr_Get( (8 * offset) +
+		      FC_(shadow_guest_offset) + FC_(tags_guest_offset),
+                      Ity_I64); // Tags are always 64 bits
+}
+
+static IRExpr *
+shadow_GetI_trace(FCEnv *fce, TraceEnv *trace_env, IRRegArray *descr,
+		  IRAtom *ix, Int bias) {
+   IRType ty   = descr->elemTy;
+   IRRegArray *new_descr;
+   tl_assert(ty != Ity_I1);
+   tl_assert(isOriginalAtom_trace(fce, trace_env, ix));
+   new_descr
+      = mkIRRegArray((8 * descr->base) +
+		     FC_(shadow_guest_offset) + FC_(tags_guest_offset),
+		     Ity_I64, descr->nElems); // Tags are always 64 bits
+
+   return IRExpr_GetI(new_descr, ix, bias);
+}
+
+static IRExpr *
+make_popcount(FCEnv *fce, TraceEnv *trace_env, IRExpr *expr) {
+   IRExpr *pop_expr;
+   IRExpr *widened;
+   IRType ty = typeOfIRExpr(fce->sb->tyenv, expr);
+
+   if (expr->tag == Iex_Const) {
+      switch (expr->Iex.Const.con->tag) {
+      case Ico_U1:
+	 return IRExpr_Const(IRConst_U32(expr->Iex.Const.con->Ico.U1));
+      case Ico_U8:
+	 {
+	    UChar val = expr->Iex.Const.con->Ico.U8;
+	    return IRExpr_Const(IRConst_U32(popcount8(val)));
+	 }
+      case Ico_U16:
+	 {
+	    UShort val = expr->Iex.Const.con->Ico.U16;
+	    return IRExpr_Const(IRConst_U32(popcount16(val)));
+	 }
+      case Ico_U32:
+	 {
+	    UInt val = expr->Iex.Const.con->Ico.U32;
+	    return IRExpr_Const(IRConst_U32(popcount32(val)));
+	 }
+      case Ico_U64:
+	 {
+	    ULong val = expr->Iex.Const.con->Ico.U64;
+	    return IRExpr_Const(IRConst_U32(popcount64(val)));
+	 }
+      default:
+	 VG_(tool_panic)("flowcheck trace:make_popcount constant");
+      }
+   }
+
+   switch (ty) {
+   case Ity_I1:
+      pop_expr = IRExpr_Unop(Iop_1Uto32, expr);
+      break;
+   case Ity_I8:
+      widened = assignNewT(fce, Ity_I32, IRExpr_Unop(Iop_8Uto32, expr));
+      pop_expr = mkIRExprCCall(Ity_I32, 0, "FC_(helperc_POPCOUNT8)",
+			       &FC_(helperc_POPCOUNT8),
+			       mkIRExprVec_1(widened));
+      break;
+   case Ity_I16:
+      widened = assignNewT(fce, Ity_I32, IRExpr_Unop(Iop_16Uto32, expr));
+      pop_expr = mkIRExprCCall(Ity_I32, 0, "FC_(helperc_POPCOUNT16)",
+			       &FC_(helperc_POPCOUNT16),
+			       mkIRExprVec_1(widened));
+      break;
+   case Ity_I32:
+      pop_expr = mkIRExprCCall(Ity_I32, 0, "FC_(helperc_POPCOUNT32)",
+			       &FC_(helperc_POPCOUNT32), mkIRExprVec_1(expr));
+      break;
+   case Ity_I64:
+      pop_expr = mkIRExprCCall(Ity_I32, 0, "FC_(helperc_POPCOUNT64)",
+			       &FC_(helperc_POPCOUNT64), mkIRExprVec_1(expr));
+      break;
+   default:
+      ppIRType(ty);
+      VG_(tool_panic)("flowcheck trace:make_popcount");
+      return 0;
+   }
+   return assignNewT(fce, Ity_I32, pop_expr);
+}
+
+static IRExpr *
+expr2tag_Unop_trace(FCEnv *fce, TraceEnv *trace_env, IRAtom *atom) {
+   IRAtom *tatom = expr2tag_trace(fce, trace_env, atom);
+   IRAtom *vatom = expr2vbits(fce, atom);
+   IRAtom *pop;
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom));
+   tl_assert(isShadowAtom(fce, vatom));
+
+   if (atom->tag == Iex_Const)
+     return tatom;
+
+   pop = make_popcount(fce, trace_env, vatom);
+
+   /* Even though this helper has side-effects (i.e., sometimes printing
+      an edge to the trace), it's a clean call because it's appropriate to
+      optimize away if its results are not used. */
+   return mkIRExprCCall(Ity_I64, 0, "FC_(helperc_UNARY_TAG)",
+			&FC_(helperc_UNARY_TAG),
+			mkIRExprVec_3(tatom, pop,
+				      mkU64(trace_env->location++)));
+}
+
+static IRExpr *
+expr2tag_Binop_trace(FCEnv *fce, TraceEnv *trace_env, IROp op,
+		      IRAtom *atom1, IRAtom *atom2) {
+   IRAtom *tatom1 = expr2tag_trace(fce, trace_env, atom1);
+   IRAtom *tatom2 = expr2tag_trace(fce, trace_env, atom2);
+   IRAtom *vatom1 = expr2vbits(fce, atom1);
+   IRAtom *vatom2 = expr2vbits(fce, atom2);
+   IRAtom *pop1, *pop2;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom1));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom2));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom1));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom2));
+
+   if (atom1->tag == Iex_Const && atom2->tag == Iex_Const) {
+      return tatom1;
+   }
+
+   pop1 = make_popcount(fce, trace_env, vatom1);
+   pop2 = make_popcount(fce, trace_env, vatom2);
+
+   /* Even though this helper has side-effects (i.e., sometimes printing
+      edges to the trace), it's a clean call because it's appropriate to
+      optimize away if its results are not used. */
+   return mkIRExprCCall(Ity_I64, 0, "FC_(helperc_BINARY_TAGS)",
+			&FC_(helperc_BINARY_TAGS),
+			mkIRExprVec_5(tatom1, tatom2, pop1, pop2,
+				      mkU64(trace_env->location++)));
+}
+
+static IRExpr *
+expr2tag_Triop_trace(FCEnv *fce, TraceEnv *trace_env, IROp op,
+		      IRAtom *atom1, IRAtom *atom2, IRAtom *atom3) {
+   IRAtom* tatom1 = expr2tag_trace(fce, trace_env, atom1);
+   IRAtom* tatom2 = expr2tag_trace(fce, trace_env, atom2);
+   IRAtom* tatom3 = expr2tag_trace(fce, trace_env, atom3);
+   IRAtom *vatom1 = expr2vbits(fce, atom1);
+   IRAtom *vatom2 = expr2vbits(fce, atom2);
+   IRAtom *vatom3 = expr2vbits(fce, atom3);
+   IRAtom *pop1, *pop2, *pop3;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom1));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom2));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom3));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom1));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom2));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom3));
+
+   pop1 = make_popcount(fce, trace_env, vatom1);
+   pop2 = make_popcount(fce, trace_env, vatom2);
+   pop3 = make_popcount(fce, trace_env, vatom3);
+
+   /* Even though this helper has side-effects (i.e., sometimes printing
+      edges to the trace), it's a clean call because it's appropriate to
+      optimize away if its results are not used. */
+   return mkIRExprCCall(Ity_I64, 0, "FC_(helperc_TERNARY_TAGS)",
+			&FC_(helperc_TERNARY_TAGS),
+			mkIRExprVec_7(tatom1, tatom2, tatom3,
+				      pop1, pop2, pop3,
+				      mkU64(trace_env->location++)));
+}
+
+static IRExpr *
+expr2tag_Qop_trace(FCEnv *fce, TraceEnv *trace_env, IROp op,
+		    IRAtom *atom1, IRAtom *atom2,
+		    IRAtom *atom3, IRAtom *atom4) {
+   IRAtom* tatom1 = expr2tag_trace(fce, trace_env, atom1);
+   IRAtom* tatom2 = expr2tag_trace(fce, trace_env, atom2);
+   IRAtom* tatom3 = expr2tag_trace(fce, trace_env, atom3);
+   IRAtom* tatom4 = expr2tag_trace(fce, trace_env, atom4);
+   IRAtom *vatom1 = expr2vbits(fce, atom1);
+   IRAtom *vatom2 = expr2vbits(fce, atom2);
+   IRAtom *vatom3 = expr2vbits(fce, atom3);
+   IRAtom *vatom4 = expr2vbits(fce, atom4);
+   IRAtom *pop1, *pop2, *pop3, *pop4;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom1));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom2));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom3));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom4));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom1));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom2));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom3));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom4));
+
+   pop1 = make_popcount(fce, trace_env, vatom1);
+   pop2 = make_popcount(fce, trace_env, vatom2);
+   pop3 = make_popcount(fce, trace_env, vatom3);
+   pop4 = make_popcount(fce, trace_env, vatom4);
+
+   /* Even though this helper has side-effects (i.e., sometimes printing
+      edges to the trace), it's a clean call because it's appropriate to
+      optimize away if its results are not used. */
+   return mkIRExprCCall(Ity_I64, 0, "FC_(helperc_FOURARY_TAGS)",
+			&FC_(helperc_FOURARY_TAGS),
+			mkIRExprVec_9(tatom1, tatom2, tatom3, tatom4,
+				      pop1, pop2, pop3, pop4,
+				      mkU64(trace_env->location++)));
+}
+
+static
+IRExpr** mkIRExprVec_11 ( IRExpr* arg1, IRExpr* arg2, IRExpr* arg3,
+			  IRExpr* arg4, IRExpr* arg5, IRExpr* arg6,
+			  IRExpr* arg7, IRExpr *arg8, IRExpr *arg9,
+			  IRExpr *arg10,IRExpr *arg11) {
+   IRExpr** vec = LibVEX_Alloc(12 * sizeof(IRExpr*));
+   vec[0] = arg1;
+   vec[1] = arg2;
+   vec[2] = arg3;
+   vec[3] = arg4;
+   vec[4] = arg5;
+   vec[5] = arg6;
+   vec[6] = arg7;
+   vec[7] = arg8;
+   vec[8] = arg9;
+   vec[9] = arg10;
+   vec[10] = arg11;
+   vec[11] = NULL;
+   return vec;
+}
+
+static IRExpr *
+expr2tag_5op_trace(FCEnv *fce, TraceEnv *trace_env, IROp op,
+		    IRAtom *atom1, IRAtom *atom2, IRAtom *atom3,
+		    IRAtom *atom4, IRAtom *atom5) {
+   IRAtom* tatom1 = expr2tag_trace(fce, trace_env, atom1);
+   IRAtom* tatom2 = expr2tag_trace(fce, trace_env, atom2);
+   IRAtom* tatom3 = expr2tag_trace(fce, trace_env, atom3);
+   IRAtom* tatom4 = expr2tag_trace(fce, trace_env, atom4);
+   IRAtom* tatom5 = expr2tag_trace(fce, trace_env, atom5);
+   IRAtom *vatom1 = expr2vbits(fce, atom1);
+   IRAtom *vatom2 = expr2vbits(fce, atom2);
+   IRAtom *vatom3 = expr2vbits(fce, atom3);
+   IRAtom *vatom4 = expr2vbits(fce, atom4);
+   IRAtom *vatom5 = expr2vbits(fce, atom5);
+   IRAtom *pop1, *pop2, *pop3, *pop4, *pop5;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom1));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom2));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom3));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom4));
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom5));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom1));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom2));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom3));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom4));
+   tl_assert(isShadowAtom_trace(fce, trace_env, tatom5));
+
+   pop1 = make_popcount(fce, trace_env, vatom1);
+   pop2 = make_popcount(fce, trace_env, vatom2);
+   pop3 = make_popcount(fce, trace_env, vatom3);
+   pop4 = make_popcount(fce, trace_env, vatom4);
+   pop5 = make_popcount(fce, trace_env, vatom5);
+
+   /* Even though this helper has side-effects (i.e., sometimes printing
+      edges to the trace), it's a clean call because it's appropriate to
+      optimize away if its results are not used. */
+   return mkIRExprCCall(Ity_I64, 0, "FC_(helperc_FIVEARY_TAGS)",
+			&FC_(helperc_FIVEARY_TAGS),
+			mkIRExprVec_11(tatom1, tatom2, tatom3, tatom4, tatom5,
+				       pop1, pop2, pop3, pop4, pop5,
+				       mkU64(trace_env->location++)));
+}
+
+static IRAtom *
+expr2tag_Load_trace(FCEnv *fce, TraceEnv *trace_env, IRType ty,
+		     IRAtom *addr) {
+   IRAtom *taddr;
+   void *helper;
+   const HChar *hname;
+   IRTemp datatag;
+   IRDirty *di;
+
+   tl_assert(addr);
+   tl_assert(isOriginalAtom_trace(fce, trace_env, addr));
+   taddr = expr2tag_trace(fce, trace_env, addr);
+   tl_assert(isShadowAtom_trace(fce, trace_env, taddr));
+
+   switch (ty) {
+   case Ity_I8:
+      helper = &FC_(helper_LOAD_TAG_1);
+      hname  = "FC_(helper_LOAD_TAG_1)";
+      break;
+   case Ity_I16:
+      helper = &FC_(helper_LOAD_TAG_2);
+      hname  = "FC_(helper_LOAD_TAG_2)";
+      break;
+   case Ity_I32: case Ity_F32:
+      helper = &FC_(helper_LOAD_TAG_4);
+      hname  = "FC_(helper_LOAD_TAG_4)";
+      break;
+   case Ity_I64: case Ity_F64:
+      helper = &FC_(helper_LOAD_TAG_8);
+      hname  = "FC_(helper_LOAD_TAG_8)";
+      break;
+   case Ity_I128: case Ity_V128:
+      helper = &FC_(helper_LOAD_TAG_16);
+      hname  = "FC_(helper_LOAD_TAG_16)";
+      break;
+   default:
+      ppIRType(ty);
+      VG_(tool_panic)("flowcheck trace:expr2tag_Load_trace");
+   }
+
+   datatag = newTempT(fce);
+   /* This is a dirty call because it can't be moved around writes to
+      memory. */
+   di = unsafeIRDirty_1_N(datatag, 0, hname, helper,
+			  mkIRExprVec_3(addr, taddr,
+					mkU64(trace_env->location++)));
+   di->nFxState = 0;
+   stmt(fce->sb, IRStmt_Dirty(di));
+
+   return mkexpr(datatag);
+}
+
+static IRAtom *
+handleCCall_trace(FCEnv *fce, TraceEnv *trace_env, IRAtom **exprvec,
+		  IRCallee* cee) {
+   tl_assert(exprvec);
+   tl_assert(exprvec[0]); // 0-arg clean calls don't make sense
+   if (!exprvec[1]) {
+      return expr2tag_Unop_trace(fce, trace_env, exprvec[0]);
+   } else if (!exprvec[2]) {
+      return expr2tag_Binop_trace(fce, trace_env, 0, exprvec[0], exprvec[1]);
+   } else if (!exprvec[3]) {
+      return expr2tag_Triop_trace(fce, trace_env, 0, exprvec[0], exprvec[1],
+				   exprvec[2]);
+   } else if (!exprvec[4]) {
+      return expr2tag_Qop_trace(fce, trace_env, 0, exprvec[0], exprvec[1],
+				 exprvec[2], exprvec[3]);
+   } else if (!exprvec[5]) {
+      return expr2tag_5op_trace(fce, trace_env, 0, exprvec[0], exprvec[1],
+				 exprvec[2], exprvec[3], exprvec[4]);
+   } else {
+      VG_(tool_panic)("flowcheck trace: too many clean call args");
+   }
+}
+
+IRExpr *expr2tag_trace(FCEnv *fce, TraceEnv *trace_env, IRExpr *e) {
+   switch (e->tag) {
+   case Iex_Get:
+      return shadow_Get_trace(fce, trace_env, e->Iex.Get.offset,
+			      e->Iex.Get.ty);
+
+   case Iex_GetI:
+      return shadow_GetI_trace(fce, trace_env, e->Iex.GetI.descr,
+			       e->Iex.GetI.ix, e->Iex.GetI.bias);
+
+   case Iex_RdTmp:
+      return IRExpr_RdTmp( findShadowTmp_trace(fce, trace_env,
+					       e->Iex.RdTmp.tmp));
+
+   case Iex_Const:
+      /* Literals are always public */
+      return IRExpr_Const(IRConst_U64(0));
+
+   case Iex_Unop:
+      return expr2tag_Unop_trace(fce, trace_env, e->Iex.Unop.arg);
+
+   case Iex_Binop:
+      return expr2tag_Binop_trace(fce, trace_env, e->Iex.Binop.op,
+				   e->Iex.Binop.arg1, e->Iex.Binop.arg2);
+
+   case Iex_Triop:
+      return expr2tag_Triop_trace(fce, trace_env, e->Iex.Triop.details->op,
+				   e->Iex.Triop.details->arg1, e->Iex.Triop.details->arg2,
+				   e->Iex.Triop.details->arg3);
+
+   case Iex_Qop:
+      return expr2tag_Qop_trace(fce, trace_env, e->Iex.Qop.details->op,
+				 e->Iex.Qop.details->arg1, e->Iex.Qop.details->arg2,
+				 e->Iex.Qop.details->arg3, e->Iex.Qop.details->arg4);
+
+   case Iex_Load:
+      return expr2tag_Load_trace(fce, trace_env, e->Iex.Load.ty,
+				 e->Iex.Load.addr);
+
+   case Iex_CCall:
+      return handleCCall_trace(fce, trace_env, e->Iex.CCall.args,
+			       e->Iex.CCall.cee);
+
+   case Iex_ITE:
+      return expr2tag_Triop_trace(fce, trace_env, 0,
+				   e->Iex.ITE.cond, e->Iex.ITE.iftrue,
+				   e->Iex.ITE.iffalse);
+
+   default:
+      VG_(printf)("\n");
+      ppIRExpr(e);
+      VG_(printf)("\n");
+      VG_(tool_panic)("flowcheck trace: expr2tag_trace");
+   }
+}
+
+static void do_shadow_Put_trace(FCEnv *fce, TraceEnv *trace_env, Int offset,
+				IRAtom *atom) {
+   IRAtom *tatom;
+   IRType ty;
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom));
+   tatom = expr2tag_trace(fce, trace_env, atom);
+
+   ty = typeOfIRExpr(fce->sb->tyenv, tatom);
+   tl_assert(ty != Ity_I1);
+
+   stmt(fce->sb,
+	IRStmt_Put((8 * offset) +
+		   FC_(shadow_guest_offset) + FC_(tags_guest_offset), tatom));
+}
+
+static void
+do_shadow_PutI_trace(FCEnv *fce, TraceEnv *trace_env, IRRegArray *descr,
+		     IRAtom *ix, Int bias, IRAtom *atom) {
+   IRAtom *tatom;
+   IRType ty;
+   IRRegArray *new_descr;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, atom));
+   tatom = expr2tag_trace(fce, trace_env, atom);
+   ty = descr->elemTy;
+   tl_assert(ty != Ity_I1);
+   tl_assert(isOriginalAtom_trace(fce, trace_env, ix));
+
+   new_descr =
+      mkIRRegArray((8 * descr->base) +
+		   FC_(shadow_guest_offset) + FC_(tags_guest_offset),
+		   Ity_I64, descr->nElems); // tags are all 64 bits
+
+   stmt(fce->sb, IRStmt_PutI(mkIRPutI(new_descr, ix, bias, tatom)));
+}
+
+static void
+do_shadow_Store_trace(FCEnv *fce, TraceEnv *trace_env, IRAtom *addr,
+		      IRAtom *data) {
+   IRAtom *taddr, *tdata;
+   void *helper;
+   const HChar *hname;
+   IRDirty *di;
+   IRType ty;
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, addr));
+   taddr = expr2tag_trace(fce, trace_env, addr);
+   tl_assert(isShadowAtom_trace(fce, trace_env, taddr));
+
+   tl_assert(isOriginalAtom_trace(fce, trace_env, data));
+   tdata = expr2tag_trace(fce, trace_env, data);
+   tl_assert(isShadowAtom_trace(fce, trace_env, tdata));
+
+   ty = typeOfIRExpr(fce->sb->tyenv, data);
+
+   switch (ty) {
+   case Ity_I8:
+      helper = &FC_(helper_STORE_TAG_1);
+      hname  = "FC_(helper_STORE_TAG_1)";
+      break;
+   case Ity_I16:
+      helper = &FC_(helper_STORE_TAG_2);
+      hname  = "FC_(helper_STORE_TAG_2)";
+      break;
+   case Ity_I32: case Ity_F32:
+      helper = &FC_(helper_STORE_TAG_4);
+      hname  = "FC_(helper_STORE_TAG_4)";
+      break;
+   case Ity_I64: case Ity_F64:
+      helper = &FC_(helper_STORE_TAG_8);
+      hname  = "FC_(helper_STORE_TAG_8)";
+      break;
+   case Ity_I128: case Ity_V128:
+      helper = &FC_(helper_STORE_TAG_16);
+      hname  = "FC_(helper_STORE_TAG_16)";
+      break;
+   default:
+      ppIRType(ty);
+      VG_(tool_panic)("flowcheck trace:do_shadow_Store_trace");
+   }
+
+   di = unsafeIRDirty_0_N(0, hname, helper,
+			  mkIRExprVec_5(addr, taddr, tdata,
+					mkU64(trace_env->location++),
+					IRExpr_RdTmp(fce->rollbackModeTmp32)));
+   di->nFxState = 0;
+   stmt(fce->sb, IRStmt_Dirty(di));
+}
+
+static void
+do_shadow_cond_trace(FCEnv *fce, TraceEnv *trace_env, IRExpr *guard) {
+   if (guard->tag == Iex_Const)
+      return;
+
+#if 0
+   IRDirty *di;
+   IRAtom *tguard;
+
+   /* Don't do this here, piggyback on complainIfUndefined instead */
+   tguard = expr2tag_trace(fce, trace_env, guard);
+   /* This needs to be dirty so it doesn't get optimized away. */
+   di = unsafeIRDirty_0_N(0, "FC_(helper_BRANCH_TAG)", &FC_(helper_BRANCH_TAG),
+			  mkIRExprVec_1(tguard));
+   di->nFxState = 0;
+   stmt(fce->sb, IRStmt_Dirty(di));
+#endif
+}
+
+static void
+do_shadow_Dirty_trace(FCEnv *fce, TraceEnv *trace_env, IRDirty *d) {
+   IRExpr **exprvec = d->args;
+   IRExpr *tresult;
+   IRTemp dst;
+
+   do_shadow_cond_trace(fce, trace_env, d->guard);
+   if (d->tmp == IRTemp_INVALID)
+      return;
+
+   if (!exprvec[0]) {
+      tresult = IRExpr_Const(IRConst_U64(0));
+   } else if (!exprvec[1]) {
+      tresult = expr2tag_Unop_trace(fce, trace_env, exprvec[0]);
+   } else if (!exprvec[2]) {
+      tresult = expr2tag_Binop_trace(fce, trace_env, 0, exprvec[0],
+				     exprvec[1]);
+   } else if (!exprvec[3]) {
+      tresult = expr2tag_Triop_trace(fce, trace_env, 0, exprvec[0],
+				     exprvec[1], exprvec[2]);
+   } else if (!exprvec[4]) {
+     tresult = expr2tag_Qop_trace(fce, trace_env, 0, exprvec[0], exprvec[1],
+				   exprvec[2], exprvec[3]);
+   } else if (!exprvec[5]) {
+      tresult = expr2tag_5op_trace(fce, trace_env, 0, exprvec[0], exprvec[1],
+				   exprvec[2], exprvec[3], exprvec[4]);
+   } else {
+      VG_(tool_panic)("flowcheck trace: too many dirty call args");
+   }
+
+   dst = findShadowTmp_trace(fce, trace_env, d->tmp);
+   assign(fce->sb, dst, tresult);
+}
+
+void
+do_shadow_jump_trace(FCEnv *fce, TraceEnv *trace_env, IRJumpKind jk,
+		     IRExpr *target, IRExpr *guard) {
+   IRDirty *di;
+   if (jk == Ijk_Call) {
+      di = unsafeIRDirty_0_N(0, "FC_(helper_WATCH_CALL)",
+			     &FC_(helper_WATCH_CALL), mkIRExprVec_1(target));
+      di->guard = guard;
+      stmt(fce->sb, IRStmt_Dirty(di));
+   } else if (jk == Ijk_Ret) {
+      di = unsafeIRDirty_0_N(0, "FC_(helper_WATCH_RET)",
+			     &FC_(helper_WATCH_RET), mkIRExprVec_0());
+      di->guard = guard;
+      stmt(fce->sb, IRStmt_Dirty(di));
+   }
+}
+
+void trace_translate_stmt(FCEnv *fce, TraceEnv *trace_env,
+			  IRStmt *st) {
+   switch (st->tag) {
+   case Ist_WrTmp:
+      {
+	 IRExpr *data = st->Ist.WrTmp.data;
+	 IRTemp result = st->Ist.WrTmp.tmp;
+	 IRTemp tresult = findShadowTmp_trace(fce, trace_env, result);
+	 IRTemp vresult = findShadowTmpV(fce, result);
+	 IRAtom *tdata = assignNewT(fce, Ity_I64,
+				    expr2tag_trace(fce, trace_env, data));
+	 IRAtom *pop = make_popcount(fce, trace_env, IRExpr_RdTmp(vresult));
+	 assign(fce->sb, tresult,
+		mkIRExprCCall(Ity_I64, 0, "FC_(helperc_RESULT_TAG)",
+			      &FC_(helperc_RESULT_TAG),
+			      mkIRExprVec_3(tdata, pop,
+					    mkU64(trace_env->location++))));
+      }
+      break;
+
+   case Ist_Put:
+      do_shadow_Put_trace(fce, trace_env, st->Ist.Put.offset,
+			  st->Ist.Put.data);
+      break;
+
+   case Ist_PutI:
+      do_shadow_PutI_trace(fce, trace_env, st->Ist.PutI.details->descr, st->Ist.PutI.details->ix,
+			   st->Ist.PutI.details->bias, st->Ist.PutI.details->data);
+      break;
+
+   case Ist_Store:
+      do_shadow_Store_trace(fce, trace_env, st->Ist.Store.addr,
+			    st->Ist.Store.data);
+      break;
+
+   case Ist_Exit:
+      do_shadow_cond_trace(fce, trace_env, st->Ist.Exit.guard);
+      do_shadow_jump_trace(fce, trace_env, st->Ist.Exit.jk,
+			   IRExpr_Const(st->Ist.Exit.dst),
+			   st->Ist.Exit.guard);
+      break;
+
+   case Ist_Dirty:
+      do_shadow_Dirty_trace(fce, trace_env, st->Ist.Dirty.details);
+      break;
+
+   case Ist_IMark:
+   case Ist_NoOp:
+   case Ist_AbiHint:
+   case Ist_MBE:
+      break;
+
+   default:
+      VG_(printf)("\n");
+      ppIRStmt(st);
+      VG_(printf)("\n");
+      VG_(tool_panic)("flowcheck trace: unhandled IRStmt");
+   }
+}
+
diff -urN exp-flowcheck-base/trace_translate.h exp-flowcheck/trace_translate.h
--- exp-flowcheck-base/trace_translate.h	1969-12-31 18:00:00.000000000 -0600
+++ exp-flowcheck/trace_translate.h	2016-08-30 15:23:14.031067800 -0500
@@ -0,0 +1,55 @@
+/*--------------------------------------------------------------------*/
+/*--- VEX IR transformations related to constructing a data-flow   ---*/
+/*--- graph of program execution.                                  ---*/
+/*---                                            trace_translate.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of FlowCheck, a heavyweight Valgrind tool for
+   detecting leakage of secret information.
+
+   By Stephen McCamant, MIT CSAIL Program Analsis group, Copyright (C)
+   2007-2008.
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef TRACE_TRANSLATE_H
+#define TRACE_TRANSLATE_H
+
+/* "TraceEnv" is a typedef to "struct _TraceEnv", but we need to
+   declare that in fc_translate.h, not here, because of the two-way
+   dependence between these two headers. */
+struct _TraceEnv {
+   /* We used to have our own tmpMap, but now that the
+      Memcheck/fc_translate one supports different shadow types, we
+      just piggyback on that. */
+
+   /* Static location identifier: guest instruction (4 bytes) + stmt
+      (1 bytes) + expr (1 byte) */
+   Long location;
+};
+
+IRExpr *expr2tag_trace(FCEnv *fce, TraceEnv *trace_env, IRExpr *e);
+
+void trace_translate_stmt(FCEnv *fce, TraceEnv *trace_env, IRStmt *st);
+
+void do_shadow_jump_trace(FCEnv *fce, TraceEnv *trace_env, IRJumpKind jk,
+			  IRExpr *target, IRExpr *guard);
+
+#endif /* TRACE_TRANSLATE_H */
